# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/pos_encoding.ipynb.

# %% auto 0
__all__ = ['SinusoidalPositionalEncoder']

# %% ../notebooks/pos_encoding.ipynb 6
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn import functional as F

# %% ../notebooks/pos_encoding.ipynb 9
class SinusoidalPositionalEncoder(nn.Module):
    """Module to encode position in a transformer like model.


    Args:
        max_seq_len (int): max length of sequence, aka L
        embedding_dim (int): dimension of embeddings in model, aka d
        dropout (float):  dropout rate. 0. for no dropout.
         n (int): user defined scalar set by default to 10000 as in paper
    """

    def positional_encoding(self):
        pos = torch.arange(0, self.max_seq_len).repeat(self.embedding_dim, 1)
        i = torch.arange(0, self.embedding_dim)
        k = i // 2
        wt = pos.T / (self.n ** (2 * k / self.embedding_dim))
        sin = torch.sin(wt)
        cos = torch.cos(wt)
        pe = torch.zeros((self.max_seq_len, self.embedding_dim))
        pe[:, 0::2] = sin[:, 0::2]
        pe[:, 1::2] = cos[:, 1::2]
        return pe

    def __init__(
        self, max_seq_len: int, embedding_dim: int, dropout: float = 0.0, n: int = 10000
    ) -> None:
        super().__init__()
        self.max_seq_len = max_seq_len
        self.embedding_dim = embedding_dim
        self.n = n
        self.dropout = nn.Dropout(p=dropout)

        pos_encodings = self.positional_encoding().unsqueeze(0)
        pos_encodings.requires_grad_(False)
        # a buffer is a state in module which is not a parameter (learned)
        self.register_buffer("pos_encodings", pos_encodings)

    def forward(self, x):
        B, T, C = x.shape
        # we only need the first T positions

        x = x + self.pos_encodings[:, :T]
        return self.dropout(x)
