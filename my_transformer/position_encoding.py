# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/pos_encoding.ipynb.

# %% auto 0
__all__ = ['PositionalEncoder']

# %% ../notebooks/pos_encoding.ipynb 2
class PositionalEncoder(nn.Module):
    """Module to encode position in a transformer like model.


    Args:
        max_seq_len (int): max length of sequence, aka L
        embedding_dim (int): dimension of embeddings in model, aka d
        dropout (float):  dropout rate. 0. for no dropout.
        is_learned (bool): true if the position is learned through gradient descent or given (non differentiable) as defined in original paper Attention is all you need, https://arxiv.org/abs/1706.03762
        n (int): user defined scalar set by default to 10000 as in paper
    """

    def positional_encoding(self):
        pos = torch.arange(0, self.max_seq_len).repeat(self.embedding_dim, 1)
        i = torch.arange(0, self.embedding_dim)
        k = i // 2
        wt = pos.T / (self.n ** (2 * k / self.embedding_dim))
        sin = torch.sin(wt)
        cos = torch.cos(wt)
        pe = torch.zeros((self.max_seq_len, self.embedding_dim))
        pe[:, 0::2] = sin[:, 0::2]
        pe[:, 1::2] = cos[:, 1::2]
        return pe

    def __init__(
        self,
        max_seq_len: int,
        embedding_dim: int,
        dropout: float = 0.0,
        is_learned: bool = True,
        n: int = 10000,
    ) -> None:
        super().__init__()
        self.max_seq_len = max_seq_len
        self.embedding_dim = embedding_dim
        self.is_learned = is_learned
        self.n = n
        self.dropout = nn.Dropout(p=dropout)
        if self.is_learned:
            pos = torch.arange(0, self.max_seq_len, dtype=torch.long).unsqueeze(
                0
            )  # shape (1, max_seq_len)
            self.register_buffer("pos", pos)
            self.pos_embedding = nn.Embedding(max_seq_len, embedding_dim)
        else:
            pos_encodings = self.positional_encoding().unsqueeze(0)
            pos_encodings.requires_grad_(False)
            # a buffer is a state in module which is not a parameter (learned)
            self.register_buffer("pos_encodings", pos_encodings)

    def forward(self, x):
        B, T, C = x.shape
        # we only need the first T positions
        if self.is_learned:
            x = x + self.pos_embedding(self.pos)[:, :T]
        else:
            x = x + self.pos_encodings[:, :T]
        return self.dropout(x)
