# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/model.ipynb.

# %% auto 0
__all__ = ['DecoderLayer', 'DecoderTransformer']

# %% ../notebooks/model.ipynb 1
import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn import functional as F
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    trainers,
    Tokenizer,
)
from tokenizers import ByteLevelBPETokenizer
import math
import os
from transformers import GPT2TokenizerFast

# %% ../notebooks/model.ipynb 4
from .attention import MultiHeadAttention


class DecoderLayer(nn.Module):
    def __init__(
        self,
        model_size: int,
        nb_heads: int = 1,
        dropout: float = 0.0,
        bias: bool = True,
        mlp_factor=4,
    ) -> None:
        super().__init__()
        self.layer_norm1 = nn.LayerNorm(model_size)  # LayerNormalization(model_size)
        self.attn = MultiHeadAttention(
            d=model_size, h=nb_heads, dropout=dropout, bias=bias
        )
        self.mlp1 = nn.Linear(model_size, mlp_factor * model_size, bias=bias)
        self.mlp2 = nn.Linear(mlp_factor * model_size, model_size, bias=bias)
        self.activation = torch.nn.GELU()
        self.layer_norm2 = nn.LayerNorm(model_size)  # LayerNormalization(model_size)
        self.dropout = nn.Dropout(dropout)

    def forward(
        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        b, l, d = x.size()
        norm_x = self.layer_norm1(x)
        attn_x = x + self.attn(norm_x, z=None, mask=mask)[0]

        norm_attn_x = self.layer_norm2(attn_x)
        lin1 = self.activation(self.mlp1(norm_attn_x))
        x = x + self.dropout(self.mlp2(lin1))
        return x

# %% ../notebooks/model.ipynb 6
from .attention import DecoderLayer, unidirectional_mask


class DecoderTransformer(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        max_seq_len: int,
        model_size: int,
        nb_heads: int = 1,
        nb_layers: int = 1,
        dropout: float = 0.0,
        bias: bool = True,
    ) -> None:
        super().__init__()
        self.max_seq_len = max_seq_len
        self.tok_emb = nn.Embedding(vocab_size, embedding_dim=model_size)
        self.pos_emb = torch.nn.Embedding(max_seq_len, embedding_dim=model_size)

        self.layers = nn.Sequential(
            *[
                DecoderLayer(
                    model_size=model_size, nb_heads=nb_heads, dropout=dropout, bias=bias
                )
                for i in range(nb_layers)
            ]
        )
        self.layer_norm = LayerNormalization(model_size)
        self.unembedding = nn.Linear(model_size, vocab_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        b, l = x.size()
        emb = self.tok_emb(x)
        x = self.pos_enc(emb)
        mask = unidirectional_mask(seq_len=l).to(x.device)
        for layer in self.layers:
            x = layer(x, mask)
        x = self.layer_norm(x)
        logits = self.unembedding(x)
        return logits

    def generate(self, x: torch.Tensor, max_new_tokens: int):
        for i in range(max_new_tokens):
            # we take at most max_seq_len tokens
            x_block = x[:, -self.max_seq_len :]
            logits = self(x_block)
            # we take the logit for last token, used to predict token.
            logits = logits[:, -1, :]
            probs = F.softmax(logits, dim=-1)
            tok_next = torch.multinomial(probs, num_samples=1)
            x = torch.cat((x, tok_next), dim=1)
        return x
