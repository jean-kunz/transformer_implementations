# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/model.ipynb.

# %% auto 0
__all__ = ['batch_size', 'seq_len', 'vocab_size', 'model_size', 'num_heads', 'num_layers', 'dropout', 'nb_epoch', 'last_epoch_nb',
           'model_name', 'model_version', 'tokenizer', 'train_ds', 'train_dl', 'test_ds', 'test_dl', 'tr', 'do_train',
           'FeedForward', 'DecoderLayer', 'DecoderTransformer', 'get_trained_tokenizer', 'BPEDataset', 'EpochTrainer']

# %% ../notebooks/model.ipynb 1
import torch.nn as nn
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter


from torch.nn import functional as F
from tokenizers import (
    decoders,
    models,
    normalizers,
    pre_tokenizers,
    trainers,
    Tokenizer,
)
from tokenizers import ByteLevelBPETokenizer


from datetime import datetime
from typing import Optional
import math
import os
from transformers import GPT2TokenizerFast
from my_transformer.attention import (
    unidirectional_mask,
    MultiHeadAttention,
    LayerNormalization,
)
from .utils import save_model, load_model

# %% ../notebooks/model.ipynb 4
class FeedForward(nn.Module):
    def __init__(
        self, model_size: int, mlp_factor: int = 4, dropout: float = 0.1
    ) -> None:
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(model_size, mlp_factor * model_size),
            nn.RELU(),
            nn.Linear(mlp_factor * model_size, model_size),
            nn.Dropout(dropout),
        )

        def forward(self, x):
            return self.net(x)


class DecoderLayer(nn.Module):
    def __init__(
        self,
        model_size: int,
        nb_heads: int = 1,
        dropout: float = 0.0,
        bias: bool = True,
        mlp_factor=4,
    ) -> None:
        super().__init__()
        self.layer_norm1 = LayerNormalization(
            model_size
        )  # LayerNormalization(model_size)
        self.attn = MultiHeadAttention(
            d=model_size, h=nb_heads, dropout=dropout, bias=bias
        )
        self.mlp1 = nn.Linear(model_size, mlp_factor * model_size, bias=bias)
        self.mlp2 = nn.Linear(mlp_factor * model_size, model_size, bias=bias)
        self.activation = nn.ReLU()
        self.layer_norm2 = LayerNormalization(
            model_size
        )  # LayerNormalization(model_size)
        self.dropout = nn.Dropout(dropout)

    def forward(
        self, x: torch.Tensor, mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        B, L, C = x.size()
        norm_x = self.layer_norm1(x)
        attn_x = x + self.attn(norm_x, z=None, mask=mask)[0]

        norm_attn_x = self.layer_norm2(attn_x)
        lin1 = self.activation(self.mlp1(norm_attn_x))
        x = x + self.dropout(self.mlp2(lin1))
        return x

# %% ../notebooks/model.ipynb 7
class DecoderTransformer(nn.Module):
    def __init__(
        self,
        vocab_size: int,
        max_seq_len: int,
        model_size: int,
        nb_heads: int = 1,
        nb_layers: int = 1,
        dropout: float = 0.0,
        bias: bool = True,
    ) -> None:
        super().__init__()
        self.max_seq_len = max_seq_len
        self.tok_emb = nn.Embedding(vocab_size, embedding_dim=model_size)
        self.pos_emb = torch.nn.Embedding(max_seq_len, embedding_dim=model_size)

        self.layers = nn.Sequential(
            *[
                DecoderLayer(
                    model_size=model_size, nb_heads=nb_heads, dropout=dropout, bias=bias
                )
                for i in range(nb_layers)
            ]
        )
        self.layer_norm = LayerNormalization(model_size)
        self.unembedding = nn.Linear(model_size, vocab_size)

    def get_device(self):
        # Check the device of parameters or buffers
        if next(self.parameters(), None) is not None:
            return next(self.parameters()).device
        elif next(self.buffers(), None) is not None:
            return next(self.buffers()).device
        else:
            return None  # No parameters or buffers in this module

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        B, L = x.size()
        tok_emb = self.tok_emb(x)
        device = self.get_device()
        pos_emb = self.pos_emb(torch.arange(L, device=device))
        mask = unidirectional_mask(seq_len=L).to(device)
        x = tok_emb + pos_emb
        # x = self.layers(x, mask)
        for layer in self.layers:
            x = layer(x, mask)
        x = self.layer_norm(x)
        logits = self.unembedding(x)
        return logits

    def generate(self, x: torch.Tensor, max_new_tokens: int):
        with torch.no_grad():
            for i in range(max_new_tokens):
                # we take at most max_seq_len tokens
                x_block = x[:, -self.max_seq_len :]
                logits = self(x_block)
                # we take the logit for last token, used to predict token.
                logits = logits[:, -1, :]
                probs = F.softmax(logits, dim=-1)
                tok_next = torch.multinomial(probs, num_samples=1)
                x = torch.cat((x, tok_next), dim=1)
        return x

# %% ../notebooks/model.ipynb 13
def get_trained_tokenizer(train_txt: str, vocab_size: int, name: str) -> Tokenizer:
    trained_tokenizer = ByteLevelBPETokenizer()
    trained_tokenizer.train_from_iterator([train_txt], vocab_size=vocab_size)
    tok_dir = f"./{name}_{vocab_size}"
    os.makedirs(tok_dir, exist_ok=True)
    trained_tokenizer.save_model(tok_dir)
    tokenizer = GPT2TokenizerFast.from_pretrained(tok_dir)
    return tokenizer

# %% ../notebooks/model.ipynb 14
class BPEDataset(Dataset):
    def __init__(
        self, text: str, tokenizer: Tokenizer, seq_len: int = 20, device: str = "cpu"
    ):
        self.device = device
        self.seq_len = seq_len
        self.text = text
        self.tokenizer = tokenizer

        self.encoded = torch.tensor(
            self.tokenizer.encode(self.text), dtype=torch.long, device=device
        )

    def __len__(self):
        return len(self.encoded) // self.seq_len

    def __getitem__(self, idx):
        data_len = len(self.encoded)
        i = idx * self.seq_len
        if i >= data_len:
            raise ValueError(f"idx {idx} bigger than data length {data_len}")
        x = self.encoded[i : i + self.seq_len]
        y = self.encoded[i + 1 : i + self.seq_len + 1]
        return x, y

# %% ../notebooks/model.ipynb 16
class EpochTrainer:
    def __init__(
        self,
        # model: nn.Module,
        get_new_model: callable,
        train_dl: DataLoader,
        test_dl: DataLoader,
        model_version: str,
        model_name: str,
        lr: float = 3e-4,
        nb_epochs: int = 100,
        loss_fn=F.cross_entropy,
        do_save_model: bool = True,
        log_interval: int = 50,
        device: str = "cpu",
    ) -> None:
        self.get_new_model_func = get_new_model
        self.loss_fn = loss_fn
        self.train_dl = train_dl
        self.test_dl = test_dl
        self.nb_epochs = nb_epochs
        self.do_save_model = do_save_model
        self.model_name = model_name
        self.model_version = model_version
        self.device = device
        self.writer = None
        self.lr = lr
        self.log_interval = log_interval

    def compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:
        B, T, C = logits.shape
        logits = logits.view(B * T, C)
        targets = targets.view(B * T)
        loss = self.loss_fn(logits, targets)
        return loss

    @torch.no_grad()
    def evaluate(self, model: nn.Module, iter: int) -> None:
        model.eval()
        losses = []
        for x, y in self.test_dl:
            logits = model(x)
            loss = self.compute_loss(logits, y)
            losses.append(loss.item())
        loss_mean = torch.tensor(losses).mean()
        self.writer.add_scalar(f"test loss", loss_mean, iter)
        model.train()

    def train(self, from_epoch: int = 0) -> nn.Module:
        if from_epoch > 0:
            model = load_model(self.model_name, self.model_version, from_epoch)
        else:
            model = self.get_new_model_func()
        self.writer = SummaryWriter(
            f"../runs/{self.model_name}_{self.model_version}/{datetime.now().strftime('%m-%d-%Y_%H:%M:%S')}"
        )
        ex_x, ex_y = next(iter(self.train_dl))
        self.writer.add_graph(model, (ex_x), use_strict_trace=False)
        self.writer.flush()

        if from_epoch > 0:
            epoch_start_nb = from_epoch + 1
        else:
            epoch_start_nb = 0
        i = 0
        nb_epochs_computed = self.nb_epochs - epoch_start_nb
        nb_batches = len(self.train_dl)
        with tqdm(
            total=nb_batches * nb_epochs_computed,
            desc=f"Epoch {nb_epochs_computed} times batch ({len(self.train_dl)})",
            unit="batch",
        ) as pbar:
            model.train()
            optimizer = torch.optim.AdamW(model.parameters(), lr=self.lr)
            for curr_epoch in range(epoch_start_nb, self.nb_epochs):
                for b, (xb, yb) in enumerate(self.train_dl):
                    i = (curr_epoch * nb_batches) + b

                    logits = model(xb)
                    optimizer.zero_grad()
                    loss = self.compute_loss(logits, yb)
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    optimizer.step()

                    self.writer.add_scalar("train loss", loss.item(), i)

                    pbar.update(1)
                    pbar.set_postfix(
                        {
                            "i": i,
                            "epoch": curr_epoch,
                            "batch_nb": b,
                            "train_loss": f"{loss.item():.4f}",
                        }
                    )

                    if i % self.log_interval == 0:
                        self.evaluate(model, i)
                        for name, weight in model.named_parameters():
                            self.writer.add_histogram(name, weight, i)
                            if weight.grad is not None:
                                g = weight.grad
                                self.writer.add_histogram(f"{name}.grad", g, i)
                                g_norm = g.data.norm(2)
                                self.writer.add_histogram(
                                    f"{name}.grad_norm", g_norm, i
                                )

                        if self.do_save_model:
                            save_model(model, self.model_name, self.model_version, i)

        # write embeddings:
        tok_embeddings = model.tok_emb.weight
        tok_metadata = [tokenizer.decode([i]) for i in range(tokenizer.vocab_size)]
        self.writer.add_embedding(tok_embeddings, metadata=tok_metadata)
        pos_embeddings = model.pos_emb.weight
        pos_metadata = range(seq_len)
        self.writer.add_embedding(pos_embeddings, metadata=pos_metadata)

        if self.do_save_model:
            save_model(model, self.model_name, self.model_version, curr_epoch)
        return model


torch.manual_seed(42)

batch_size = 64  # how many independent sequences will we process in parallel?
seq_len = 128  # what is the maximum context length for predictions?
vocab_size = 512 + 1
model_size = 384
num_heads = 6
num_layers = 6
dropout = 0.2
nb_epoch = 100
last_epoch_nb = 0
model_name = "gpt2"
model_version = f"t{vocab_size}_0.1"

tokenizer = get_trained_tokenizer(
    train_txt, vocab_size=vocab_size, name="shakespare_tok"
)
train_ds = BPEDataset(train_txt, tokenizer=tokenizer, seq_len=seq_len, device=device)
train_dl = DataLoader(train_ds, batch_size=batch_size)
test_ds = BPEDataset(test_txt, tokenizer=tokenizer, seq_len=seq_len, device=device)
test_dl = DataLoader(test_ds, batch_size=batch_size)


tr = EpochTrainer(
    get_new_model=lambda: DecoderTransformer(
        vocab_size=vocab_size,
        max_seq_len=seq_len,
        model_size=model_size,
        nb_heads=num_heads,
        nb_layers=num_layers,
        dropout=dropout,
    ).to(device),
    lr=3e-4,
    train_dl=train_dl,
    test_dl=test_dl,
    loss_fn=F.cross_entropy,
    nb_epochs=nb_epoch,
    do_save_model=True,
    log_interval=100,
    model_name=model_name,
    model_version=model_version,
    device=device,
)
do_train: bool = True
if do_train:
    model = tr.train(from_epoch=last_epoch_nb)
else:
    model = load_model(model_name=model_name, model_version=model_version, iter=1100)
