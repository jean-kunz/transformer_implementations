# Autogenerated by nbdev

d = { 'settings': { 'doc_baseurl': '/transformer_implementations',
                'doc_host': 'https://jean-kunz.github.io',
                'git_url': 'https://github.com/jean-kunz/transformer_implementations',
                'lib_path': 'my_transformer'},
  'syms': { 'my_transformer.attention': { 'my_transformer.attention.DecoderTransformer': ( 'attention.html#decodertransformer',
                                                                                           'my_transformer/attention.py'),
                                          'my_transformer.attention.DecoderTransformer.__init__': ( 'attention.html#decodertransformer.__init__',
                                                                                                    'my_transformer/attention.py'),
                                          'my_transformer.attention.DecoderTransformer.forward': ( 'attention.html#decodertransformer.forward',
                                                                                                   'my_transformer/attention.py'),
                                          'my_transformer.attention.DecoderTransformer.generate': ( 'attention.html#decodertransformer.generate',
                                                                                                    'my_transformer/attention.py'),
                                          'my_transformer.attention.LayerNormalization': ( 'attention.html#layernormalization',
                                                                                           'my_transformer/attention.py'),
                                          'my_transformer.attention.LayerNormalization.__init__': ( 'attention.html#layernormalization.__init__',
                                                                                                    'my_transformer/attention.py'),
                                          'my_transformer.attention.LayerNormalization.forward': ( 'attention.html#layernormalization.forward',
                                                                                                   'my_transformer/attention.py'),
                                          'my_transformer.attention.MultiHeadAttention': ( 'attention.html#multiheadattention',
                                                                                           'my_transformer/attention.py'),
                                          'my_transformer.attention.MultiHeadAttention.__init__': ( 'attention.html#multiheadattention.__init__',
                                                                                                    'my_transformer/attention.py'),
                                          'my_transformer.attention.MultiHeadAttention.forward': ( 'attention.html#multiheadattention.forward',
                                                                                                   'my_transformer/attention.py'),
                                          'my_transformer.attention.Trainer': ('attention.html#trainer', 'my_transformer/attention.py'),
                                          'my_transformer.attention.Trainer.__init__': ( 'attention.html#trainer.__init__',
                                                                                         'my_transformer/attention.py'),
                                          'my_transformer.attention.Trainer.compute_loss': ( 'attention.html#trainer.compute_loss',
                                                                                             'my_transformer/attention.py'),
                                          'my_transformer.attention.Trainer.estimate_loss': ( 'attention.html#trainer.estimate_loss',
                                                                                              'my_transformer/attention.py'),
                                          'my_transformer.attention.Trainer.train': ( 'attention.html#trainer.train',
                                                                                      'my_transformer/attention.py'),
                                          'my_transformer.attention.attention': ('attention.html#attention', 'my_transformer/attention.py'),
                                          'my_transformer.attention.unidirectional_mask': ( 'attention.html#unidirectional_mask',
                                                                                            'my_transformer/attention.py')},
            'my_transformer.model': { 'my_transformer.model.DecoderLayer': ('model.html#decoderlayer', 'my_transformer/model.py'),
                                      'my_transformer.model.DecoderLayer.__init__': ( 'model.html#decoderlayer.__init__',
                                                                                      'my_transformer/model.py'),
                                      'my_transformer.model.DecoderLayer.forward': ( 'model.html#decoderlayer.forward',
                                                                                     'my_transformer/model.py'),
                                      'my_transformer.model.DecoderTransformer': ( 'model.html#decodertransformer',
                                                                                   'my_transformer/model.py'),
                                      'my_transformer.model.DecoderTransformer.__init__': ( 'model.html#decodertransformer.__init__',
                                                                                            'my_transformer/model.py'),
                                      'my_transformer.model.DecoderTransformer.forward': ( 'model.html#decodertransformer.forward',
                                                                                           'my_transformer/model.py'),
                                      'my_transformer.model.DecoderTransformer.generate': ( 'model.html#decodertransformer.generate',
                                                                                            'my_transformer/model.py')},
            'my_transformer.position_encoding': { 'my_transformer.position_encoding.SinusoidalPositionalEncoder': ( 'pos_encoding.html#sinusoidalpositionalencoder',
                                                                                                                    'my_transformer/position_encoding.py'),
                                                  'my_transformer.position_encoding.SinusoidalPositionalEncoder.__init__': ( 'pos_encoding.html#sinusoidalpositionalencoder.__init__',
                                                                                                                             'my_transformer/position_encoding.py'),
                                                  'my_transformer.position_encoding.SinusoidalPositionalEncoder.forward': ( 'pos_encoding.html#sinusoidalpositionalencoder.forward',
                                                                                                                            'my_transformer/position_encoding.py'),
                                                  'my_transformer.position_encoding.SinusoidalPositionalEncoder.positional_encoding': ( 'pos_encoding.html#sinusoidalpositionalencoder.positional_encoding',
                                                                                                                                        'my_transformer/position_encoding.py')},
            'my_transformer.tokenizers': { 'my_transformer.tokenizers.BPETokenizer': ( 'tokenizers.html#bpetokenizer',
                                                                                       'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.__init__': ( 'tokenizers.html#bpetokenizer.__init__',
                                                                                                'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer._pad': ( 'tokenizers.html#bpetokenizer._pad',
                                                                                            'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.compute_pair_freqs': ( 'tokenizers.html#bpetokenizer.compute_pair_freqs',
                                                                                                          'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.compute_words_freqs': ( 'tokenizers.html#bpetokenizer.compute_words_freqs',
                                                                                                           'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.decode': ( 'tokenizers.html#bpetokenizer.decode',
                                                                                              'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.enable_padding': ( 'tokenizers.html#bpetokenizer.enable_padding',
                                                                                                      'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.encode': ( 'tokenizers.html#bpetokenizer.encode',
                                                                                              'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.find_best_pair': ( 'tokenizers.html#bpetokenizer.find_best_pair',
                                                                                                      'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.init_vocab': ( 'tokenizers.html#bpetokenizer.init_vocab',
                                                                                                  'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.merge_pair': ( 'tokenizers.html#bpetokenizer.merge_pair',
                                                                                                  'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.pre_tokenize': ( 'tokenizers.html#bpetokenizer.pre_tokenize',
                                                                                                    'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.train': ( 'tokenizers.html#bpetokenizer.train',
                                                                                             'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.BPETokenizer.train_from_iterator': ( 'tokenizers.html#bpetokenizer.train_from_iterator',
                                                                                                           'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.ByteLevelDecoder': ( 'tokenizers.html#byteleveldecoder',
                                                                                           'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.ByteLevelDecoder.decode': ( 'tokenizers.html#byteleveldecoder.decode',
                                                                                                  'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.ByteLevelPreTokenizer': ( 'tokenizers.html#bytelevelpretokenizer',
                                                                                                'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.ByteLevelPreTokenizer.__init__': ( 'tokenizers.html#bytelevelpretokenizer.__init__',
                                                                                                         'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.ByteLevelPreTokenizer.pre_tokenize_str': ( 'tokenizers.html#bytelevelpretokenizer.pre_tokenize_str',
                                                                                                                 'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.Encoding': ( 'tokenizers.html#encoding',
                                                                                   'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.Encoding.__init__': ( 'tokenizers.html#encoding.__init__',
                                                                                            'my_transformer/tokenizers.py'),
                                           'my_transformer.tokenizers.Encoding.__repr__': ( 'tokenizers.html#encoding.__repr__',
                                                                                            'my_transformer/tokenizers.py')},
            'my_transformer.utils': { 'my_transformer.utils.load_model': ('utils.html#load_model', 'my_transformer/utils.py'),
                                      'my_transformer.utils.save_model': ('utils.html#save_model', 'my_transformer/utils.py')}}}
