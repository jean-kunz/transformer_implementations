{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "output-file: tokenizers.html\n",
    "title: Tokenizers\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# we use transformer tokenezier to validate my code.\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ByteLevelDecoder\n",
       "\n",
       ">      ByteLevelDecoder ()\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ByteLevelDecoder\n",
       "\n",
       ">      ByteLevelDecoder ()\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ByteLevelDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### ByteLevelPreTokenizer\n",
       "\n",
       ">      ByteLevelPreTokenizer (add_prefix_space:bool=False)\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### ByteLevelPreTokenizer\n",
       "\n",
       ">      ByteLevelPreTokenizer (add_prefix_space:bool=False)\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(ByteLevelPreTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def test_encode_decode():\n",
    "    txt = \"Hello, world! How  are you?good .\"\n",
    "    toks = ByteLevelPreTokenizer(add_prefix_space=False).pre_tokenize_str(txt)\n",
    "    assert ByteLevelDecoder().decode(toks) == txt\n",
    "\n",
    "\n",
    "test_encode_decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def test_pre_tokenizer_against_transformers():\n",
    "    \"\"\"Test ByteLevelPreTokenizer against transformers' ByteLevel pre-tokenizer\"\"\"\n",
    "    ByteLevelPreTokenizer(add_prefix_space=False).pre_tokenize_str(\"Hello, world! How  are you?good .\")\n",
    "    sents = [\n",
    "        \"Hello, world! How  are you?good .\",\n",
    "        \"Hello world,  How are you ? Go !\",\n",
    "    ]\n",
    "    add_prefix_space: bool = False\n",
    "    trf_pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n",
    "    my_pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=add_prefix_space)\n",
    "    my_decoder = ByteLevelDecoder()\n",
    "\n",
    "    for s in sents:\n",
    "        toks = my_pre_tokenizer.pre_tokenize_str(s)\n",
    "        trf_toks = trf_pre_tokenizer.pre_tokenize_str(s)\n",
    "        for t, trf_t in zip(toks, trf_toks):\n",
    "            assert t[0] == trf_t[0], f\"{t} != {trf_t}\"\n",
    "            assert t[1][0] == trf_t[1][0], f\"{t} != {trf_t}\"\n",
    "            assert t[1][1] == trf_t[1][1], f\"{t} != {trf_t}\"\n",
    "\n",
    "        assert my_decoder.decode(my_pre_tokenizer.pre_tokenize_str(s)) == \" \" + s if add_prefix_space else s\n",
    "\n",
    "\n",
    "test_pre_tokenizer_against_transformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### BPETokenizer\n",
       "\n",
       ">      BPETokenizer (vocab_size:int=1000)\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### BPETokenizer\n",
       "\n",
       ">      BPETokenizer (vocab_size:int=1000)\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(BPETokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "has_sd": true,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "### Encoding\n",
       "\n",
       ">      Encoding (ids, tokens, offsets:Optional[list]=None,\n",
       ">                attention_mask:Optional[list]=None)\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "### Encoding\n",
       "\n",
       ">      Encoding (ids, tokens, offsets:Optional[list]=None,\n",
       ">                attention_mask:Optional[list]=None)\n",
       "\n",
       "*Initialize self.  See help(type(self)) for accurate signature.*"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "tk = BPETokenizer(vocab_size=50)\n",
    "text = \"A Hello world, this is a test. A new world is coming, Hell yes . I love this\"\n",
    "tk.train(text)\n",
    "txt_to_enc = \"Hello world, I love to test this new thing\"\n",
    "\n",
    "encodings = tk.encode(txt_to_enc)\n",
    "\n",
    "assert \"\".join([t for t in encodings.tokens]).replace(\"Ġ\", \" \") == txt_to_enc\n",
    "assert \"\".join([tk.decode([i]) for i in encodings.ids]) == txt_to_enc\n",
    "assert \"\".join([txt_to_enc[s:e] for s, e in encodings.offsets]) == txt_to_enc\n",
    "\n",
    "encodings = tk.encode(\"Hello TOTO\")\n",
    "assert encodings.tokens[-5:] == [\"Ġ\", \"[UNK]\", \"[UNK]\", \"[UNK]\", \"[UNK]\"]\n",
    "\n",
    "tk.enable_padding(direction=\"right\", length=15)\n",
    "padded_encodings = tk.encode(\"Hello world TOTO\")\n",
    "assert padded_encodings.tokens[-5:] == [\"[UNK]\", \"[PAD]\", \"[PAD]\", \"[PAD]\", \"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# example using transformer tokenizer on a new training dataset\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "# tokenizer.enable_padding(length=20, pad_token=\"[PAD]\")\n",
    "trainer = trainers.BpeTrainer(vocab_size=50, special_tokens=[\"[PAD]\", \"[UNK]\"])  # pad is 0, unk is 1\n",
    "tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "tokenizer.get_vocab_size()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.enable_padding(length=20, pad_token=\"[PAD]\")\n",
    "sent_to_encode = \"Hello World TOTO\"\n",
    "encodings = tokenizer.encode(sent_to_encode)\n",
    "encodings.offsets, encodings.tokens, encodings.attention_mask, encodings.ids\n",
    "# tokenizer.id_to_token(0)\n",
    "print(encodings.ids), print(encodings.tokens)\n",
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# tokenizer.enable_padding(pad_id=2, pad_token=\"<|PAD|>\", length=55)\n",
    "tokenizer.enable_padding(length=30, pad_token=\"[PAD]\")\n",
    "tokenizer.encode(txt_to_enc[:50]).attention_mask\n",
    "tokenizer.encode(txt_to_enc[:50]).tokens\n",
    "# tokenizer.padding\n",
    "# tokenizer.get_vocab()\n",
    "tokenizer.encode(txt_to_enc[:50]).tokens\n",
    "tokenizer.token_to_id(\"TOTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "tk = BPETokenizer(vocab_size=50)\n",
    "\n",
    "text = \"A Hello world, this is a test. A new world is coming, Hell yes . I love this\"\n",
    "tk.train(text)\n",
    "text_enc = tk.encode(\"Hello, I love to test this new thing\")\n",
    "print(tk.decode(text_enc.ids))\n",
    "print(text_enc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
