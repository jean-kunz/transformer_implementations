{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# | default_exp attention\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers components to be assembled in a model\n",
    "\n",
    "Work in progress\n",
    "\n",
    "References:\n",
    "- https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture09-transformers.pdf\n",
    "\n",
    "Evolutions:\n",
    "\n",
    "try techniques as defined in : \n",
    "- https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Getting_the_most_out_of_LLMs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from my_transformer.utils import save_model, load_model\n",
    "from my_transformer.tokenizers import BPETokenizer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "import warnings\n",
    "from torch.jit import TracerWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "g = torch.Generator(device=device).manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## positional encoder must be dynamic for any lenght (up to max length) provided while generating tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "Based on formal algorithm for transformers (https://arxiv.org/abs/2207.09238) where it can be used for different attention architectures like \n",
    "- encoder-decoder/seq-to-seq (very first transformer), \n",
    "- encoder-only (bert), \n",
    "- decoder-only (gpt-*, gopher). \n",
    "\n",
    "### Encoder-decoder architecture use 2 sequences as input\n",
    "- context sequence of tokens (z), which is used to encode context as a vector per token with bidirectional attention\n",
    "- primary sequence of tokens (x), which is used to decode both the encoded context and a masked primary sequence (tokens in primary sequence that precedes current). This is used to train a translator from FR to EN for instance. FR tokens are the context sequence and EN tokens are the primary sequence (target)\n",
    " \n",
    "### Encoder only architecture use 1 primary sequence as input\n",
    "Given a primary input sequence (x) with some tokens masked out, the goal is to recover the masked tokens. the goal is to learn a generally usefull representation of text. Uses a bidirectional attention\n",
    "\n",
    "### Decoder only architecture use 1 primary sequence as input\n",
    "Autoregressive language modelling where the goal is to predict the next token of a primary token sequence (x). Uses a unidirectional causal (masked) attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def unidirectional_mask(seq_len: int) -> torch.Tensor:\n",
    "    inverse_mask = torch.triu(torch.ones((1, seq_len, seq_len)), diagonal=1)  # .type(torch.uint8)\n",
    "    # ic(inverse_mask)\n",
    "    mask = inverse_mask == 0\n",
    "    # ic(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.disable()\n",
    "mask = unidirectional_mask(5)\n",
    "\n",
    "plt.imshow(mask[0])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    softmax: torch.nn.Module = nn.Softmax(dim=-1),\n",
    "    dropout: Optional[torch.nn.Module] = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"compute attention weigths and attention weights applied to value tensor.\n",
    "\n",
    "    Arguments:\n",
    "        query -- query tensor in batch_size, head_nb, seq_len, d_k shape\n",
    "        key -- same shape structure as query\n",
    "        value -- same shape structure as query\n",
    "\n",
    "    Keyword Arguments:\n",
    "        mask -- mask of tokens (default: {None})\n",
    "        softmax -- softmax module (default: {nn.Softmax(dim=-1)})\n",
    "        dropout -- dropout ratio (default: {None})\n",
    "\n",
    "    Returns:\n",
    "        attention -- attention weight applied to value\n",
    "        attn-weights\n",
    "    \"\"\"\n",
    "    # d_k is the size of atttention per head (=d//nb heads, where d is the size of attn model)\n",
    "    # IT SHOULD BE, batch_size, head_nb, seq_len , d_k\n",
    "    batch_size, head_nb, seq_len, d_k = query.shape\n",
    "\n",
    "    scores = (query @ key.transpose(-2, -1)) * (d_k**-0.5)\n",
    "    if mask is not None:\n",
    "        # mask scores with -inf when mask is False. If we masked_fill(mask, -inf), it would mask when mask ==1 (is true). We want to mask when it's 0 (false).\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    attn_weights = softmax(scores)  # , dim=-1)\n",
    "    if dropout is not None:\n",
    "        attn_weights = dropout(attn_weights)\n",
    "\n",
    "    atn = attn_weights @ value\n",
    "    # ic(atn.shape, attn_weights.shape, value.shape)\n",
    "    assert (\n",
    "        atn.shape == query.shape\n",
    "    ), f\"atn shape {atn.shape} should be the same as input tensors key, query and value shapes. Query shape: {query.shape}\"\n",
    "    return atn, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 4\n",
    "embedding_dim = 5\n",
    "h = 1  # nb of heads\n",
    "d_k = embedding_dim // h\n",
    "batch_size = 1\n",
    "# shape is batch, h, seq_len, d_k\n",
    "Q = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "K = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "V = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "mask = unidirectional_mask(seq_len).to(device)\n",
    "\n",
    "ic.disable()\n",
    "ic.enable()\n",
    "attn, attn_weight = attention(Q, K, V, mask=mask, softmax=torch.nn.Softmax(dim=-1), dropout=torch.nn.Dropout(0.0))\n",
    "attn_no_mask, attn_weight_no_mask = attention(Q, K, V, dropout=torch.nn.Dropout(0.0))\n",
    "sum_attn_w = attn_weight.sum(dim=-1)\n",
    "assert torch.isclose(sum_attn_w, torch.ones_like(sum_attn_w), atol=0.001).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention viz\n",
    "\n",
    "we see that masked attention weight is a matrix where upper diagonal is 0. But attention (attention weight applied to value), is a weighted sum of values at each time step. It takes only current and previous steps (so values are not masked, only the weights are masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_matrix(m: torch.Tensor, title: str = \"Matrix\"):\n",
    "    np_m = m.cpu().detach().numpy()\n",
    "\n",
    "    plt.imshow(np_m)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_matrix(attn[0][0], title=\"Attention WITH mask\")\n",
    "display_matrix(attn_weight[0][0], title=\"Attention weight WITH mask\")\n",
    "\n",
    "display_matrix(attn_no_mask[0][0], title=\"Attention NO mask\")\n",
    "display_matrix(attn_weight_no_mask[0][0], title=\"Attention weight NO mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multihead attention module as defined in Formal algorithm for transformers (https://arxiv.org/abs/2207.09238)\n",
    "    It can be used for different attention architectures like encoder-decoder/seq-to-seq (very first transformer),\n",
    "    encoder-only (bert), decoder-only (gpt-*, gopher).\n",
    "\n",
    "    It splits weights into h heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d: int, h: int, dropout: float = 0.0, bias: bool = True):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Arguments:\n",
    "            d -- dimension of hidden state (aka model dimension)\n",
    "            h -- nb of heads\n",
    "\n",
    "        Keyword Arguments:\n",
    "            dropout -- dropout rate (default: {0.0})\n",
    "            bias -- do we include bias in linear computations (default: {True})\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        assert (\n",
    "            d % h == 0\n",
    "        ), f\"Model dim {d} must be a multiple of nb of heads {h}. d_k {d//h} is the model dim per head, because we split by nb of heads.\"\n",
    "        self.d_k = d // h  # model dim on one head.\n",
    "        self.wq = nn.Linear(d, d, bias=bias)\n",
    "        self.wk = nn.Linear(d, d, bias=bias)\n",
    "        self.wv = nn.Linear(d, d, bias=bias)\n",
    "        self.wo = nn.Linear(d, d, bias=bias)  # linear projection of output matrix.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, z: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): primary sequence\n",
    "            z (torch.Tensor): context sequence, only for encoder-decoder architecture.\n",
    "            mask (torch.Tensor, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, d = x.size()\n",
    "        if z is None:\n",
    "            z = x\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(z)\n",
    "        v = self.wk(z)\n",
    "        # shape has to be batch, h, seq_len, d_k (d//h)\n",
    "        # first view to : batch,seq_len,h,d_k , then transpose h and seq_len so we got per head q,k,v\n",
    "        q = q.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # attention has to be done on each head.\n",
    "        mh_attn, mh_attn_weight = attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "        assert mh_attn_weight.shape == (batch_size, self.h, seq_len, seq_len)\n",
    "        # we need to create a contiguous memory space for tensor after transpose so we can apply view.\n",
    "        concat_attn = mh_attn.transpose(2, 1).contiguous().view(batch_size, seq_len, self.h * self.d_k)\n",
    "        concat_attn_weight = torch.sum(mh_attn_weight, dim=1)\n",
    "        assert concat_attn.size() == (batch_size, seq_len, d)\n",
    "        # apply linear layer on concatenated attention.\n",
    "        output = self.dropout(self.wo(concat_attn))\n",
    "        return output, concat_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "primary_seq = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "ctx_seq = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "mh = MultiHeadAttention(d=embedding_dim, h=5, dropout=0.0, bias=False).to(device)\n",
    "mh_attn, mh_attn_weight = mh(primary_seq, ctx_seq, mask=mask)\n",
    "\n",
    "display_matrix(mh_attn[0], title=\"Attention output\")\n",
    "display_matrix(mh_attn_weight[0], title=\"Attention weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with multihead_attn = nn.MultiheadAttention(embed_dim, num_heads) so that result is the same.\n",
    "# the graph output should look the same, even if they are not exactly the same due to different initialization\n",
    "nn_mh = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=5, dropout=0.0, batch_first=True).to(device)\n",
    "nn_mh_atn, nn_mh_atn_weights = nn_mh(primary_seq, ctx_seq, ctx_seq, attn_mask=mask[0] == 0)\n",
    "\n",
    "\n",
    "display_matrix(nn_mh_atn[0], title=\"Attention output\")\n",
    "display_matrix(nn_mh_atn_weights[0], title=\"Attention weight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d: int, eps: float = 1e-05, use_torch_implem: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.eps = eps\n",
    "        self.use_torch_implem = use_torch_implem\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(d, dtype=torch.float))  # scale\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(d, dtype=torch.float))  # offset\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_torch_implem:\n",
    "            # we normalize across features for each example and seq item.\n",
    "            x_hat = F.layer_norm(x, normalized_shape=[self.d], weight=self.gamma, bias=self.beta)\n",
    "        else:\n",
    "            # normalization across features (independently) for each sample. We compute mean and var on the last 2 axis, so we have it per sampel\n",
    "            mean = x.mean((-1), keepdim=True)  # .unsqueeze(-1)\n",
    "            var = x.var((-1), keepdim=True)  # .unsqueeze(-1)\n",
    "            x_hat = torch.mul(((x - mean) / torch.sqrt(var + self.eps)), self.gamma) + self.beta\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check implement is close to pytorch one.\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "x = torch.tensor([[[2, 1, 1, 1, 1], [2, 2, 2, 2, 1]]], dtype=torch.float).to(device)\n",
    "layer_norm = LayerNormalization(d=embedding_dim, eps=1e-05, use_torch_implem=False).to(device)\n",
    "x_hat = layer_norm(x)\n",
    "\n",
    "torch_x_hat = F.layer_norm(\n",
    "    x, normalized_shape=[embedding_dim], weight=torch.ones(embedding_dim, dtype=torch.float, device=device), eps=1e-05\n",
    ")\n",
    "assert torch.allclose(x_hat, torch_x_hat, rtol=0.15, atol=0.0), \"my implementation and torch should be close\"\n",
    "x_hat, torch_x_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder transformer\n",
    "\n",
    "Architecture used by GPT-*, Gopher, where it want to predict the next token given previous ones. Uses a mask to make attention causal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_size: int, nb_heads: int = 1, dropout: float = 0.0, bias: bool = True, mlp_factor=4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(model_size)  # LayerNormalization(model_size)\n",
    "        self.attn = MultiHeadAttention(d=model_size, h=nb_heads, dropout=dropout, bias=bias)\n",
    "        self.mlp1 = nn.Linear(model_size, mlp_factor * model_size, bias=bias)\n",
    "        self.mlp2 = nn.Linear(mlp_factor * model_size, model_size, bias=bias)\n",
    "        self.activation = torch.nn.GELU()\n",
    "        self.layer_norm2 = nn.LayerNorm(model_size)  # LayerNormalization(model_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        b, l, d = x.size()\n",
    "        norm_x = self.layer_norm1(x)\n",
    "        attn_x = x + self.attn(norm_x, z=None, mask=mask)[0]\n",
    "\n",
    "        norm_attn_x = self.layer_norm2(attn_x)\n",
    "        lin1 = self.activation(self.mlp1(norm_attn_x))\n",
    "        x = x + self.dropout(self.mlp2(lin1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "dl = DecoderLayer(embedding_dim, nb_heads=1, dropout=0.1).to(device)\n",
    "mask = unidirectional_mask(seq_len=seq_len).to(device)\n",
    "dlo = dl(x, mask=mask)\n",
    "\n",
    "plt.imshow(x[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(dlo[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        model_size: int,\n",
    "        nb_heads: int = 1,\n",
    "        nb_layers: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embedding_dim=model_size)\n",
    "        self.pos_enc = PositionalEncoder(\n",
    "            max_seq_len=max_seq_len, embedding_dim=model_size, dropout=dropout, is_learned=True\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                DecoderLayer(model_size=model_size, nb_heads=nb_heads, dropout=dropout, bias=bias)\n",
    "                for i in range(nb_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = LayerNormalization(model_size)\n",
    "        self.unembedding = nn.Linear(model_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, l = x.size()\n",
    "        emb = self.tok_emb(x)\n",
    "        x = self.pos_enc(emb)\n",
    "        mask = unidirectional_mask(seq_len=l).to(x.device)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.unembedding(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, x: torch.Tensor, max_new_tokens: int):\n",
    "        for i in range(max_new_tokens):\n",
    "            # we take at most max_seq_len tokens\n",
    "            x_block = x[:, -self.max_seq_len :]\n",
    "            logits = self(x_block)\n",
    "            # we take the logit for last token, used to predict token.\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            tok_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, tok_next), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(model: nn.Module):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "\n",
    "dec = DecoderTransformer(vocab_size=6, max_seq_len=5, model_size=10, nb_heads=1).to(device)\n",
    "print_model_size(dec)\n",
    "x = torch.tensor([[0, 1, 2, 3, 4], [1, 2, 3, 4, 5]], dtype=torch.int, device=device)\n",
    "y_hat = dec(x)\n",
    "y_hat.shape\n",
    "\n",
    "\n",
    "target = torch.rand(*y_hat.shape).to(device)\n",
    "ic(target.shape, y_hat.shape)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss = loss(y_hat, target)\n",
    "loss.retain_grad()\n",
    "y_hat.retain_grad()\n",
    "loss\n",
    "\n",
    "\n",
    "input = torch.zeros((3, 1), dtype=torch.long, device=device)\n",
    "dec.generate(input, max_new_tokens=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../dataset/shakespeare.txt\") as f:\n",
    "#    text = f.read()\n",
    "\n",
    "df = pd.read_csv(\"../dataset/bob_dylan_lyrics.csv\")\n",
    "text = \"\\n\\n\".join(df.lyrics.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(text: str, vocab_size: int = 1000) -> Tokenizer:\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\"])  # pad is 0, unk is 1\n",
    "    tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "vocab_size = 1000\n",
    "tokenizer = train_tokenizer(text, vocab_size=vocab_size)\n",
    "sent_to_encode = \"How does it feel to be on your own !\"\n",
    "encodings = tokenizer.encode(sent_to_encode)\n",
    "encodings.ids, tokenizer.decode(encodings.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEDataset(Dataset):\n",
    "    def __init__(self, text: str, mode: str = \"train\", seq_len: int = 20, vocab_size: int = 1000, device: str = \"cpu\"):\n",
    "        assert mode in [\"train\", \"test\"], \"Mode must be 'train' or 'test'\"\n",
    "        self.mode = mode\n",
    "        self.device = device\n",
    "        self.seq_len = seq_len\n",
    "        self.text = text\n",
    "\n",
    "        words = text.split(\" \")\n",
    "        train_pos = math.ceil(len(words) * 0.8)\n",
    "        train_words = words[:train_pos]\n",
    "        test_words = words[train_pos:]\n",
    "        self.train_txt = \" \".join(train_words[:])\n",
    "        self.test_txt = \" \".join(test_words[:])\n",
    "        self.tokenizer = train_tokenizer(self.train_txt, vocab_size=vocab_size)\n",
    "        train_encoding = self.tokenizer.encode(self.train_txt)\n",
    "        test_encoding = self.tokenizer.encode(self.test_txt)\n",
    "        self.train_encoded = torch.tensor(train_encoding.ids, dtype=torch.long, device=device)\n",
    "        self.test_encoded = torch.tensor(test_encoding.ids, dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            data = self.train_encoded\n",
    "        else:\n",
    "            data = self.test_encoded\n",
    "        return len(data) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"train\":\n",
    "            data = self.train_encoded\n",
    "        else:\n",
    "            data = self.test_encoded\n",
    "\n",
    "        data_len = data.shape[0]\n",
    "        i = idx * self.seq_len\n",
    "        if i >= data_len:\n",
    "            raise ValueError(f\"idx {idx} bigger than {self.mode} data length {data_len}\")\n",
    "        x = data[i : i + self.seq_len]\n",
    "        y = data[i + 1 : i + self.seq_len + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "device = \"mps\"\n",
    "\n",
    "batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "seq_len = 256  # what is the maximum context length for predictions?\n",
    "vocab_size = 5000\n",
    "model_size = 384\n",
    "num_heads = 6\n",
    "num_layers = 6\n",
    "dropout = 0.2\n",
    "model_version = f\"t{vocab_size}_0.5\"\n",
    "\n",
    "train_ds = BPEDataset(text, mode=\"train\", seq_len=seq_len, vocab_size=vocab_size, device=device)\n",
    "test_ds = BPEDataset(text, mode=\"test\", seq_len=seq_len, vocab_size=vocab_size, device=device)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = DecoderTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=seq_len,\n",
    "    model_size=model_size,\n",
    "    nb_heads=num_heads,\n",
    "    nb_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "xb, yb = next(iter(test_dl))\n",
    "train_logit = model(xb)\n",
    "train_logit.shape\n",
    "\n",
    "print(f\"\"\"cross entropy of logit of non-trained model should be the about the same as cross entropy of uniform distrib across all tokens: \n",
    "      {F.cross_entropy(train_logit.view(batch_size * seq_len, vocab_size), yb.view(batch_size * seq_len)).item()}, \n",
    "      {(-torch.log(torch.tensor(1)/torch.tensor(vocab_size))).item()}\"\"\")\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        train_dl: DataLoader,\n",
    "        test_dl: DataLoader,\n",
    "        model_version: str,\n",
    "        model_name: str,\n",
    "        nb_epochs: int = 100,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        do_save_model: bool = True,\n",
    "        save_every_epoch_nb: int = 20,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_dl = train_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.do_save_model = do_save_model\n",
    "        self.save_every_epoch_nb = save_every_epoch_nb\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.device = device\n",
    "        self.writer = None\n",
    "\n",
    "    def compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "        loss = self.loss_fn(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self, i: int):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            dl = self.train_dl if split == \"train\" else self.test_dl\n",
    "            if dl is not None:\n",
    "                losses = []\n",
    "                for x, y in dl:\n",
    "                    logits = self.model(x)\n",
    "                    loss = self.compute_loss(logits, y)\n",
    "                    losses.append(loss.item())\n",
    "                loss_mean = torch.tensor(losses).mean()\n",
    "                out[split] = loss_mean\n",
    "                self.writer.add_scalar(f\"{split} loss\", loss_mean, i)\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    def train(self, from_epoch: int = 0):\n",
    "        self.writer = SummaryWriter(\n",
    "            f\"../runs/{self.model_name}_{self.model_version}/{datetime.now().strftime('%m-%d-%Y_%H:%M:%S')}\"\n",
    "        )\n",
    "        ex_x, ex_y = next(iter(self.train_dl))\n",
    "        self.writer.add_graph(self.model, (ex_x), use_strict_trace=False)\n",
    "        self.writer.flush()\n",
    "        if from_epoch > 0:\n",
    "            epoch_start_nb = from_epoch + 1\n",
    "        else:\n",
    "            epoch_start_nb = 0\n",
    "        nb_epochs_computed = self.nb_epochs - epoch_start_nb\n",
    "        with tqdm(\n",
    "            total=len(self.train_dl) * nb_epochs_computed,\n",
    "            desc=f\"Epoch {nb_epochs_computed} times batch ({len(self.train_dl)})\",\n",
    "            unit=\"batch\",\n",
    "        ) as pbar:\n",
    "            for curr_epoch in range(epoch_start_nb, self.nb_epochs):\n",
    "                for b, (xb, yb) in enumerate(self.train_dl):\n",
    "                    if device=='mps':\n",
    "                        logits = self.model(xb)\n",
    "                        loss = self.compute_loss(logits, yb)\n",
    "                    else:\n",
    "                        # use bf16 when possible based on autocast rules. bf16 is same range than float32, but less precision.\n",
    "                        with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                            logits = self.model(xb)\n",
    "                            loss = self.compute_loss(logits, yb)\n",
    "                    self.optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    pbar.update(1)\n",
    "\n",
    "                losses = self.estimate_loss(curr_epoch)\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"epoch\": curr_epoch,\n",
    "                        \"train_loss\": f\"{losses.get('train',torch.inf):.4f}\",\n",
    "                        \"test_loss\": f\"{losses.get('test', torch.inf):.4f}\",\n",
    "                    }\n",
    "                )\n",
    "                # print(f\"epoch {curr_epoch}: train loss {losses.get('train',torch.inf):.4f}, val loss {losses.get('test', torch.inf):.4f}\")\n",
    "                for name, weight in self.model.named_parameters():\n",
    "                    self.writer.add_histogram(name, weight, curr_epoch)\n",
    "\n",
    "                # every once in a while evaluate the loss on train and val sets\n",
    "                if self.do_save_model:\n",
    "                    if curr_epoch % self.save_every_epoch_nb == 0:\n",
    "                        save_model(self.model, self.model_name, self.model_version, curr_epoch)\n",
    "\n",
    "        if self.do_save_model:\n",
    "            save_model(self.model, self.model_name, self.model_version, curr_epoch)\n",
    "\n",
    "\n",
    "do_train_small_dataset: bool = True\n",
    "if do_train_small_dataset:\n",
    "    sm_text = text[: seq_len * batch_size]\n",
    "    sm_ds = BPEDataset(sm_text, mode=\"train\", seq_len=seq_len, vocab_size=vocab_size, device=device)\n",
    "    sm_dl = DataLoader(sm_ds, batch_size=batch_size, shuffle=True)\n",
    "    assert len(iter(sm_dl)) == 1\n",
    "\n",
    "    model_name = \"sm_decoder_transformer\"\n",
    "    last_epoch_nb = 0  # 0 to start from scratch\n",
    "    nb_epoch = 150\n",
    "    if last_epoch_nb > 0:\n",
    "        model = load_model(model_name, model_version, last_epoch_nb)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    tr = EpochTrainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        sm_dl,\n",
    "        None,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        nb_epochs=nb_epoch,\n",
    "        do_save_model=False,\n",
    "        save_every_epoch_nb=20,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        device=device,\n",
    "    )\n",
    "    tr.train(from_epoch=last_epoch_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch_nb = 0  # 0 to start from scratch\n",
    "# after 30 epoch, it overfit\n",
    "# should implement early stopping.\n",
    "# try with a smaller model\n",
    "nb_epoch = 30\n",
    "do_train: bool = True\n",
    "model_name = \"decoder_transformer\"\n",
    "if last_epoch_nb > 0:\n",
    "    model = load_model(model_name, model_version, last_epoch_nb)\n",
    "else:\n",
    "    model = DecoderTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_len=seq_len,\n",
    "        model_size=model_size,\n",
    "        nb_heads=num_heads,\n",
    "        nb_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "if do_train:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    tr = EpochTrainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_dl,\n",
    "        test_dl=test_dl,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        nb_epochs=nb_epoch,\n",
    "        do_save_model=True,\n",
    "        save_every_epoch_nb=20,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        device=device,\n",
    "    )\n",
    "    tr.train(from_epoch=last_epoch_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sentence(sent_start: str, nb_tokens: int = 50):\n",
    "    batch_input_encodings = train_ds.tokenizer.encode_batch([sent_start])\n",
    "    batch_input_toks = [s.ids for s in batch_input_encodings]\n",
    "    batch_input_tensor = torch.tensor(batch_input_toks, dtype=torch.long, device=device)\n",
    "    gen_tokens = model.generate(batch_input_tensor, max_new_tokens=nb_tokens)\n",
    "    return train_ds.tokenizer.decode_batch(gen_tokens.tolist())\n",
    "\n",
    "\n",
    "complete_sentence(\"The answer, my friend is blowing \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# trainer class for pytorch model that encapsulate training loop\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        train_dl: DataLoader,\n",
    "        test_dl: DataLoader,\n",
    "        model_version: str,\n",
    "        model_name: str,\n",
    "        eval_interval: int = 500,\n",
    "        eval_iters: int = 200,\n",
    "        max_iters: int = 5000,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        do_save_model: bool = True,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_dl = train_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.eval_interval = eval_interval\n",
    "        self.eval_iters = eval_iters\n",
    "        self.max_iters = max_iters\n",
    "        self.do_save_model = do_save_model\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.device = device\n",
    "\n",
    "        self.writer = None\n",
    "\n",
    "    def compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "        loss = self.loss_fn(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self, i: int):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            dl = self.train_dl if split == \"train\" else self.test_dl\n",
    "            losses = torch.zeros(self.eval_iters)\n",
    "            for k in range(self.eval_iters):\n",
    "                x, y = next(iter(dl))\n",
    "                logits = self.model(x)\n",
    "                loss = self.compute_loss(logits, y)\n",
    "                losses[k] = loss.item()\n",
    "            loss_mean = losses.mean()\n",
    "            out[split] = loss_mean\n",
    "            self.writer.add_scalar(f\"{split} loss\", loss_mean, i)\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    def train(self, from_iter: int = 0):\n",
    "        self.writer = SummaryWriter(\n",
    "            f\"../runs/{self.model_name}_{self.model_version}/{datetime.now().strftime('%m-%d-%Y_%H:%M:%S')}\"\n",
    "        )\n",
    "        ex_x, ex_y = next(iter(self.train_dl))\n",
    "        self.writer.add_graph(self.model, (ex_x), use_strict_trace=False)\n",
    "        self.writer.flush()\n",
    "        for i in range(last_iter + 1, self.max_iters):\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if i % self.eval_interval == 0:\n",
    "                losses = self.estimate_loss(i)\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['test']:.4f}\")\n",
    "                save_model(self.model, self.model_name, self.model_version, i)\n",
    "\n",
    "                for name, weight in self.model.named_parameters():\n",
    "                    self.writer.add_histogram(name, weight, i)\n",
    "\n",
    "            # sample a batch of data\n",
    "            xb, yb = next(iter(self.train_dl))\n",
    "\n",
    "            logits = self.model(xb)\n",
    "            loss = self.compute_loss(logits, yb)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.do_save_model:\n",
    "            save_model(self.model, self.model_name, self.model_version, i)\n",
    "\n",
    "\n",
    "# TODO: add save every eval interval\n",
    "# TODO: keep iteration so we can start at a given iteration. by reloading previous model\n",
    "\n",
    "do_train: bool = True\n",
    "model_version = f\"t{vocab_size}_0.5\"\n",
    "model_name = \"decoder_transformer\"\n",
    "last_iter = 7000  # 0 to start from scratch\n",
    "max_iter = 10001\n",
    "# last_iter = 7000\n",
    "if last_iter > 0:\n",
    "    model = load_model(model_name, model_version, last_iter)\n",
    "if do_train:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "    tr = Trainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        eval_interval=200,\n",
    "        eval_iters=100,\n",
    "        max_iters=max_iter,\n",
    "        do_save_model=True,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        device=device,\n",
    "    )\n",
    "    tr.train(from_iter=last_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_sent_toks = tokenizer.encode(\"here are the\")\n",
    "tokenizer.decode(beg_sent_toks.ids)\n",
    "sent_input = torch.tensor(beg_sent_toks.ids, dtype=torch.int).expand((1, -1)).to(device)\n",
    "sent_output = model.generate(sent_input, max_new_tokens=10)\n",
    "tokenizer.decode_batch(sent_output.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "try to implement\n",
    "https://towardsdatascience.com/deep-dive-into-anthropics-sparse-autoencoders-by-hand-%EF%B8%8F-eebe0ef59709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export(\"./components.ipynb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class AKSortDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the Sort problem. E.g. for problem length 6:\n",
    "    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 0 2 1 0 1 0 0 0 1 1\n",
    "    output: I I I I I 0 0 0 1 1 2\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=6, num_digits=3):\n",
    "        assert split in {\"train\", \"test\"}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "        self.num_digits = num_digits\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10000  # ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.num_digits\n",
    "\n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer,\n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return self.length * 2 - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # use rejection sampling to generate an input example from the desired split\n",
    "        while True:\n",
    "            # generate some random integers\n",
    "            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n",
    "            # half of the time let's try to boost the number of examples that\n",
    "            # have a large number of repeats, as this is what the model seems to struggle\n",
    "            # with later in training, and they are kind of rate\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                if inp.unique().nelement() > self.length // 2:\n",
    "                    # too many unqiue digits, re-sample\n",
    "                    continue\n",
    "            # figure out if this generated example is train or test based on its hash\n",
    "            h = hash(pickle.dumps(inp.tolist()))\n",
    "            inp_split = \"test\" if h % 4 == 0 else \"train\"  # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break  # ok\n",
    "\n",
    "        # solve the task: i.e. sort\n",
    "        sol = torch.sort(inp)[0]\n",
    "\n",
    "        # concatenate the problem specification and the solution\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[: self.length - 1] = -1\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# print an example instance of the dataset\n",
    "train_dataset = AKSortDataset(\"train\")\n",
    "\n",
    "x, y = train_dataset[0]\n",
    "# for a, b in zip(x,y):\n",
    "#    print(int(a),int(b))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-implementations-urBBcPaT-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
