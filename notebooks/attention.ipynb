{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# | default_exp attention\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention based modules\n",
    "\n",
    "\n",
    "References:\n",
    "- https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture09-transformers.pdf\n",
    "\n",
    "Technical evolutions:\n",
    " \n",
    "- https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Getting_the_most_out_of_LLMs.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from my_transformer.utils import save_model, load_model\n",
    "from my_transformer.tokenizers import BPETokenizer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "import warnings\n",
    "from torch.jit import TracerWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=TracerWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "g = torch.Generator(device=device).manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "Based on formal algorithm for transformers (https://arxiv.org/abs/2207.09238) where it can be used for different attention architectures like \n",
    "- encoder-decoder/seq-to-seq (very first transformer), \n",
    "- encoder-only (bert), \n",
    "- decoder-only (gpt-*, gopher). \n",
    "\n",
    "### Encoder-decoder architecture use 2 sequences as input\n",
    "- context sequence of tokens (z), which is used to encode context as a vector per token with bidirectional attention\n",
    "- primary sequence of tokens (x), which is used to decode both the encoded context and a masked primary sequence (tokens in primary sequence that precedes current). This is used to train a translator from FR to EN for instance. FR tokens are the context sequence and EN tokens are the primary sequence (target)\n",
    " \n",
    "### Encoder only architecture use 1 primary sequence as input\n",
    "Given a primary input sequence (x) with some tokens masked out, the goal is to recover the masked tokens. the goal is to learn a generally usefull representation of text. Uses a bidirectional attention\n",
    "\n",
    "### Decoder only architecture use 1 primary sequence as input\n",
    "Autoregressive language modelling where the goal is to predict the next token of a primary token sequence (x). Uses a unidirectional causal (masked) attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic mechanism behind attention\n",
    "How can we compute a matrix of size TxT (T is nb timesteps or length) to split attention from each token in sequence to only previouse tokens\n",
    "\n",
    "That matrix would be called attention matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| wei: tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "                 [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "                 [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "                 [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "                 [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "                 [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "                 [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "                 [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "ic| \"weight\": 'weight'\n",
      "    wei: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "                 [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "                 [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    }
   ],
   "source": [
    "# 1st method to compute the matrix\n",
    "wei = torch.tril(torch.ones((T, T)))\n",
    "ic(wei)\n",
    "wei = wei / wei.sum(axis=1, keepdim=True)\n",
    "ic(\"weight\", wei)\n",
    "xbow = wei @ x  # B,T,C @ B,T,T ---> B,T,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| tril: tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "                  [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "                  [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "                  [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "                  [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "                  [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "                  [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "                  [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
      "ic| wei: tensor([[4., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "                 [4., 4., -inf, -inf, -inf, -inf, -inf, -inf],\n",
      "                 [4., 4., 4., -inf, -inf, -inf, -inf, -inf],\n",
      "                 [4., 4., 4., 4., -inf, -inf, -inf, -inf],\n",
      "                 [4., 4., 4., 4., 4., -inf, -inf, -inf],\n",
      "                 [4., 4., 4., 4., 4., 4., -inf, -inf],\n",
      "                 [4., 4., 4., 4., 4., 4., 4., -inf],\n",
      "                 [4., 4., 4., 4., 4., 4., 4., 4.]])\n",
      "ic| wei: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
      "                 [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
      "                 [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
      "                 [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# method 2 using softmax\n",
    "tril = torch.tril(torch.ones((T, T)))\n",
    "ic(tril)\n",
    "# we can initiate with zeros, ones or any values\n",
    "wei = torch.zeros((T, T))\n",
    "#\n",
    "wei = torch.ones((T, T))\n",
    "wei = torch.full((T, T), 4.0)\n",
    "\n",
    "# ic(wei)\n",
    "wei = wei.masked_fill(tril == 0, value=float(\"-inf\"))\n",
    "ic(wei)\n",
    "wei = F.softmax(wei, dim=1)\n",
    "ic(wei)\n",
    "\n",
    "xbow2 = wei @ x\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let put it together\n",
    "\n",
    "- we project the input (x) to a \n",
    "  - query: current element we focus on\n",
    "  - key: context (other words)\n",
    "  - value : \n",
    "- we compute similarity with dot product between query and key, to capture how similar current element and context are aligned/similar, aka raw attention score\n",
    "- we mask the score matrix so each query focus only on past tokens (key)\n",
    "- we normalize the similarity score with a softmax -> it compute the weight each token should put on previous tokens.\n",
    "- apply the attention matrix (weight) to the value matrix. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| B: 1, T: 8, C: 5\n",
      "ic| wei.shape: torch.Size([1, 8, 8])\n",
      "    v.shape: torch.Size([1, 8, 5])\n",
      "    out.shape: torch.Size([1, 8, 5])\n",
      "ic| wei[0]: tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                    [0.4967, 0.5033, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                    [0.3345, 0.3286, 0.3369, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                    [0.2571, 0.2444, 0.2441, 0.2544, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "                    [0.1978, 0.1931, 0.2006, 0.2130, 0.1955, 0.0000, 0.0000, 0.0000],\n",
      "                    [0.1666, 0.1617, 0.1675, 0.1753, 0.1628, 0.1661, 0.0000, 0.0000],\n",
      "                    [0.1391, 0.1407, 0.1470, 0.1503, 0.1398, 0.1403, 0.1428, 0.0000],\n",
      "                    [0.1232, 0.1230, 0.1296, 0.1297, 0.1201, 0.1202, 0.1258, 0.1285]],\n",
      "                   grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAGzCAYAAAAc+X/PAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0vklEQVR4nO3de3xU1b3///dkIBMuSUBJAkggCiogAhIkJ0AUNJLDQVr0IVK0JWBFpYkCOVabVgxUJWiFL7alQVDAxxFO8AJIK0KREtACPyAYC15ABCQHTQAtSQiawMz+/YGMjLmQuSR7T+b1fDzW49HZ2WuvT2LKJ+uy17IZhmEIAACYJszsAAAACHUkYwAATEYyBgDAZCRjAABMRjIGAMBkJGMAAExGMgYAwGQkYwAATEYyBgDAZCRj4EdsNptmzpxpdhgBN3HiRCUkJPhct23btoENCIAbyRgB9Ze//EU2m01JSUm1fv3jjz/WzJkzdeTIkVrrLlu2rHED/N66deuaZcI125kzZzRz5kwVFBSYHQoQVEjGCKjly5crISFBO3fu1MGDB2t8/eOPP9asWbMskYxnzZpV69e+/fZbPfHEE00SR1NavHix9u/f36htnDlzRrNmzSIZA14iGSNgDh8+rG3btmnevHmKiYnR8uXLzQ7JJxEREWrRooXZYQRcy5Yt5XA4zA4DQC1IxgiY5cuXq3379ho1apTuuuuuGsl42bJlGjt2rCRp+PDhstlsstlsKigoUEJCgj766CNt2bLFfX3YsGHuuqdOndK0adMUHx8vh8OhHj166Nlnn5XL5XLfc+TIEdlsNj3//PNatGiRunfvLofDoRtvvFG7du1y3zdx4kQtWLBAktxt2Ww299drmzP+4IMPNHLkSEVFRalt27a69dZbtWPHjhrfn81m0z//+U9lZWUpJiZGbdq00R133KETJ07U+7Nbu3atbDab/vWvf7mvvfnmm7LZbLrzzjs97u3Vq5fGjRvnce3VV19VYmKiWrVqpcsuu0w/+9nPVFxc7HFPbXPGX3/9tX7xi18oKipK7dq1U3p6uj788EPZbLZaRymOHTumMWPGqG3btoqJidGjjz4qp9Mp6fzPPyYmRpI0a9Ys98+V6QDg0prfn/8wzfLly3XnnXcqPDxc48ePV15ennbt2qUbb7xRknTTTTfpkUce0R//+Ef99re/Va9evSSdTy7z58/Xww8/rLZt2+p3v/udJCkuLk7S+aHPm2++WceOHdODDz6orl27atu2bcrOztZXX32l+fPne8SxYsUKVVRU6MEHH5TNZtNzzz2nO++8U4cOHVLLli314IMP6ssvv9TGjRv1P//zP5f8vj766COlpKQoKipKjz32mFq2bKkXX3xRw4YN05YtW2rMjz/88MNq3769cnJydOTIEc2fP1+ZmZlauXJlnW0MHTpUNptNW7duVd++fSVJ7733nsLCwvT++++77ztx4oQ+/fRTZWZmuq8988wzmjFjhu6++27df//9OnHihP70pz/ppptu0gcffKB27drV2qbL5dLo0aO1c+dOTZkyRT179tRbb72l9PT0Wu93Op1KS0tTUlKSnn/+eb377ruaO3euunfvrilTpigmJkZ5eXmaMmWK7rjjDvcfERe+HwD1MIAA2L17tyHJ2Lhxo2EYhuFyuYwuXboYU6dO9bjv9ddfNyQZmzdvrvGM6667zrj55ptrXH/qqaeMNm3aGAcOHPC4/pvf/Maw2+3G0aNHDcMwjMOHDxuSjMsvv9z45ptv3Pe99dZbhiTjr3/9q/taRkaGUdevvyQjJyfH/XnMmDFGeHi48fnnn7uvffnll0ZkZKRx0003ua8tXbrUkGSkpqYaLpfLfX369OmG3W43Tp06VWt7F3//d999t/vzgAEDjLFjxxqSjE8++cQwDMNYtWqVIcn48MMPDcMwjCNHjhh2u9145plnPJ61d+9eo0WLFh7X09PTjW7durk/v/nmm4YkY/78+e5rTqfTuOWWWwxJxtKlSz3qSjJ+//vfe7Rzww03GImJie7PJ06cqPHzA3BpDFMjIJYvX664uDgNHz5c0vmh3nHjxik/P989jOmr119/XSkpKWrfvr1OnjzpLqmpqXI6ndq6davH/ePGjVP79u3dn1NSUiRJhw4d8rptp9Opv//97xozZoyuuuoq9/VOnTrpnnvu0fvvv6/y8nKPOg888IDHsHdKSoqcTqe++OKLettKSUnRe++9J0mqqKjQhx9+qAceeEAdOnRwX3/vvffUrl079enTR5K0atUquVwu3X333R4/m44dO+rqq6/W5s2b62xv/fr1atmypSZPnuy+FhYWpoyMjDrrPPTQQzVi9uXnCsATw9Twm9PpVH5+voYPH67Dhw+7ryclJWnu3LnatGmTRowY4fPzP/vsM/3rX/9yz0f+2PHjxz0+d+3a1ePzhcT873//2+u2T5w4oTNnzujaa6+t8bVevXrJ5XKpuLhY1113nd/tp6SkaOHChTp48KA+//xz2Ww2JScnu5P05MmT9d5772nIkCEKCzv/d/Rnn30mwzB09dVX1/rMli1b1tneF198oU6dOql169Ye13v06FHr/RERETX+G7Rv396nnysATyRj+O0f//iHvvrqK+Xn5ys/P7/G15cvX+5XMna5XLrtttv02GOP1fr1a665xuOz3W6v9T7DMHyOwRu+tj906FBJ0tatW3Xo0CENGDBAbdq0UUpKiv74xz/q9OnT+uCDD/TMM8+467hcLtlsNr3zzju1thvIjTrq+r4A+I9kDL8tX75csbGx7hXKF1u1apVWr16thQsXqlWrVh7Dtz9W19e6d++u06dPKzU1NWAx1xfHxWJiYtS6deta38/99NNPFRYWpvj4+IDE1LVrV3Xt2lXvvfeeDh065B5ev+mmm5SVlaXXX39dTqdTN910k7tO9+7dZRiGrrzyyhp/lFxKt27dtHnzZp05c8ajd1zb++EN1dCfKwBPzBnDL99++61WrVql22+/XXfddVeNkpmZqYqKCq1du1aS1KZNG0nnX1X6sTZt2tR6/e6779b27du1YcOGGl87deqUzp0753Xc9cVxMbvdrhEjRuitt97y2KiktLRUK1as0NChQxUVFeV1+3VJSUnRP/7xD+3cudOdjPv376/IyEjNmTNHrVq1UmJiovv+O++8U3a7XbNmzarR8zYMQ19//XWdbaWlpens2bNavHix+5rL5ar1j6qGupDUL/VzBeCJnjH8snbtWlVUVOgnP/lJrV//j//4D/cGIOPGjVP//v1lt9v17LPPqqysTA6HQ7fccotiY2OVmJiovLw8Pf300+rRo4diY2N1yy236Ne//rXWrl2r22+/XRMnTlRiYqIqKyu1d+9evfHGGzpy5Ig6dOjgVdwXEtojjzyitLQ02e12/exnP6v13qefflobN27U0KFD9atf/UotWrTQiy++qKqqKj333HPe/cAuISUlRcuXL5fNZnMPW9vtdg0ePFgbNmzQsGHDFB4e7r6/e/fuevrpp5Wdna0jR45ozJgxioyM1OHDh7V69Wo98MADevTRR2tta8yYMRo0aJD++7//WwcPHlTPnj21du1affPNN5J86+W2atVKvXv31sqVK3XNNdfosssuU58+fdwLzgDUwcyl3Ah+o0ePNiIiIozKyso675k4caLRsmVL4+TJk4ZhGMbixYuNq666yrDb7R6vOZWUlBijRo0yIiMjDUkerzlVVFQY2dnZRo8ePYzw8HCjQ4cOxuDBg43nn3/eqK6uNgzjh1eb/vCHP9SIQT963ebcuXPGww8/bMTExBg2m83jNacf32sYhrFnzx4jLS3NaNu2rdG6dWtj+PDhxrZt2zzuufBq065duzyub968uc7XuX7so48+MiQZvXr18rj+9NNPG5KMGTNm1FrvzTffNIYOHWq0adPGaNOmjdGzZ08jIyPD2L9/v/ueH7/aZBjnX0W65557jMjISCM6OtqYOHGi8c9//tOQZOTn53vUbdOmTY12c3Jyarwitm3bNiMxMdEIDw/nNSeggWyG0USrWgAEhTVr1uiOO+7Q+++/ryFDhpgdDhASSMZACPv222/VqlUr92en06kRI0Zo9+7dKikp8fgagMbDnDEQwh5++GF9++23Sk5OVlVVlVatWqVt27Zp9uzZJGKgCdEzBkLYihUrNHfuXB08eFDfffedevTooSlTpnjsfQ2g8fFqExDC7rnnHhUWFqqsrExVVVX66KOPSMQIaVu3btXo0aPVuXNn2Ww2rVmz5pJ1CgoKNGDAAPeJcr6cy04yBgDge5WVlerXr1+D37c/fPiwRo0apeHDh6uoqEjTpk3T/fffX+u+CPVhmBoAgFrYbDatXr1aY8aMqfOexx9/XG+//bb27dvnvvazn/1Mp06d0vr16xvcVpMv4HK5XPryyy8VGRnJ1nkAEGQMw1BFRYU6d+7sPrCkMXz33Xeqrq72+zmGYdTINQ6HQw6Hw+9nS9L27dtrbNWblpamadOmefWcJk/GX375ZcD28gUAmKO4uFhdunRplGd/9913urJbW5Uc9+/4Ven8YSmnT5/2uJaTk6OZM2f6/WxJKikpUVxcnMe1uLg4lZeX13h1sD5NnowjIyMlSV/sSVBU2+Casr7jmuvNDgEATHVOZ/W+1rn/LW8M1dXVKjnu1OHCboqK9D1PlFe4dGXiFyouLvbYQz5QveJAavJkfGG4IKptmF8/ZDO0sNV9NiwAhITvVxk1xTRjVGRg8kRUVFRAD3S5WMeOHVVaWupxrbS0VFFRUV69q8+mHwAAS3IaLjn9WGLsNFyBC6YOycnJWrdunce1jRs3Kjk52avnBFfXFAAQMlwy/C7eOn36tIqKilRUVCTp/KtLRUVFOnr0qCQpOztbEyZMcN//0EMP6dChQ3rsscf06aef6i9/+Ytee+01TZ8+3at26RkDACzJJZf86dv6Unv37t0aPny4+3NWVpYkKT09XcuWLdNXX33lTsySdOWVV+rtt9/W9OnT9cILL6hLly566aWXlJaW5lW7JGMAAL43bNgw1bf9Rm27aw0bNkwffPCBX+2SjAEAluQ0DDn92JfKn7pNjWQMALAkX+d9L64fLFjABQCAyegZAwAsySVDzhDpGZOMAQCWxDA1AABoMvSMAQCWxGpqAABM5vq++FM/WDBMDQCAyegZAwAsyennamp/6jY1kjEAwJKchvw8tSlwsTQ2kjEAwJKYM76EBQsWKCEhQREREUpKStLOnTsDHRcAACHD62S8cuVKZWVlKScnR3v27FG/fv2Ulpam48ePN0Z8AIAQ5ZJNTj+KSzazv4UG8zoZz5s3T5MnT9akSZPUu3dvLVy4UK1bt9aSJUsaIz4AQIhyGf6XYOFVMq6urlZhYaFSU1N/eEBYmFJTU7V9+/Za61RVVam8vNyjAACAH3iVjE+ePCmn06m4uDiP63FxcSopKam1Tm5urqKjo90lPj7e92gBACHDnyHqCyVYNPqmH9nZ2SorK3OX4uLixm4SANAMhFIy9urVpg4dOshut6u0tNTjemlpqTp27FhrHYfDIYfD4XuEAAA0c171jMPDw5WYmKhNmza5r7lcLm3atEnJyckBDw4AELpchs3vEiy83vQjKytL6enpGjhwoAYNGqT58+ersrJSkyZNaoz4AAAhyt+h5mY7TC1J48aN04kTJ/Tkk0+qpKRE/fv31/r162ss6gIAAA3j03aYmZmZyszMDHQsAAC4ORUmpx/rjJ0BjKWxsTc1AMCSDD/nfY3mPGcMAEBTCKU540Z/zxgAANSPnjEAwJKcRpichh9zxkG0NzXJGABgSS7Z5PJjANel4MnGDFMDAGAyesYAAEsKpQVcJGMAgCX5P2fMMDUAAGggesYAAEs6v4DL96Fmf+o2NZIxAMCSXH5uh8lqagAA0GD0jAEAlhRKC7hIxgAAS3IpLGQ2/SAZAwAsyWnY5PTj5CV/6jY105JxvzfvU1hEhFnN++b/mR2Ab3pM32F2CACAetAzBgBYktPP1dROhqkBAPCPywiTy48FXK4gWsDFq00AAJiMnjEAwJIYpgYAwGQu+bci2hW4UBodw9QAAJiMnjEAwJL83/QjePqbJGMAgCX5vx1m8CTj4IkUAIBmip4xAMCSOM8YAACThdIwNckYAGBJ/r9nHDzJOHgiBQCgmaJnDACwJJdhk8ufTT84QhEAAP+4/BymDqb3jIMnUgAAmil6xgAAS/L/CMXg6W+SjAEAluSUTU4/3hX2p25TC54/GwAAaKa8TsZbt27V6NGj1blzZ9lsNq1Zs6YRwgIAhLoLw9T+lGDhdaSVlZXq16+fFixY0BjxAAAgSXLqh6Fq30rw8HrOeOTIkRo5cmRjxAIAQEhq9AVcVVVVqqqqcn8uLy9v7CYBAM1AKK2mbvRIc3NzFR0d7S7x8fGN3SQAoBm4cFCEPyVYNHqk2dnZKisrc5fi4uLGbhIA0AwY3x+h6GsxfHy1acGCBUpISFBERISSkpK0c+fOeu+fP3++rr32WrVq1Urx8fGaPn26vvvuO6/abPRhaofDIYfD0djNAADgt5UrVyorK0sLFy5UUlKS5s+fr7S0NO3fv1+xsbE17l+xYoV+85vfaMmSJRo8eLAOHDigiRMnymazad68eQ1uN3j68ACAkGLGMPW8efM0efJkTZo0Sb1799bChQvVunVrLVmypNb7t23bpiFDhuiee+5RQkKCRowYofHjx1+yN/1jXkd6+vRpFRUVqaioSJJ0+PBhFRUV6ejRo94+CgCAOl04tcmfIp1fOHxxuXhR8cWqq6tVWFio1NRU97WwsDClpqZq+/bttdYZPHiwCgsL3cn30KFDWrdunf7rv/7Lq+/V62Hq3bt3a/jw4e7PWVlZkqT09HQtW7bM28cBANCofrxwOCcnRzNnzqxx38mTJ+V0OhUXF+dxPS4uTp9++mmtz77nnnt08uRJDR06VIZh6Ny5c3rooYf029/+1qsYvU7Gw4YNk2EY3lYDAMArTj+PULxQt7i4WFFRUe7rgVzHVFBQoNmzZ+svf/mLkpKSdPDgQU2dOlVPPfWUZsyY0eDncFAEAMCSLh5q9rW+JEVFRXkk47p06NBBdrtdpaWlHtdLS0vVsWPHWuvMmDFDv/jFL3T//fdLkq6//npVVlbqgQce0O9+9zuFhTXsjwkWcAEAICk8PFyJiYnatGmT+5rL5dKmTZuUnJxca50zZ87USLh2u12SvBpFpmcMALAkl8Lk8qPP6EvdrKwspaena+DAgRo0aJDmz5+vyspKTZo0SZI0YcIEXXHFFcrNzZUkjR49WvPmzdMNN9zgHqaeMWOGRo8e7U7KDUEyBgBYktOwyenHMLUvdceNG6cTJ07oySefVElJifr376/169e7F3UdPXrUoyf8xBNPyGaz6YknntCxY8cUExOj0aNH65lnnvGqXZvRxKuxysvLFR0dra65TyssIqIpm/Zf8JxT7aHH9B1mhwCgmThnnFWB3lJZWVmD5mF9cSFPTHnvTjnatvT5OVWnzyovZVWjxhoo9IwBAJYUqAVcwYBkDACwJMPPU5uMIDoogmQMALAkp2xy+jE/6E/dphY8fzYAANBM0TMGAFiSy/Bv3tcVRJtFkowBAJbk8nPO2J+6TS14IgUAoJmiZwwAsCSXbHL5sQjLn7pNjWQMALAkM3bgMgvD1AAAmMy0nnH0AZvs4cHzV4sk2YJoZd7FTj5Y+2kjwaDDi9vNDgGASUJpARfD1AAAS3LJz+0wg2jOOHj+bAAAoJmiZwwAsCTDz9XURhD1jEnGAABL4tQmAABMFkoLuIInUgAAmil6xgAAS2KYGgAAk4XSdpgMUwMAYDJ6xgAAS2KYGgAAk4VSMmaYGgAAk9EzBgBYUij1jEnGAABLCqVkzDA1AAAmo2cMALAkQ/69KxxMR9CTjAEAlhRKw9QkYwCAJYVSMmbOGAAAk3mVjHNzc3XjjTcqMjJSsbGxGjNmjPbv399YsQEAQtiFnrE/JVh4lYy3bNmijIwM7dixQxs3btTZs2c1YsQIVVZWNlZ8AIAQFUrJ2Ks54/Xr13t8XrZsmWJjY1VYWKibbropoIEBABAq/FrAVVZWJkm67LLL6rynqqpKVVVV7s/l5eX+NAkACBGGYZPhR+/Wn7pNzecFXC6XS9OmTdOQIUPUp0+fOu/Lzc1VdHS0u8THx/vaJAAghFw4z9ifEix8TsYZGRnat2+f8vPz670vOztbZWVl7lJcXOxrkwAANEs+DVNnZmbqb3/7m7Zu3aouXbrUe6/D4ZDD4fApOABA6Aql94y9SsaGYejhhx/W6tWrVVBQoCuvvLKx4gIAhLhQmjP2KhlnZGRoxYoVeuuttxQZGamSkhJJUnR0tFq1atUoAQIA0Nx5lYzz8vIkScOGDfO4vnTpUk2cODFQMQEAwDB1XQwjmM7AAAAEM4apAQAwmeFnzziYkjEHRQAAYDJ6xgAASzIk+TM7GkwTqyRjAIAluWSTzY9dtEJiBy4AABAY9IwBAJbEamoAAEzmMmyyhch7xgxTAwBgMnrGAABLMgw/V1MH0XJqkjEAwJJCac6YYWoAAExGzxgAYEmh1DMmGQMALCmUVlOTjAEAlhRKC7iYMwYAwGT0jAEAlnS+Z+zPnHEAg2lkpiXjVl+71KKly6zmfWJzBtF/2WbizJ1JZofgk9ar/j+zQwCCXigt4GKYGgAAkzFMDQCwJEP+nUkcTGOZJGMAgCUxTA0AAJoMPWMAgDWF0Dg1PWMAgDV9P0zta5GPw9QLFixQQkKCIiIilJSUpJ07d9Z7/6lTp5SRkaFOnTrJ4XDommuu0bp167xqk54xAMCSzNiBa+XKlcrKytLChQuVlJSk+fPnKy0tTfv371dsbGyN+6urq3XbbbcpNjZWb7zxhq644gp98cUXateunVftkowBAPjevHnzNHnyZE2aNEmStHDhQr399ttasmSJfvOb39S4f8mSJfrmm2+0bds2tWzZUpKUkJDgdbsMUwMALMmfIeqLV2KXl5d7lKqqqlrbq66uVmFhoVJTU93XwsLClJqaqu3bt9daZ+3atUpOTlZGRobi4uLUp08fzZ49W06n06vvlWQMALCmC/O+/hRJ8fHxio6Odpfc3Nxamzt58qScTqfi4uI8rsfFxamkpKTWOocOHdIbb7whp9OpdevWacaMGZo7d66efvppr75VhqkBAM1acXGxoqKi3J8dDkfAnu1yuRQbG6tFixbJbrcrMTFRx44d0x/+8Afl5OQ0+DkkYwCAJQVqAVdUVJRHMq5Lhw4dZLfbVVpa6nG9tLRUHTt2rLVOp06d1LJlS9ntdve1Xr16qaSkRNXV1QoPD29QrAxTAwCsyQhA8UJ4eLgSExO1adMm9zWXy6VNmzYpOTm51jpDhgzRwYMH5XL9cPDRgQMH1KlTpwYnYolkDACAW1ZWlhYvXqxXXnlFn3zyiaZMmaLKykr36uoJEyYoOzvbff+UKVP0zTffaOrUqTpw4IDefvttzZ49WxkZGV61yzA1AMCSzNibety4cTpx4oSefPJJlZSUqH///lq/fr17UdfRo0cVFvZDPzY+Pl4bNmzQ9OnT1bdvX11xxRWaOnWqHn/8ca/aJRkDAKzLhC0tMzMzlZmZWevXCgoKalxLTk7Wjh07/GqTYWoAAExGzxgAYEmhdIQiyRgAYE2c2lS7vLw89e3b1/3OVnJyst55553Gig0AENJsASjBwatk3KVLF82ZM0eFhYXavXu3brnlFv30pz/VRx991FjxAQDQ7Hk1TD169GiPz88884zy8vK0Y8cOXXfddbXWqaqq8tiUu7y83IcwAQAhh2HqS3M6ncrPz1dlZWWdO5NIUm5urscG3fHx8b42CQAIJU28A5eZvE7Ge/fuVdu2beVwOPTQQw9p9erV6t27d533Z2dnq6yszF2Ki4v9ChgAgObG69XU1157rYqKilRWVqY33nhD6enp2rJlS50J2eFwBPSEDABAiLjoGESf6wcJr5NxeHi4evToIUlKTEzUrl279MILL+jFF18MeHAAgNAVqFObgoHfO3C5XC6PBVoAAMA7XvWMs7OzNXLkSHXt2lUVFRVasWKFCgoKtGHDhsaKDwAQqkJoNbVXyfj48eOaMGGCvvrqK0VHR6tv377asGGDbrvttsaKDwAQqpgzrt3LL7/cWHEAABCy2JsaAGBJNuN88ad+sCAZAwCsiTljAABMFkJzxn6/2gQAAPxDzxgAYE0MUwMAYLIQSsYMUwMAYDJ6xgAAawqhnjHJGABgTaymBgAATYWeMQDAktiBCwAAs4XQnDHD1AAAmIxkDACAyRimBgBYkk1+zhkHLJLGZ1oydpw6qxYt7GY175sgmn+4mBEWTL+SnowWwRm7c/gAs0PwiX3zHrNDAH7Aq00AAKCpMEwNALCmEFpNTTIGAFhTCCVjhqkBADAZPWMAgCWxAxcAAGZjmBoAADQVesYAAGsKoZ4xyRgAYEmhNGfMMDUAACajZwwAsKYQ2g6TZAwAsCbmjAEAMBdzxgAAoMnQMwYAWBPD1AAAmMzPYepgSsYMUwMAYDJ6xgAAa2KYGgAAk4VQMvZrmHrOnDmy2WyaNm1agMIBACD0+Nwz3rVrl1588UX17ds3kPEAACCJ94wv6fTp07r33nu1ePFitW/fPtAxAQAQUnxKxhkZGRo1apRSU1MveW9VVZXKy8s9CgAA+IHXw9T5+fnas2ePdu3a1aD7c3NzNWvWLK8DAwCEOBZw1a64uFhTp07V8uXLFRER0aA62dnZKisrc5fi4mKfAgUAhJYLc8b+lGDhVc+4sLBQx48f14ABA9zXnE6ntm7dqj//+c+qqqqS3W73qONwOORwOAITLQAgtARRQvWHV8n41ltv1d69ez2uTZo0ST179tTjjz9eIxEDAIBL8yoZR0ZGqk+fPh7X2rRpo8svv7zGdQAA/BJCc8bswAUAsKRQes/Y72RcUFAQgDAAAAhd9IwBANbEMDUAAOYKpWFqzjMGAMBkJGMAgDUZASg+WLBggRISEhQREaGkpCTt3LmzQfXy8/Nls9k0ZswYr9skGQMArMmEZLxy5UplZWUpJydHe/bsUb9+/ZSWlqbjx4/XW+/IkSN69NFHlZKS4n2jIhkDAJq5Hx9WVFVVVee98+bN0+TJkzVp0iT17t1bCxcuVOvWrbVkyZI66zidTt17772aNWuWrrrqKp9iJBkDACwpUHtTx8fHKzo62l1yc3Nrba+6ulqFhYUeJxKGhYUpNTVV27dvrzPO3//+94qNjdUvf/lLn79XVlMDAKwpQK82FRcXKyoqyn25rvMSTp48KafTqbi4OI/rcXFx+vTTT2ut8/777+vll19WUVGRH4GSjAEAVhWgZBwVFeWRjAOloqJCv/jFL7R48WJ16NDBr2eRjAEAkNShQwfZ7XaVlpZ6XC8tLVXHjh1r3P/555/ryJEjGj16tPuay+WSJLVo0UL79+9X9+7dG9Q2c8YAAEtq6vOMw8PDlZiYqE2bNrmvuVwubdq0ScnJyTXu79mzp/bu3auioiJ3+clPfqLhw4erqKhI8fHxDW6bnjEAwJpM2A4zKytL6enpGjhwoAYNGqT58+ersrJSkyZNkiRNmDBBV1xxhXJzcxUREVHjxMJ27dpJktcnGZKMAQD43rhx43TixAk9+eSTKikpUf/+/bV+/Xr3oq6jR48qLCzwg8okYwCAJZm1N3VmZqYyMzNr/dqlTipctmyZT22SjAEA1sSpTY3P/u052VucM6t537iC6L/sxcJsZkfgMyNIY7edc5kdgm/+o6/ZEfhmx7/MjgDwCz1jAIA10TMGAMBctu+LP/WDBe8ZAwBgMnrGAABrYpgaAABzmfVqkxlIxgAAawqhnjFzxgAAmIyeMQDAuoKod+sPkjEAwJJCac6YYWoAAExGzxgAYE0htICLZAwAsCSGqQEAQJOhZwwAsCaGqQEAMBfD1AAAoMnQMwYAWBPD1AAAmIxkDACAuZgzrsPMmTNls9k8Ss+ePRsrNgAAQoLXPePrrrtO77777g8PaEHnGgDQCBimrqdCixbq2LFjY8QCAICbzTBkM3zPqP7UbWpev9r02WefqXPnzrrqqqt077336ujRo/XeX1VVpfLyco8CAAB+4FUyTkpK0rJly7R+/Xrl5eXp8OHDSklJUUVFRZ11cnNzFR0d7S7x8fF+Bw0ACAFGAEqQ8CoZjxw5UmPHjlXfvn2VlpamdevW6dSpU3rttdfqrJOdna2ysjJ3KS4u9jtoAEDzd2E1tT8lWPi1+qpdu3a65pprdPDgwTrvcTgccjgc/jQDAECz5td2mKdPn9bnn3+uTp06BSoeAADOY5i6do8++qi2bNmiI0eOaNu2bbrjjjtkt9s1fvz4xooPABCiGKauw//93/9p/Pjx+vrrrxUTE6OhQ4dqx44diomJaaz4AABo9rxKxvn5+Y0VBwAAntj0AwAAc4XS3tQkYwCANYVQz9iv1dQAAMB/9IwBAJYVTEPN/iAZAwCsyTDOF3/qBwmGqQEAMBk9YwCAJbGaGgAAs7GaGgAANBV6xgAAS7K5zhd/6gcLkjEAwJoYpgYAAE2FnjEAwJJYTQ0AgNlCaNMPkjEAwJLoGTeBsO/OKcx+1qzmfWOzmR2BT4wgjVuSbEG6qsF2LoiWcTYH/XubHYHPXEUfmx0CLICeMQDAmkJoNTXJGABgSaE0TB2kg4AAADQf9IwBANbEamoAAMzFMDUAAGgy9IwBANbEamoAAMzFMDUAAGgy9IwBANbkMs4Xf+oHCZIxAMCamDMGAMBcNvk5ZxywSBofc8YAAJiMnjEAwJrYgQsAAHPxahMAACFqwYIFSkhIUEREhJKSkrRz58467128eLFSUlLUvn17tW/fXqmpqfXeXxeSMQDAmowAFC+tXLlSWVlZysnJ0Z49e9SvXz+lpaXp+PHjtd5fUFCg8ePHa/Pmzdq+fbvi4+M1YsQIHTt2zKt2ScYAAEuyGYbfRZLKy8s9SlVVVZ1tzps3T5MnT9akSZPUu3dvLVy4UK1bt9aSJUtqvX/58uX61a9+pf79+6tnz5566aWX5HK5tGnTJq++V5IxAKBZi4+PV3R0tLvk5ubWel91dbUKCwuVmprqvhYWFqbU1FRt3769QW2dOXNGZ8+e1WWXXeZVjF4n42PHjunnP/+5Lr/8crVq1UrXX3+9du/e7e1jAAConysARVJxcbHKysrcJTs7u9bmTp48KafTqbi4OI/rcXFxKikpaVDIjz/+uDp37uyR0BvCq9XU//73vzVkyBANHz5c77zzjmJiYvTZZ5+pffv2XjUKAMClXDzU7Gt9SYqKilJUVFSgwqrTnDlzlJ+fr4KCAkVERHhV16tk/Oyzzyo+Pl5Lly51X7vyyiu9ahAAACvq0KGD7Ha7SktLPa6XlpaqY8eO9dZ9/vnnNWfOHL377rvq27ev1217NUy9du1aDRw4UGPHjlVsbKxuuOEGLV68uN46VVVVNSbPAQC4pCZeTR0eHq7ExESPxVcXFmMlJyfXWe+5557TU089pfXr12vgwIHeNfo9r5LxoUOHlJeXp6uvvlobNmzQlClT9Mgjj+iVV16ps05ubq7HxHl8fLxPgQIAQsyFHbj8KV7KysrS4sWL9corr+iTTz7RlClTVFlZqUmTJkmSJkyY4DHn/Oyzz2rGjBlasmSJEhISVFJSopKSEp0+fdqrdr0apna5XBo4cKBmz54tSbrhhhu0b98+LVy4UOnp6bXWyc7OVlZWlvtzeXk5CRkAcElm7MA1btw4nThxQk8++aRKSkrUv39/rV+/3r2o6+jRowoL+6Efm5eXp+rqat11110ez8nJydHMmTMb3K5XybhTp07q3bu3x7VevXrpzTffrLOOw+GQw+HwphkAAEyTmZmpzMzMWr9WUFDg8fnIkSMBadOrZDxkyBDt37/f49qBAwfUrVu3gAQDAIAbB0XUbvr06Ro8eLBmz56tu+++Wzt37tSiRYu0aNGixooPABCibK7zxZ/6wcKrBVw33nijVq9erf/93/9Vnz599NRTT2n+/Pm69957Gys+AACaPa+PULz99tt1++23N0YsAAD8gGFqAABM5uPJSx71gwQHRQAAYDJ6xgAASwrU3tTBgGQMALCmEJozZpgaAACT0TMGAFiTIfeZxD7XDxIkYwCAJTFnDACA2Qz5OWccsEgaHXPGAACYjJ4xAMCaQmg1NckYAGBNLkk2P+sHCYapAQAwGT1jAIAlsZoaAACzhdCcMcPUAACYjJ4xAMCaQqhnbFoytp11yuZymtV8aAljAKSp2Zz8bjepIP4dt193rdkheMVwVkmfNFVjoZOMg/c3GACAZoJhagCANYXQe8YkYwCAJfFqEwAAZmPOGAAANBV6xgAAa3IZks2P3q0reHrGJGMAgDUxTA0AAJoKPWMAgEX52TNW8PSMScYAAGtimBoAADQVesYAAGtyGfJrqJnV1AAA+MlwnS/+1A8SDFMDAGAyesYAAGsKoQVcJGMAgDUxZwwAgMlCqGfMnDEAACbzKhknJCTIZrPVKBkZGY0VHwAgVBn6oXfsUzH7G2g4r4apd+3aJafT6f68b98+3XbbbRo7dmzAAwMAhLgQGqb2KhnHxMR4fJ4zZ466d++um2++OaBBAQAQSnxewFVdXa1XX31VWVlZstlsdd5XVVWlqqoq9+fy8nJfmwQAhBKXS5IfG3e4QmDTjzVr1ujUqVOaOHFivffl5uYqOjraXeLj431tEgAQSvyaL/b3xKem5XMyfvnllzVy5Eh17ty53vuys7NVVlbmLsXFxb42CQBAs+TTMPUXX3yhd999V6tWrbrkvQ6HQw6Hw5dmAAChjAVc9Vu6dKliY2M1atSoQMcDAMB5IbQDl9fD1C6XS0uXLlV6erpatGADLwAA/OV1Nn333Xd19OhR3XfffY0RDwAAkiTDcMnw4xhEf+o2Na+T8YgRI2QE0Tg8ACBIGYZ/Q81BlKsYZwYAWJPh55xxECVjDooAAMBk9IwBANbkckk2P+Z9m/OcMQAATYJhagAA0FToGQMALMlwuWT4MUzdrF9tAgCgSTBMDQAAmgo9YwCANbkMyRYaPWOSMQDAmgxDkj+vNgVPMmaYGgAAk9EzBgBYkuEyZPgxTB1M5yjQMwYAWJPh8r/4YMGCBUpISFBERISSkpK0c+fOeu9//fXX1bNnT0VEROj666/XunXrvG6TZAwAsCTDZfhdvLVy5UplZWUpJydHe/bsUb9+/ZSWlqbjx4/Xev+2bds0fvx4/fKXv9QHH3ygMWPGaMyYMdq3b59X7ZKMAQD43rx58zR58mRNmjRJvXv31sKFC9W6dWstWbKk1vtfeOEF/ed//qd+/etfq1evXnrqqac0YMAA/fnPf/aq3SafM74whn/OWdXUTYcsw+BvrqZmczrNDiG08DveZC78290U87HnjCq/Dns4p7OSpPLyco/rDodDDoejxv3V1dUqLCxUdna2+1pYWJhSU1O1ffv2WtvYvn27srKyPK6lpaVpzZo1XsXa5Mm4oqJCkrTlM+/+agAAWEdFRYWio6Mb5dnh4eHq2LGj3i/xfu71x9q2bav4+HiPazk5OZo5c2aNe0+ePCmn06m4uDiP63Fxcfr0009rfX5JSUmt95eUlHgVZ5Mn486dO6u4uFiRkZGy2WwBfXZ5ebni4+NVXFysqKiogD67MRF30yLuphessRN3TYZhqKKiQp07dw7ocy8WERGhw4cPq7q62u9nGYZRI9fU1is2W5Mn47CwMHXp0qVR24iKigqq/+NcQNxNi7ibXrDGTtyeGqtHfLGIiAhFREQ0ejsX69Chg+x2u0pLSz2ul5aWqmPHjrXW6dixo1f314WJFgAAdH54PDExUZs2bXJfc7lc2rRpk5KTk2utk5yc7HG/JG3cuLHO++vCph8AAHwvKytL6enpGjhwoAYNGqT58+ersrJSkyZNkiRNmDBBV1xxhXJzcyVJU6dO1c0336y5c+dq1KhRys/P1+7du7Vo0SKv2m1WydjhcCgnJ8eS8wH1Ie6mRdxNL1hjJ+7QM27cOJ04cUJPPvmkSkpK1L9/f61fv969SOvo0aMKC/thUHnw4MFasWKFnnjiCf32t7/V1VdfrTVr1qhPnz5etWszgmm/MAAAmiHmjAEAMBnJGAAAk5GMAQAwGckYAACTkYwBADBZs0nG3p4/aQVbt27V6NGj1blzZ9lsNq83FjdLbm6ubrzxRkVGRio2NlZjxozR/v37zQ7rkvLy8tS3b1/3rkTJycl65513zA7La3PmzJHNZtO0adPMDqVeM2fOlM1m8yg9e/Y0O6wGOXbsmH7+85/r8ssvV6tWrXT99ddr9+7dZod1SQkJCTV+5jabTRkZGWaHhktoFsnY2/MnraKyslL9+vXTggULzA7FK1u2bFFGRoZ27NihjRs36uzZsxoxYoQqKyvNDq1eXbp00Zw5c1RYWKjdu3frlltu0U9/+lN99NFHZofWYLt27dKLL76ovn37mh1Kg1x33XX66quv3OX99983O6RL+ve//60hQ4aoZcuWeuedd/Txxx9r7ty5at++vdmhXdKuXbs8ft4bN26UJI0dO9bkyHBJRjMwaNAgIyMjw/3Z6XQanTt3NnJzc02MyjuSjNWrV5sdhk+OHz9uSDK2bNlidihea9++vfHSSy+ZHUaDVFRUGFdffbWxceNG4+abbzamTp1qdkj1ysnJMfr162d2GF57/PHHjaFDh5odRkBMnTrV6N69u+FyucwOBZcQ9D3jC+dPpqamuq9d6vxJBFZZWZkk6bLLLjM5koZzOp3Kz89XZWWl13vImiUjI0OjRo3y+F23us8++0ydO3fWVVddpXvvvVdHjx41O6RLWrt2rQYOHKixY8cqNjZWN9xwgxYvXmx2WF6rrq7Wq6++qvvuuy/gJ+Qh8II+Gdd3/qS350nCey6XS9OmTdOQIUO83v7NDHv37lXbtm3lcDj00EMPafXq1erdu7fZYV1Sfn6+9uzZ494PNxgkJSVp2bJlWr9+vfLy8nT48GGlpKS4zzS3qkOHDikvL09XX321NmzYoClTpuiRRx7RK6+8YnZoXlmzZo1OnTqliRMnmh0KGqBZ7U2NppeRkaF9+/YFxVygJF177bUqKipSWVmZ3njjDaWnp2vLli2WTsjFxcWaOnWqNm7c2ORHyvlj5MiR7v/dt29fJSUlqVu3bnrttdf0y1/+0sTI6udyuTRw4EDNnj1bknTDDTdo3759WrhwodLT002OruFefvlljRw5slHPHUbgBH3P2JfzJxEYmZmZ+tvf/qbNmzc3+hnVgRIeHq4ePXooMTFRubm56tevn1544QWzw6pXYWGhjh8/rgEDBqhFixZq0aKFtmzZoj/+8Y9q0aKFnE6n2SE2SLt27XTNNdfo4MGDZodSr06dOtX446xXr15BMcR+wRdffKF3331X999/v9mhoIGCPhn7cv4k/GMYhjIzM7V69Wr94x//0JVXXml2SD5zuVyqqqoyO4x63Xrrrdq7d6+KiorcZeDAgbr33ntVVFQku91udogNcvr0aX3++efq1KmT2aHUa8iQITVe1Ttw4IC6detmUkTeW7p0qWJjYzVq1CizQ0EDNYth6kudP2lVp0+f9uglHD58WEVFRbrsssvUtWtXEyOrX0ZGhlasWKG33npLkZGR7rn56OhotWrVyuTo6padna2RI0eqa9euqqio0IoVK1RQUKANGzaYHVq9IiMja8zHt2nTRpdffrml5+kfffRRjR49Wt26ddOXX36pnJwc2e12jR8/3uzQ6jV9+nQNHjxYs2fP1t13362dO3dq0aJFXp9PaxaXy6WlS5cqPT1dLVo0i3/iQ4PZy7kD5U9/+pPRtWtXIzw83Bg0aJCxY8cOs0O6pM2bNxuSapT09HSzQ6tXbTFLMpYuXWp2aPW67777jG7duhnh4eFGTEyMceuttxp///vfzQ7LJ8HwatO4ceOMTp06GeHh4cYVV1xhjBs3zjh48KDZYTXIX//6V6NPnz6Gw+EwevbsaSxatMjskBpsw4YNhiRj//79ZocCL3CeMQAAJgv6OWMAAIIdyRgAAJORjAEAMBnJGAAAk5GMAQAwGckYAACTkYwBADAZyRgAAJORjAEAMBnJGAAAk5GMAQAw2f8P+7G76BQrqxkAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "B, T, C = 1, 8, 5\n",
    "ic(B, T, C)\n",
    "# the input after token embedding lookup\n",
    "x = torch.rand(B, T, C)\n",
    "\n",
    "head_size = 5\n",
    "key = nn.Linear(C, head_size)\n",
    "k = key(x)\n",
    "query = nn.Linear(C, head_size)\n",
    "q = query(x)\n",
    "value = nn.Linear(C, head_size)\n",
    "v = value(x)\n",
    "# v = torch.full((B,T,C), fill_value=1.)\n",
    "\n",
    "# dot product as a similarity measure\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ v\n",
    "ic(wei.shape, v.shape, out.shape)\n",
    "ic(wei[0])\n",
    "\n",
    "plt.imshow(wei[0].detach())\n",
    "plt.colorbar()\n",
    "plt.title(\"Attention weight\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# first token out value should be the same as first value\n",
    "assert all(out[0][0] == v[0][0])\n",
    "# then it's no more the case\n",
    "assert not all(out[0][3] == v[0][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7146, -1.3507, -0.3433, -0.1324, -0.6624],\n",
       "         [-0.7258, -1.1921, -0.3642, -0.1794, -0.5712],\n",
       "         [-0.7042, -1.2338, -0.3153, -0.2440, -0.5862],\n",
       "         [-0.6235, -1.1861, -0.3129, -0.1985, -0.5715],\n",
       "         [-0.6094, -1.1152, -0.3548, -0.1481, -0.5481],\n",
       "         [-0.5886, -1.0363, -0.3631, -0.0948, -0.5108],\n",
       "         [-0.6097, -1.0338, -0.3311, -0.1276, -0.4962],\n",
       "         [-0.6139, -1.0518, -0.3069, -0.1462, -0.4956]]],\n",
       "       grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let write torch modules and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def unidirectional_mask(seq_len: int) -> torch.Tensor:\n",
    "    # inverse_mask = torch.triu(torch.ones((1, seq_len, seq_len)), diagonal=1)  # .type(torch.uint8)\n",
    "    # inverse_mask = torch.tril(torch.ones((T, T)))\n",
    "    # ic(inverse_mask)\n",
    "    # tmask = inverse_mask == 0\n",
    "    # ic(tmask)\n",
    "    mask = torch.tril(torch.ones((1, seq_len, seq_len)))\n",
    "    ic(mask)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| mask: tensor([[[1., 0., 0., 0., 0.],\n",
      "                   [1., 1., 0., 0., 0.],\n",
      "                   [1., 1., 1., 0., 0.],\n",
      "                   [1., 1., 1., 1., 0.],\n",
      "                   [1., 1., 1., 1., 1.]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ic.disable()\n",
    "ic.enable()\n",
    "mask = unidirectional_mask(5)\n",
    "\n",
    "plt.imshow(mask[0])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: Optional[torch.Tensor] = None,\n",
    "    softmax: torch.nn.Module = nn.Softmax(dim=-1),\n",
    "    dropout: Optional[torch.nn.Module] = None,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"compute attention weigths and attention weights applied to value tensor.\n",
    "\n",
    "    Arguments:\n",
    "        query -- query tensor in batch_size, head_nb, seq_len, d_k shape\n",
    "        key -- same shape structure as query\n",
    "        value -- same shape structure as query\n",
    "\n",
    "    Keyword Arguments:\n",
    "        mask -- mask of tokens (default: {None})\n",
    "        softmax -- softmax module (default: {nn.Softmax(dim=-1)})\n",
    "        dropout -- dropout ratio (default: {None})\n",
    "\n",
    "    Returns:\n",
    "        attention -- attention weight applied to value\n",
    "        attn-weights\n",
    "    \"\"\"\n",
    "    # d_k is the size of atttention per head (=d//nb heads, where d is the size of attn model)\n",
    "    # IT SHOULD BE, batch_size, head_nb, seq_len , d_k\n",
    "    batch_size, head_nb, seq_len, d_k = query.shape\n",
    "\n",
    "    scores = (query @ key.transpose(-2, -1)) * (d_k**-0.5)\n",
    "    if mask is not None:\n",
    "        # mask scores with -inf when mask is False. If we masked_fill(mask, -inf), it would mask when mask ==1 (is true). We want to mask when it's 0 (false).\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    attn_weights = softmax(scores)  # , dim=-1)\n",
    "    if dropout is not None:\n",
    "        attn_weights = dropout(attn_weights)\n",
    "\n",
    "    atn = attn_weights @ value\n",
    "    # ic(atn.shape, attn_weights.shape, value.shape)\n",
    "    assert (\n",
    "        atn.shape == query.shape\n",
    "    ), f\"atn shape {atn.shape} should be the same as input tensors key, query and value shapes. Query shape: {query.shape}\"\n",
    "    return atn, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 4\n",
    "embedding_dim = 5\n",
    "h = 1  # nb of heads\n",
    "d_k = embedding_dim // h\n",
    "batch_size = 1\n",
    "# shape is batch, h, seq_len, d_k\n",
    "Q = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "K = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "V = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "mask = unidirectional_mask(seq_len).to(device)\n",
    "\n",
    "ic.disable()\n",
    "ic.enable()\n",
    "attn, attn_weight = attention(Q, K, V, mask=mask, softmax=torch.nn.Softmax(dim=-1), dropout=torch.nn.Dropout(0.0))\n",
    "attn_no_mask, attn_weight_no_mask = attention(Q, K, V, dropout=torch.nn.Dropout(0.0))\n",
    "sum_attn_w = attn_weight.sum(dim=-1)\n",
    "assert torch.isclose(sum_attn_w, torch.ones_like(sum_attn_w), atol=0.001).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention viz\n",
    "\n",
    "we see that masked attention weight is a matrix where upper diagonal is 0. But attention (attention weight applied to value), is a weighted sum of values at each time step. It takes only current and previous steps (so values are not masked, only the weights are masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_matrix(m: torch.Tensor, title: str = \"Matrix\"):\n",
    "    np_m = m.cpu().detach().numpy()\n",
    "\n",
    "    plt.imshow(np_m)\n",
    "    plt.colorbar()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "display_matrix(attn[0][0], title=\"Attention WITH mask\")\n",
    "display_matrix(attn_weight[0][0], title=\"Attention weight WITH mask\")\n",
    "\n",
    "display_matrix(attn_no_mask[0][0], title=\"Attention NO mask\")\n",
    "display_matrix(attn_weight_no_mask[0][0], title=\"Attention weight NO mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multihead attention module as defined in Formal algorithm for transformers (https://arxiv.org/abs/2207.09238)\n",
    "    It can be used for different attention architectures like encoder-decoder/seq-to-seq (very first transformer),\n",
    "    encoder-only (bert), decoder-only (gpt-*, gopher).\n",
    "\n",
    "    It splits weights into h heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d: int, h: int, dropout: float = 0.0, bias: bool = True):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Arguments:\n",
    "            d -- dimension of hidden state (aka model dimension)\n",
    "            h -- nb of heads\n",
    "\n",
    "        Keyword Arguments:\n",
    "            dropout -- dropout rate (default: {0.0})\n",
    "            bias -- do we include bias in linear computations (default: {True})\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        assert (\n",
    "            d % h == 0\n",
    "        ), f\"Model dim {d} must be a multiple of nb of heads {h}. d_k {d//h} is the model dim per head, because we split by nb of heads.\"\n",
    "        self.d_k = d // h  # model dim on one head.\n",
    "        self.wq = nn.Linear(d, d, bias=bias)\n",
    "        self.wk = nn.Linear(d, d, bias=bias)\n",
    "        self.wv = nn.Linear(d, d, bias=bias)\n",
    "        self.wo = nn.Linear(d, d, bias=bias)  # linear projection of output matrix.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, z: torch.Tensor, mask: Optional[torch.Tensor] = None\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): primary sequence\n",
    "            z (torch.Tensor): context sequence, only for encoder-decoder architecture.\n",
    "            mask (torch.Tensor, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len, d = x.size()\n",
    "        if z is None:\n",
    "            z = x\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(z)\n",
    "        v = self.wk(z)\n",
    "        # shape has to be batch, h, seq_len, d_k (d//h)\n",
    "        # first view to : batch,seq_len,h,d_k , then transpose h and seq_len so we got per head q,k,v\n",
    "        q = q.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
    "        k = k.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
    "        v = v.view(batch_size, seq_len, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # attention has to be done on each head.\n",
    "        mh_attn, mh_attn_weight = attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "        assert mh_attn_weight.shape == (batch_size, self.h, seq_len, seq_len)\n",
    "        # we need to create a contiguous memory space for tensor after transpose so we can apply view.\n",
    "        concat_attn = mh_attn.transpose(2, 1).contiguous().view(batch_size, seq_len, self.h * self.d_k)\n",
    "        concat_attn_weight = torch.sum(mh_attn_weight, dim=1)\n",
    "        assert concat_attn.size() == (batch_size, seq_len, d)\n",
    "        # apply linear layer on concatenated attention.\n",
    "        output = self.dropout(self.wo(concat_attn))\n",
    "        return output, concat_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "primary_seq = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "ctx_seq = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "mh = MultiHeadAttention(d=embedding_dim, h=5, dropout=0.0, bias=False).to(device)\n",
    "mh_attn, mh_attn_weight = mh(primary_seq, ctx_seq, mask=mask)\n",
    "\n",
    "display_matrix(mh_attn[0], title=\"Attention output\")\n",
    "display_matrix(mh_attn_weight[0], title=\"Attention weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with multihead_attn = nn.MultiheadAttention(embed_dim, num_heads) so that result is the same.\n",
    "# the graph output should look the same, even if they are not exactly the same due to different initialization\n",
    "nn_mh = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=5, dropout=0.0, batch_first=True).to(device)\n",
    "nn_mh_atn, nn_mh_atn_weights = nn_mh(primary_seq, ctx_seq, ctx_seq, attn_mask=mask[0] == 0)\n",
    "\n",
    "\n",
    "display_matrix(nn_mh_atn[0], title=\"Attention output\")\n",
    "display_matrix(nn_mh_atn_weights[0], title=\"Attention weight\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d: int, eps: float = 1e-05, use_torch_implem: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.eps = eps\n",
    "        self.use_torch_implem = use_torch_implem\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(d, dtype=torch.float))  # scale\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(d, dtype=torch.float))  # offset\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_torch_implem:\n",
    "            # we normalize across features for each example and seq item.\n",
    "            x_hat = F.layer_norm(x, normalized_shape=[self.d], weight=self.gamma, bias=self.beta)\n",
    "        else:\n",
    "            # normalization across features (independently) for each sample. We compute mean and var on the last 2 axis, so we have it per sampel\n",
    "            mean = x.mean((-1), keepdim=True)  # .unsqueeze(-1)\n",
    "            var = x.var((-1), keepdim=True)  # .unsqueeze(-1)\n",
    "            x_hat = torch.mul(((x - mean) / torch.sqrt(var + self.eps)), self.gamma) + self.beta\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check implement is close to pytorch one.\n",
    "torch.manual_seed(0)\n",
    "x = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "x = torch.tensor([[[2, 1, 1, 1, 1], [2, 2, 2, 2, 1]]], dtype=torch.float).to(device)\n",
    "layer_norm = LayerNormalization(d=embedding_dim, eps=1e-05, use_torch_implem=False).to(device)\n",
    "x_hat = layer_norm(x)\n",
    "\n",
    "torch_x_hat = F.layer_norm(\n",
    "    x, normalized_shape=[embedding_dim], weight=torch.ones(embedding_dim, dtype=torch.float, device=device), eps=1e-05\n",
    ")\n",
    "assert torch.allclose(x_hat, torch_x_hat, rtol=0.15, atol=0.0), \"my implementation and torch should be close\"\n",
    "x_hat, torch_x_hat"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder transformer\n",
    "\n",
    "Architecture used by GPT-*, Gopher, where it want to predict the next token given previous ones. Uses a mask to make attention causal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_size: int, nb_heads: int = 1, dropout: float = 0.0, bias: bool = True, mlp_factor=4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(model_size)  # LayerNormalization(model_size)\n",
    "        self.attn = MultiHeadAttention(d=model_size, h=nb_heads, dropout=dropout, bias=bias)\n",
    "        self.mlp1 = nn.Linear(model_size, mlp_factor * model_size, bias=bias)\n",
    "        self.mlp2 = nn.Linear(mlp_factor * model_size, model_size, bias=bias)\n",
    "        self.activation = torch.nn.GELU()\n",
    "        self.layer_norm2 = nn.LayerNorm(model_size)  # LayerNormalization(model_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        b, l, d = x.size()\n",
    "        norm_x = self.layer_norm1(x)\n",
    "        attn_x = x + self.attn(norm_x, z=None, mask=mask)[0]\n",
    "\n",
    "        norm_attn_x = self.layer_norm2(attn_x)\n",
    "        lin1 = self.activation(self.mlp1(norm_attn_x))\n",
    "        x = x + self.dropout(self.mlp2(lin1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "dl = DecoderLayer(embedding_dim, nb_heads=1, dropout=0.1).to(device)\n",
    "mask = unidirectional_mask(seq_len=seq_len).to(device)\n",
    "dlo = dl(x, mask=mask)\n",
    "\n",
    "plt.imshow(x[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(dlo[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        model_size: int,\n",
    "        nb_heads: int = 1,\n",
    "        nb_layers: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embedding_dim=model_size)\n",
    "        self.pos_enc = PositionalEncoder(\n",
    "            max_seq_len=max_seq_len, embedding_dim=model_size, dropout=dropout, is_learned=True\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                DecoderLayer(model_size=model_size, nb_heads=nb_heads, dropout=dropout, bias=bias)\n",
    "                for i in range(nb_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = LayerNormalization(model_size)\n",
    "        self.unembedding = nn.Linear(model_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, l = x.size()\n",
    "        emb = self.tok_emb(x)\n",
    "        x = self.pos_enc(emb)\n",
    "        mask = unidirectional_mask(seq_len=l).to(x.device)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.unembedding(x)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, x: torch.Tensor, max_new_tokens: int):\n",
    "        for i in range(max_new_tokens):\n",
    "            # we take at most max_seq_len tokens\n",
    "            x_block = x[:, -self.max_seq_len :]\n",
    "            logits = self(x_block)\n",
    "            # we take the logit for last token, used to predict token.\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            tok_next = torch.multinomial(probs, num_samples=1)\n",
    "            x = torch.cat((x, tok_next), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(model: nn.Module):\n",
    "    print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "\n",
    "\n",
    "dec = DecoderTransformer(vocab_size=6, max_seq_len=5, model_size=10, nb_heads=1).to(device)\n",
    "print_model_size(dec)\n",
    "x = torch.tensor([[0, 1, 2, 3, 4], [1, 2, 3, 4, 5]], dtype=torch.int, device=device)\n",
    "y_hat = dec(x)\n",
    "y_hat.shape\n",
    "\n",
    "\n",
    "target = torch.rand(*y_hat.shape).to(device)\n",
    "ic(target.shape, y_hat.shape)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss = loss(y_hat, target)\n",
    "loss.retain_grad()\n",
    "y_hat.retain_grad()\n",
    "loss\n",
    "\n",
    "\n",
    "input = torch.zeros((3, 1), dtype=torch.long, device=device)\n",
    "dec.generate(input, max_new_tokens=16)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../dataset/shakespeare.txt\") as f:\n",
    "#    text = f.read()\n",
    "\n",
    "df = pd.read_csv(\"../dataset/bob_dylan_lyrics.csv\")\n",
    "text = \"\\n\\n\".join(df.lyrics.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(text: str, vocab_size: int = 1000) -> Tokenizer:\n",
    "    tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "    trainer = trainers.BpeTrainer(vocab_size=vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\"])  # pad is 0, unk is 1\n",
    "    tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "    tokenizer.decoder = decoders.ByteLevel()\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "vocab_size = 1000\n",
    "tokenizer = train_tokenizer(text, vocab_size=vocab_size)\n",
    "sent_to_encode = \"How does it feel to be on your own !\"\n",
    "encodings = tokenizer.encode(sent_to_encode)\n",
    "encodings.ids, tokenizer.decode(encodings.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEDataset(Dataset):\n",
    "    def __init__(self, text: str, mode: str = \"train\", seq_len: int = 20, vocab_size: int = 1000, device: str = \"cpu\"):\n",
    "        assert mode in [\"train\", \"test\"], \"Mode must be 'train' or 'test'\"\n",
    "        self.mode = mode\n",
    "        self.device = device\n",
    "        self.seq_len = seq_len\n",
    "        self.text = text\n",
    "\n",
    "        words = text.split(\" \")\n",
    "        train_pos = math.ceil(len(words) * 0.8)\n",
    "        train_words = words[:train_pos]\n",
    "        test_words = words[train_pos:]\n",
    "        self.train_txt = \" \".join(train_words[:])\n",
    "        self.test_txt = \" \".join(test_words[:])\n",
    "        self.tokenizer = train_tokenizer(self.train_txt, vocab_size=vocab_size)\n",
    "        train_encoding = self.tokenizer.encode(self.train_txt)\n",
    "        test_encoding = self.tokenizer.encode(self.test_txt)\n",
    "        self.train_encoded = torch.tensor(train_encoding.ids, dtype=torch.long, device=device)\n",
    "        self.test_encoded = torch.tensor(test_encoding.ids, dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            data = self.train_encoded\n",
    "        else:\n",
    "            data = self.test_encoded\n",
    "        return len(data) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"train\":\n",
    "            data = self.train_encoded\n",
    "        else:\n",
    "            data = self.test_encoded\n",
    "\n",
    "        data_len = data.shape[0]\n",
    "        i = idx * self.seq_len\n",
    "        if i >= data_len:\n",
    "            raise ValueError(f\"idx {idx} bigger than {self.mode} data length {data_len}\")\n",
    "        x = data[i : i + self.seq_len]\n",
    "        y = data[i + 1 : i + self.seq_len + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "device = \"mps\"\n",
    "\n",
    "batch_size = 64  # how many independent sequences will we process in parallel?\n",
    "seq_len = 256  # what is the maximum context length for predictions?\n",
    "vocab_size = 5000\n",
    "model_size = 384\n",
    "num_heads = 6\n",
    "num_layers = 6\n",
    "dropout = 0.2\n",
    "model_version = f\"t{vocab_size}_0.5\"\n",
    "\n",
    "train_ds = BPEDataset(text, mode=\"train\", seq_len=seq_len, vocab_size=vocab_size, device=device)\n",
    "test_ds = BPEDataset(text, mode=\"test\", seq_len=seq_len, vocab_size=vocab_size, device=device)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = DecoderTransformer(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=seq_len,\n",
    "    model_size=model_size,\n",
    "    nb_heads=num_heads,\n",
    "    nb_layers=num_layers,\n",
    "    dropout=dropout,\n",
    ").to(device)\n",
    "xb, yb = next(iter(test_dl))\n",
    "train_logit = model(xb)\n",
    "train_logit.shape\n",
    "\n",
    "print(f\"\"\"cross entropy of logit of non-trained model should be the about the same as cross entropy of uniform distrib across all tokens: \n",
    "      {F.cross_entropy(train_logit.view(batch_size * seq_len, vocab_size), yb.view(batch_size * seq_len)).item()}, \n",
    "      {(-torch.log(torch.tensor(1)/torch.tensor(vocab_size))).item()}\"\"\")\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        train_dl: DataLoader,\n",
    "        test_dl: DataLoader,\n",
    "        model_version: str,\n",
    "        model_name: str,\n",
    "        nb_epochs: int = 100,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        do_save_model: bool = True,\n",
    "        save_every_epoch_nb: int = 20,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_dl = train_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.do_save_model = do_save_model\n",
    "        self.save_every_epoch_nb = save_every_epoch_nb\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.device = device\n",
    "        self.writer = None\n",
    "\n",
    "    def compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "        loss = self.loss_fn(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self, i: int):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            dl = self.train_dl if split == \"train\" else self.test_dl\n",
    "            if dl is not None:\n",
    "                losses = []\n",
    "                for x, y in dl:\n",
    "                    logits = self.model(x)\n",
    "                    loss = self.compute_loss(logits, y)\n",
    "                    losses.append(loss.item())\n",
    "                loss_mean = torch.tensor(losses).mean()\n",
    "                out[split] = loss_mean\n",
    "                self.writer.add_scalar(f\"{split} loss\", loss_mean, i)\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    def train(self, from_epoch: int = 0):\n",
    "        self.writer = SummaryWriter(\n",
    "            f\"../runs/{self.model_name}_{self.model_version}/{datetime.now().strftime('%m-%d-%Y_%H:%M:%S')}\"\n",
    "        )\n",
    "        ex_x, ex_y = next(iter(self.train_dl))\n",
    "        self.writer.add_graph(self.model, (ex_x), use_strict_trace=False)\n",
    "        self.writer.flush()\n",
    "        if from_epoch > 0:\n",
    "            epoch_start_nb = from_epoch + 1\n",
    "        else:\n",
    "            epoch_start_nb = 0\n",
    "        nb_epochs_computed = self.nb_epochs - epoch_start_nb\n",
    "        with tqdm(\n",
    "            total=len(self.train_dl) * nb_epochs_computed,\n",
    "            desc=f\"Epoch {nb_epochs_computed} times batch ({len(self.train_dl)})\",\n",
    "            unit=\"batch\",\n",
    "        ) as pbar:\n",
    "            for curr_epoch in range(epoch_start_nb, self.nb_epochs):\n",
    "                for b, (xb, yb) in enumerate(self.train_dl):\n",
    "                    if device == \"mps\":\n",
    "                        logits = self.model(xb)\n",
    "                        loss = self.compute_loss(logits, yb)\n",
    "                    else:\n",
    "                        # use bf16 when possible based on autocast rules. bf16 is same range than float32, but less precision.\n",
    "                        with torch.autocast(device_type=\"cuda\", dtype=torch.bfloat16):\n",
    "                            logits = self.model(xb)\n",
    "                            loss = self.compute_loss(logits, yb)\n",
    "                    self.optimizer.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    pbar.update(1)\n",
    "\n",
    "                losses = self.estimate_loss(curr_epoch)\n",
    "                pbar.set_postfix(\n",
    "                    {\n",
    "                        \"epoch\": curr_epoch,\n",
    "                        \"train_loss\": f\"{losses.get('train',torch.inf):.4f}\",\n",
    "                        \"test_loss\": f\"{losses.get('test', torch.inf):.4f}\",\n",
    "                    }\n",
    "                )\n",
    "                # print(f\"epoch {curr_epoch}: train loss {losses.get('train',torch.inf):.4f}, val loss {losses.get('test', torch.inf):.4f}\")\n",
    "                for name, weight in self.model.named_parameters():\n",
    "                    self.writer.add_histogram(name, weight, curr_epoch)\n",
    "\n",
    "                # every once in a while evaluate the loss on train and val sets\n",
    "                if self.do_save_model:\n",
    "                    if curr_epoch % self.save_every_epoch_nb == 0:\n",
    "                        save_model(self.model, self.model_name, self.model_version, curr_epoch)\n",
    "\n",
    "        if self.do_save_model:\n",
    "            save_model(self.model, self.model_name, self.model_version, curr_epoch)\n",
    "\n",
    "\n",
    "do_train_small_dataset: bool = True\n",
    "if do_train_small_dataset:\n",
    "    sm_text = text[: seq_len * batch_size]\n",
    "    sm_ds = BPEDataset(sm_text, mode=\"train\", seq_len=seq_len, vocab_size=vocab_size, device=device)\n",
    "    sm_dl = DataLoader(sm_ds, batch_size=batch_size, shuffle=True)\n",
    "    assert len(iter(sm_dl)) == 1\n",
    "\n",
    "    model_name = \"sm_decoder_transformer\"\n",
    "    last_epoch_nb = 0  # 0 to start from scratch\n",
    "    nb_epoch = 150\n",
    "    if last_epoch_nb > 0:\n",
    "        model = load_model(model_name, model_version, last_epoch_nb)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    tr = EpochTrainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        sm_dl,\n",
    "        None,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        nb_epochs=nb_epoch,\n",
    "        do_save_model=False,\n",
    "        save_every_epoch_nb=20,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        device=device,\n",
    "    )\n",
    "    tr.train(from_epoch=last_epoch_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch_nb = 0  # 0 to start from scratch\n",
    "# after 30 epoch, it overfit\n",
    "# should implement early stopping.\n",
    "# try with a smaller model\n",
    "nb_epoch = 30\n",
    "do_train: bool = True\n",
    "model_name = \"decoder_transformer\"\n",
    "if last_epoch_nb > 0:\n",
    "    model = load_model(model_name, model_version, last_epoch_nb)\n",
    "else:\n",
    "    model = DecoderTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        max_seq_len=seq_len,\n",
    "        model_size=model_size,\n",
    "        nb_heads=num_heads,\n",
    "        nb_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "    ).to(device)\n",
    "if do_train:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "    tr = EpochTrainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_dl,\n",
    "        test_dl=test_dl,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        nb_epochs=nb_epoch,\n",
    "        do_save_model=True,\n",
    "        save_every_epoch_nb=20,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        device=device,\n",
    "    )\n",
    "    tr.train(from_epoch=last_epoch_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_sentence(sent_start: str, nb_tokens: int = 50):\n",
    "    batch_input_encodings = train_ds.tokenizer.encode_batch([sent_start])\n",
    "    batch_input_toks = [s.ids for s in batch_input_encodings]\n",
    "    batch_input_tensor = torch.tensor(batch_input_toks, dtype=torch.long, device=device)\n",
    "    gen_tokens = model.generate(batch_input_tensor, max_new_tokens=nb_tokens)\n",
    "    return train_ds.tokenizer.decode_batch(gen_tokens.tolist())\n",
    "\n",
    "\n",
    "complete_sentence(\"The answer, my friend is blowing \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# trainer class for pytorch model that encapsulate training loop\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        train_dl: DataLoader,\n",
    "        test_dl: DataLoader,\n",
    "        model_version: str,\n",
    "        model_name: str,\n",
    "        eval_interval: int = 500,\n",
    "        eval_iters: int = 200,\n",
    "        max_iters: int = 5000,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        do_save_model: bool = True,\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.train_dl = train_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.eval_interval = eval_interval\n",
    "        self.eval_iters = eval_iters\n",
    "        self.max_iters = max_iters\n",
    "        self.do_save_model = do_save_model\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        self.device = device\n",
    "\n",
    "        self.writer = None\n",
    "\n",
    "    def compute_loss(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = logits.shape\n",
    "        logits = logits.view(B * T, C)\n",
    "        targets = targets.view(B * T)\n",
    "        loss = self.loss_fn(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self, i: int):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            dl = self.train_dl if split == \"train\" else self.test_dl\n",
    "            losses = torch.zeros(self.eval_iters)\n",
    "            for k in range(self.eval_iters):\n",
    "                x, y = next(iter(dl))\n",
    "                logits = self.model(x)\n",
    "                loss = self.compute_loss(logits, y)\n",
    "                losses[k] = loss.item()\n",
    "            loss_mean = losses.mean()\n",
    "            out[split] = loss_mean\n",
    "            self.writer.add_scalar(f\"{split} loss\", loss_mean, i)\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    def train(self, from_iter: int = 0):\n",
    "        self.writer = SummaryWriter(\n",
    "            f\"../runs/{self.model_name}_{self.model_version}/{datetime.now().strftime('%m-%d-%Y_%H:%M:%S')}\"\n",
    "        )\n",
    "        ex_x, ex_y = next(iter(self.train_dl))\n",
    "        self.writer.add_graph(self.model, (ex_x), use_strict_trace=False)\n",
    "        self.writer.flush()\n",
    "        for i in range(last_iter + 1, self.max_iters):\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if i % self.eval_interval == 0:\n",
    "                losses = self.estimate_loss(i)\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['test']:.4f}\")\n",
    "                save_model(self.model, self.model_name, self.model_version, i)\n",
    "\n",
    "                for name, weight in self.model.named_parameters():\n",
    "                    self.writer.add_histogram(name, weight, i)\n",
    "\n",
    "            # sample a batch of data\n",
    "            xb, yb = next(iter(self.train_dl))\n",
    "\n",
    "            logits = self.model(xb)\n",
    "            loss = self.compute_loss(logits, yb)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.do_save_model:\n",
    "            save_model(self.model, self.model_name, self.model_version, i)\n",
    "\n",
    "\n",
    "# TODO: add save every eval interval\n",
    "# TODO: keep iteration so we can start at a given iteration. by reloading previous model\n",
    "\n",
    "do_train: bool = True\n",
    "model_version = f\"t{vocab_size}_0.5\"\n",
    "model_name = \"decoder_transformer\"\n",
    "last_iter = 7000  # 0 to start from scratch\n",
    "max_iter = 10001\n",
    "# last_iter = 7000\n",
    "if last_iter > 0:\n",
    "    model = load_model(model_name, model_version, last_iter)\n",
    "if do_train:\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "    tr = Trainer(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_dl,\n",
    "        test_dl,\n",
    "        loss_fn=F.cross_entropy,\n",
    "        eval_interval=200,\n",
    "        eval_iters=100,\n",
    "        max_iters=max_iter,\n",
    "        do_save_model=True,\n",
    "        model_name=model_name,\n",
    "        model_version=model_version,\n",
    "        device=device,\n",
    "    )\n",
    "    tr.train(from_iter=last_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beg_sent_toks = tokenizer.encode(\"here are the\")\n",
    "tokenizer.decode(beg_sent_toks.ids)\n",
    "sent_input = torch.tensor(beg_sent_toks.ids, dtype=torch.int).expand((1, -1)).to(device)\n",
    "sent_output = model.generate(sent_input, max_new_tokens=10)\n",
    "tokenizer.decode_batch(sent_output.cpu().numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explainability\n",
    "try to implement\n",
    "https://towardsdatascience.com/deep-dive-into-anthropics-sparse-autoencoders-by-hand-%EF%B8%8F-eebe0ef59709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export(\"./components.ipynb\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class AKSortDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the Sort problem. E.g. for problem length 6:\n",
    "    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 0 2 1 0 1 0 0 0 1 1\n",
    "    output: I I I I I 0 0 0 1 1 2\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=6, num_digits=3):\n",
    "        assert split in {\"train\", \"test\"}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "        self.num_digits = num_digits\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10000  # ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.num_digits\n",
    "\n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer,\n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return self.length * 2 - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # use rejection sampling to generate an input example from the desired split\n",
    "        while True:\n",
    "            # generate some random integers\n",
    "            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n",
    "            # half of the time let's try to boost the number of examples that\n",
    "            # have a large number of repeats, as this is what the model seems to struggle\n",
    "            # with later in training, and they are kind of rate\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                if inp.unique().nelement() > self.length // 2:\n",
    "                    # too many unqiue digits, re-sample\n",
    "                    continue\n",
    "            # figure out if this generated example is train or test based on its hash\n",
    "            h = hash(pickle.dumps(inp.tolist()))\n",
    "            inp_split = \"test\" if h % 4 == 0 else \"train\"  # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break  # ok\n",
    "\n",
    "        # solve the task: i.e. sort\n",
    "        sol = torch.sort(inp)[0]\n",
    "\n",
    "        # concatenate the problem specification and the solution\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[: self.length - 1] = -1\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# print an example instance of the dataset\n",
    "train_dataset = AKSortDataset(\"train\")\n",
    "\n",
    "x, y = train_dataset[0]\n",
    "# for a, b in zip(x,y):\n",
    "#    print(int(a),int(b))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-implementations-urBBcPaT-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
