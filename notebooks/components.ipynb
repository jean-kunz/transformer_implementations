{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp components\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers components to be assembled in a model\n",
    "\n",
    "Work in progress\n",
    "\n",
    "References:\n",
    "- https://web.stanford.edu/class/cs224n/slides/cs224n-2022-lecture09-transformers.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "from dataclasses import dataclass\n",
    "from my_transformer.utils import save_model, load_model\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import math\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device('cpu')\n",
    "g = torch.Generator(device=device).manual_seed(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word. Consequently, there’s still the need for a way to incorporate the order of the words into our model.\n",
    "So we give the model some sense of position of the token in the sequence. \n",
    "\n",
    "Either we give the position as an input to the model or the model learns it.\n",
    "\n",
    "#### Non learned positional embeddings\n",
    "\n",
    "##### Potential solutions:\n",
    "\n",
    "The first idea that might come to mind is to assign a number to each time-step within the [0, 1] range in which 0 means the first word and 1 is the last time-step. One of the problems it will introduce is that you can’t figure out how many words are present within a specific range. In other words, time-step delta doesn’t have consistent meaning across different sentences.\n",
    "\n",
    "Another idea is to assign a number to each time-step linearly. That is, the first word is given “1”, the second word is given “2”, and so on. The problem with this approach is that not only the values could get quite large, but also our model can face sentences longer than the ones in training.\n",
    "\n",
    "Ideally, the following criteria should be satisfied:\n",
    "\n",
    "- It should output a unique encoding for each time-step (word’s position in a sentence)\n",
    "- Distance between any two time-steps should be consistent across sentences with different lengths.\n",
    "- Our model should generalize to longer sentences without any efforts. Its values should be bounded.\n",
    "- It must be deterministic.\n",
    "\n",
    "##### Proposed solutions:\n",
    "\n",
    "The initial solution that was proposed isn’t a single number. Instead, it’s a d-dimensional vector that contains information about a specific position in a sentence. This vector, if not learned, is not integrated in the model.\n",
    "\n",
    "$\\begin{align}\n",
    "  \\vec{p_t}^{(i)} = f(t)^{(i)} & := \n",
    "  \\begin{cases}\n",
    "      \\sin({\\omega_k} . t),  & \\text{if}\\  i = 2k \\\\\n",
    "      \\cos({\\omega_k} . t),  & \\text{if}\\  i = 2k + 1\n",
    "  \\end{cases}\n",
    "\\end{align} $\n",
    "\n",
    "where $\\omega_k = \\frac{1}{n^{2k / d}}$\n",
    "\n",
    "where : \n",
    "- L: sequence length\n",
    "- t: position of token in input sequence\n",
    "- d: dimension of positon embedding (same as token embedding)\n",
    "- P(t,j): position function to map a position t in sequence to index (t,i) in positional matrix\n",
    "- n: user defined scalar (ex: 10'000)\n",
    "- i: index on the position embedding\n",
    "- k: floor division (partie entière) of i by 2, so i = 2k (i is even - pair) or i=2k+1 (i is odd - impair), because we add a sin/cos pair on every two embedding slot\n",
    "\n",
    "The frequency of sinusoidal is decreasing with the vector dimension (as i grows)\n",
    "\n",
    "So we got a vector with pairs of sin/cosines for each frequency.\n",
    "\n",
    "$\\vec{p_t} = \\begin{bmatrix} \n",
    "\\sin({\\omega_1}.t)\\\\ \n",
    "\\cos({\\omega_1}.t)\\\\ \n",
    "\\\\\n",
    "\\sin({\\omega_2}.t)\\\\ \n",
    "\\cos({\\omega_2}.t)\\\\ \n",
    "\\\\\n",
    "\\vdots\\\\ \n",
    "\\\\\n",
    "\\sin({\\omega_{d/2}}.t)\\\\ \n",
    "\\cos({\\omega_{d/2}}.t) \n",
    "\\end{bmatrix}_{d \\times 1}$\n",
    "\n",
    "It's like the encoding of numbers in binary format: \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  10: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\ \n",
    "  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "\\end{align}$\n",
    "\n",
    "where last bit is alternating on every number and previous on every 2 numbers, and so on. So instead of using bits (which would be a waste of space), we use continuous float variant with sinuosidal functions..\n",
    "\n",
    "## References\n",
    "\n",
    "- https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers\n",
    "- https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "- https://kikaben.com/transformers-positional-encoding/\n",
    "- http://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding\n",
    "- https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## positional encoder must be dynamic for any lenght (up to max length) provided while generating tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    \"\"\"Module to encode position in a transformer like model.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        max_seq_len (int): max length of sequence, aka L\n",
    "        embedding_dim (int): dimension of embeddings in model, aka d\n",
    "        dropout (float):  dropout rate. 0. for no dropout.\n",
    "        is_learned (bool): true if the position is learned through gradient descent or given (non differentiable) as defined in original paper Attention is all you need, https://arxiv.org/abs/1706.03762\n",
    "        n (int): user defined scalar set by default to 10000 as in paper\n",
    "    \"\"\"\n",
    "\n",
    "    def positional_encoding(self):\n",
    "        pos = torch.arange(0, self.max_seq_len).repeat(self.embedding_dim, 1)\n",
    "        i = torch.arange(0, self.embedding_dim)\n",
    "        k = i // 2\n",
    "        wt = pos.T / (self.n ** (2 * k / self.embedding_dim))\n",
    "        sin = torch.sin(wt)\n",
    "        cos = torch.cos(wt)\n",
    "        pe = torch.zeros((self.max_seq_len, self.embedding_dim))\n",
    "        pe[:, 0::2] = sin[:, 0::2]\n",
    "        pe[:, 1::2] = cos[:, 1::2]\n",
    "        return pe\n",
    "\n",
    "    def __init__(\n",
    "        self, max_seq_len: int, embedding_dim: int, dropout: float = 0.0, is_learned: bool = True, n: int = 10000\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.is_learned = is_learned\n",
    "        self.n = n\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        if self.is_learned:\n",
    "            pos = torch.arange(0, self.max_seq_len, dtype=torch.long).unsqueeze(0)  # shape (1, max_seq_len)\n",
    "            self.register_buffer(\"pos\", pos)\n",
    "            self.pos_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        else:\n",
    "            pos_encodings = self.positional_encoding().unsqueeze(0)\n",
    "            pos_encodings.requires_grad_(False)\n",
    "            # a buffer is a state in module which is not a parameter (learned)\n",
    "            self.register_buffer(\"pos_encodings\", pos_encodings)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # we only need the first T positions\n",
    "        if self.is_learned:\n",
    "            x = x + self.pos_embedding(self.pos)[:, :T]\n",
    "        else:\n",
    "            x = x + self.pos_encodings[:, :T]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non learned positional encoding\n",
    "seq_len = 20\n",
    "embedding_dim = 50\n",
    "pos_enc = PositionalEncoder(max_seq_len=seq_len, embedding_dim=embedding_dim, dropout=0.1, is_learned=False)\n",
    "pos_enc.to(device)\n",
    "pos_enc.positional_encoding()\n",
    "x = torch.zeros(1, seq_len, embedding_dim, device=device)\n",
    "pe = pos_enc.forward(x)\n",
    "np_pe = pe.cpu().detach().numpy()\n",
    "plt.pcolormesh(np_pe[0].T, cmap=\"RdBu\")\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Depth\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanisms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def unidirectional_mask(seq_len: int):\n",
    "    inverse_mask = torch.triu(torch.ones((1, seq_len, seq_len)), diagonal=1).type(torch.uint8)\n",
    "    mask = inverse_mask == 0\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = unidirectional_mask(20)\n",
    "plt.imshow(mask[0])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "Based on formal algorithm for transformers (https://arxiv.org/abs/2207.09238) where it can be used for different attention architectures like \n",
    "- encoder-decoder/seq-to-seq (very first transformer), \n",
    "- encoder-only (bert), \n",
    "- decoder-only (gpt-*, gopher). \n",
    "\n",
    "### Encoder-decoder architecture use 2 sequences as input\n",
    "- context sequence of tokens (z), which is used to encode context as a vector per token with bidirectional attention\n",
    "- primary sequence of tokens (x), which is used to decode both the encoded context and a masked primary sequence (tokens in primary sequence that precedes current). This is used to train a translator from FR to EN for instance. FR tokens are the context sequence and EN tokens are the primary sequence (target)\n",
    " \n",
    "### Encoder only architecture use 1 primary sequence as input\n",
    "Given a primary input sequence (x) with some tokens masked out, the goal is to recover the masked tokens. the goal is to learn a generally usefull representation of text. Uses a bidirectional attention\n",
    "\n",
    "### Decoder only architecture use 1 primary sequence as input\n",
    "Autoregressive language modelling where the goal is to predict the next token of a primary token sequence (x). Uses a unidirectional attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "def attention(\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    mask: torch.Tensor = None,\n",
    "    dropout: torch.nn.Module = None,\n",
    "    verbose: bool = False,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    # is the size of atttention (=d//nb heads, where d is the size of attn model)\n",
    "    d_k = query.size(-1)\n",
    "    # scores = (query @ key.transpose(-2,-1)) /(torch.sqrt(torch.tensor(d_k, requires_grad=False)))\n",
    "    scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # mask scores with -inf when mask is False. If we masked_fill(mask, -inf), it would mask when mask ==1 (is true). We want to mask when it's 0 (false).\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    if verbose:\n",
    "        print(\"scores, mask\", mask is not None, scores)\n",
    "    soft = F.softmax(scores, dim=-1)\n",
    "    if verbose:\n",
    "        print(\"soft, mask\", mask is not None, soft)\n",
    "    if dropout is not None:\n",
    "        soft = dropout(soft)\n",
    "    atn = soft @ value\n",
    "    if verbose:\n",
    "        print(\"atn, mask\", mask is not None, atn)\n",
    "\n",
    "    assert (\n",
    "        atn.shape == query.shape\n",
    "    ), f\"atn shape {atn.shape} should be the same as input tensors key, query and value shapes. Query shape: {query.shape}\"\n",
    "    return atn, soft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 5  # nb of heads\n",
    "d_k = embedding_dim // h\n",
    "batch_size = 2\n",
    "# shape is batch, h, seq_len, d_k\n",
    "Q = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "K = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "V = torch.randn((batch_size, h, seq_len, d_k), generator=g, device=device, requires_grad=True)\n",
    "mask = unidirectional_mask(seq_len).to(device)\n",
    "attn, attn_weight = attention(Q, K, V, mask=mask, dropout=torch.nn.Dropout(0.0))\n",
    "attn_no_mask, attn_weight_no_mask = attention(Q, K, V, dropout=torch.nn.Dropout(0.0))\n",
    "\n",
    "plt.imshow(attn[0][0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "np_attn_weight = attn_weight.cpu()\n",
    "np_sum = torch.sum(np_attn_weight, dim=1).cpu().detach().numpy()\n",
    "plt.imshow(np_sum[0])\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "target = torch.rand(*attn.shape).to(device)\n",
    "print(target.shape, pe.shape)\n",
    "loss = torch.norm(attn - target)\n",
    "loss.retain_grad()\n",
    "attn.retain_grad()\n",
    "loss.backward()\n",
    "loss.item(), attn.grad.shape, loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multihead attention module as defined in Formal algorithm for transformers (https://arxiv.org/abs/2207.09238)\n",
    "    It can be used for different attention architectures like encoder-decoder/seq-to-seq (very first transformer),\n",
    "    encoder-only (bert), decoder-only (gpt-*, gopher).\n",
    "\n",
    "    It splits weights into h heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d: int, h: int, dropout: float = 0.0, bias: bool = True):\n",
    "        super().__init__()\n",
    "        # g = torch.Generator().manual_seed(42)\n",
    "        self.d = d  # dim of attention\n",
    "        self.h = h\n",
    "        assert (\n",
    "            d % h == 0\n",
    "        ), f\"dim of attention {d} must be a multiple of nb of heads {h}. d_k {d//h} is the model dim per head, because we split by nb of heads.\"\n",
    "        self.d_k = d // h  # dim of attention on one head.\n",
    "        self.wq = nn.Linear(d, d, bias=bias)\n",
    "        self.wk = nn.Linear(d, d, bias=bias)\n",
    "        self.wv = nn.Linear(d, d, bias=bias)\n",
    "        self.wo = nn.Linear(d, d, bias=bias)  # linear transform by output weight matrix.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, z: torch.Tensor, mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): primary sequence\n",
    "            z (torch.Tensor): context sequence, only for encoder-decoder architecture.\n",
    "            mask (torch.Tensor, optional): _description_. Defaults to None.\n",
    "        \"\"\"\n",
    "        b, l, e = x.size()\n",
    "        if z is None:\n",
    "            z = x\n",
    "        q = self.wq(x)\n",
    "        k = self.wk(z)\n",
    "        v = self.wk(z)\n",
    "        # shape has to be batch, h, seq, d_k (d//h)\n",
    "        # first view to : b,l,h,d_k , then transpose h and l so we got per head q,k,v\n",
    "        q = q.view(b, l, self.h, self.d_k).transpose(1, 2)\n",
    "        k = k.view(b, l, self.h, self.d_k).transpose(1, 2)\n",
    "        v = v.view(b, l, self.h, self.d_k).transpose(1, 2)\n",
    "\n",
    "        # attention has to be done on each head.\n",
    "        mh_attn, mh_attn_weight = attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "        # print(mh_attn.shape, mh_attn_weight.shape)\n",
    "        # we need to create a contiguous memory space for tensor after transpose so we can apply view.\n",
    "        concat_attn = mh_attn.transpose(2, 1).contiguous().view(b, l, self.h * self.d_k)\n",
    "        concat_attn_weight = torch.sum(mh_attn_weight, dim=1)\n",
    "        assert concat_attn.size() == (b, l, e)\n",
    "        # apply linear layer on concatenated attention.\n",
    "        output = self.dropout(self.wo(concat_attn))\n",
    "        return output, concat_attn_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check it can compute gradientbatch_size=2\n",
    "primary_seq = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "ctx_seq = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "mh = MultiHeadAttention(d=embedding_dim, h=5, dropout=0.0, bias=False).to(device)\n",
    "mh_attn, mh_attn_weight = mh(primary_seq, ctx_seq, mask=mask)\n",
    "\n",
    "# mh_attn.retain_grad()\n",
    "target = torch.rand(*mh_attn.shape).to(device)\n",
    "print(target.shape, pe.shape)\n",
    "loss = torch.norm(mh_attn - target)\n",
    "mh_attn.retain_grad()\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "loss.item(), mh_attn.grad.shape, loss.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "primary_seq = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "ctx_seq = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "mh = MultiHeadAttention(d=embedding_dim, h=5, dropout=0.0, bias=False).to(device)\n",
    "mh_attn, mh_attn_weight = mh(primary_seq, ctx_seq, mask=mask)\n",
    "\n",
    "plt.imshow(mh_attn[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(mh_attn_weight[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with multihead_attn = nn.MultiheadAttention(embed_dim, num_heads) so that result is the same.\n",
    "# the graph output should look the same, even if they are not exactly the same due to different initialization\n",
    "nn_mh = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=5, dropout=0.0, batch_first=True).to(device)\n",
    "nn_mh_atn, nn_mh_atn_weights = nn_mh(primary_seq, ctx_seq, ctx_seq, attn_mask=(mask[0] == 0))\n",
    "plt.imshow(nn_mh_atn[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(nn_mh_atn_weights[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# check it can compute gradient\n",
    "nn_mh_atn.retain_grad()\n",
    "target = torch.rand(*nn_mh_atn.shape).to(device)\n",
    "loss = torch.norm(nn_mh_atn - target)\n",
    "loss.retain_grad()\n",
    "loss.backward()\n",
    "loss.item()\n",
    "loss.grad, nn_mh_atn.grad.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, d: int, eps: float = 1e-05, use_torch_implem: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.eps = eps\n",
    "        self.use_torch_implem = use_torch_implem\n",
    "        self.gamma = torch.nn.Parameter(torch.ones(d, dtype=torch.float))  # scale\n",
    "        self.beta = torch.nn.Parameter(torch.zeros(d, dtype=torch.float))  # offset\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.use_torch_implem:\n",
    "            # we normalize across features for each example and seq item.\n",
    "            normalized_shape = x.size()[-1:]\n",
    "            b, l, e = x.size()\n",
    "            x_hat = F.layer_norm(x, normalized_shape=[e], weight=self.gamma, bias=self.beta)\n",
    "        else:\n",
    "            # normalization across features (independently) for each sample. We compute mean and var on the last 2 axis, so we have it per sampel\n",
    "            mean = x.mean((-1), keepdim=True)  # .unsqueeze(-1)\n",
    "            var = x.var((-1), keepdim=True)  # .unsqueeze(-1)\n",
    "            x_hat = torch.mul((x - mean) / (torch.sqrt(var) + self.eps), self.gamma) + self.beta\n",
    "\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check implement is close to pytorch one.\n",
    "x = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "layer_norm = LayerNormalization(d=embedding_dim, use_torch_implem=False).to(device)\n",
    "x_hat = layer_norm(x)\n",
    "\n",
    "torch_layer_norm = nn.LayerNorm(embedding_dim, eps=1e-05, elementwise_affine=True).to(device)\n",
    "torch_x_hat = torch_layer_norm(x)\n",
    "assert torch.allclose(x_hat, torch_x_hat, rtol=0.02, atol=0.0), \"my implementation and torch should be close\"\n",
    "\n",
    "target = torch.rand(*x_hat.shape).to(device)\n",
    "print(target.shape, x_hat.shape)\n",
    "loss = torch.norm(x_hat - target)\n",
    "loss.retain_grad()\n",
    "x_hat.retain_grad()\n",
    "loss.backward()\n",
    "loss.item(), x_hat.grad.shape, loss.grad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder transformer\n",
    "\n",
    "Architecture used by GPT-*, Gopher, where it want to predict the next token given previous ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_size: int, nb_heads: int = 1, dropout: float = 0.0, bias: bool = True, mlp_factor=4\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = LayerNormalization(model_size)\n",
    "        self.attn = MultiHeadAttention(d=model_size, h=nb_heads, dropout=dropout, bias=bias)\n",
    "        self.mlp1 = nn.Linear(model_size, mlp_factor * model_size, bias=True)\n",
    "        self.mlp2 = nn.Linear(mlp_factor * model_size, model_size, bias=True)\n",
    "        self.activation = torch.nn.GELU()\n",
    "        self.layer_norm2 = LayerNormalization(model_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, l, d = x.size()\n",
    "        norm_x = self.layer_norm1(x)\n",
    "        mask = unidirectional_mask(seq_len=l)\n",
    "        attn_x = x + self.attn(norm_x, z=None, mask=None)[0]\n",
    "\n",
    "        norm_attn_x = self.layer_norm2(attn_x)\n",
    "        lin1 = self.activation(self.mlp1(norm_attn_x))\n",
    "        x = x + self.dropout(self.mlp2(lin1))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((batch_size, seq_len, embedding_dim)).to(device)\n",
    "dl = DecoderLayer(embedding_dim, nb_heads=1, dropout=0.1).to(device)\n",
    "dlo = dl(x)\n",
    "\n",
    "plt.imshow(x[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(dlo[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        model_size: int,\n",
    "        nb_heads: int = 1,\n",
    "        nb_layers: int = 1,\n",
    "        dropout: float = 0.0,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(vocab_size, embedding_dim=model_size)\n",
    "        self.pos_enc = PositionalEncoder(\n",
    "            max_seq_len=max_seq_len, embedding_dim=model_size, dropout=dropout, is_learned=True\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderLayer(model_size=model_size, nb_heads=nb_heads, dropout=dropout, bias=bias)\n",
    "                for i in range(nb_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = LayerNormalization(model_size)\n",
    "        self.unembedding = nn.Linear(model_size, vocab_size)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        seq_len = x.size()[1]\n",
    "        emb = self.tok_emb(x)\n",
    "        x = self.pos_enc(emb)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.unembedding(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "dec = DecoderTransformer(vocab_size=6, max_seq_len=5, model_size=10, nb_heads=1).to(device)\n",
    "x = torch.tensor([[0, 1, 2, 3, 4], [1, 2, 3, 4, 5]], dtype=torch.int, device=device)\n",
    "y_hat = dec(x)\n",
    "y_hat.shape\n",
    "\n",
    "target = torch.rand(*y_hat.shape).to(device)\n",
    "print(target.shape, y_hat.shape)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "loss = loss(y_hat, target)\n",
    "loss.retain_grad()\n",
    "y_hat.retain_grad()\n",
    "loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"../dataset/shakespeare.txt\") as f:\n",
    "#    text = f.read()\n",
    "\n",
    "df = pd.read_csv(\"../dataset/bob_dylan_lyrics.csv\")\n",
    "text = \"\\n\\n\".join(df.lyrics.values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    def __init__(self, text: str, mode: str = \"train\", seq_len: int = 100, device: str = \"cpu\"):\n",
    "        assert mode in [\"train\", \"test\"], \"Mode must be 'train' or 'test'\"\n",
    "        self.mode = mode\n",
    "        self.device = device\n",
    "        self.test_start_pos = int(0.8 * len(text))\n",
    "        self.train_text = text[: self.test_start_pos]\n",
    "        self.test_text = text[self.test_start_pos :]\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab = list(set(text))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char_to_idx = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {i: c for i, c in enumerate(self.vocab)}\n",
    "\n",
    "        self.train_encoded = torch.tensor(self.encode(self.train_text), dtype=torch.long, device=device)\n",
    "        self.test_encoded = torch.tensor(self.encode(self.test_text), dtype=torch.long, device=device)\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.mode == \"train\":\n",
    "            data = self.train_encoded\n",
    "        else:\n",
    "            data = self.test_encoded\n",
    "        return len(data) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"train\":\n",
    "            data = self.train_encoded\n",
    "        else:\n",
    "            data = self.test_encoded\n",
    "\n",
    "        data_len = data.shape[0]\n",
    "        i = idx * self.seq_len\n",
    "        if i >= data_len:\n",
    "            raise ValueError(f\"idx {idx} bigger than {self.mode} data length {data_len}\")\n",
    "        x = data[i : i + self.seq_len]\n",
    "        y = data[i + 1 : i + self.seq_len + 1]\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        return x, y\n",
    "\n",
    "    def decode(self, x):\n",
    "        return \"\".join([self.idx_to_char[i] for i in x])\n",
    "\n",
    "    def encode(self, x):\n",
    "        return [self.char_to_idx[c] for c in x]\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cds = CharDataset(text, seq_len=10)\n",
    "l = len(cds)\n",
    "x, y = cds[l]\n",
    "assert isinstance(x, torch.Tensor)\n",
    "assert isinstance(y, torch.Tensor)\n",
    "try:\n",
    "    cds[l + 1]\n",
    "    assert False, \"it should\"\n",
    "except ValueError as e:\n",
    "    assert True, \"it should raise a value error if bigger than len of dataset\"\n",
    "\n",
    "batch_size = 8\n",
    "seq_len = 10\n",
    "train_ds = CharDataset(text, seq_len=seq_len, device=device)\n",
    "test_ds = CharDataset(text, seq_len=seq_len, mode=\"test\", device=device)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "test_x, test_y = next(iter(test_dl))\n",
    "assert test_x.shape == (batch_size, seq_len)\n",
    "assert test_y.shape == (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        seq_len: int,\n",
    "        model_size: int,\n",
    "        nb_heads: int = 1,\n",
    "        nb_layers: int = 1,\n",
    "        dropout: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.model_size = model_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_emb = torch.nn.Embedding(vocab_size, model_size)\n",
    "        self.pos_enc = PositionalEncoder(\n",
    "            max_seq_len=seq_len, embedding_dim=model_size, dropout=dropout, is_learned=True\n",
    "        )\n",
    "        self.layers = nn.Sequential(\n",
    "            *[\n",
    "                DecoderLayer(model_size=model_size, nb_heads=nb_heads, dropout=dropout, bias=True)\n",
    "                for i in range(nb_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.layer_norm = LayerNormalization(model_size)\n",
    "        self.lm_head = torch.nn.Linear(model_size, vocab_size)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        tok_emb = self.token_emb(idx)\n",
    "        x = self.pos_enc(tok_emb)\n",
    "        assert x.shape == (B, T, self.model_size)\n",
    "        # it changes on each iteration.\n",
    "        # pos_emb = self.pos_emb(torch.arange(T, device=device))\n",
    "        # x = tok_emb + pos_emb  # B, T, C\n",
    "        # print(f\"x shape: {x.shape}\")\n",
    "        x = self.layers(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.lm_head(x)\n",
    "        # print(f\"logits shape: {logits.shape}\")\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # we take the last seq_len tokens as condition\n",
    "            idx_cond = idx[:, -self.seq_len :]\n",
    "            logits, _ = self(idx_cond)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "device = \"mps\"\n",
    "batch_size = 16\n",
    "seq_len = 10\n",
    "model_size = 128\n",
    "\n",
    "train_ds = CharDataset(text, seq_len=seq_len, device=device)\n",
    "test_ds = CharDataset(text, seq_len=seq_len, mode=\"test\", device=device)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)\n",
    "dls = {\"train\": train_dl, \"test\": test_dl}\n",
    "\n",
    "vocab_size = train_ds.get_vocab_size()\n",
    "\n",
    "model = DecoderModel(vocab_size=vocab_size, seq_len=seq_len, model_size=model_size, nb_heads=4, nb_layers=1).to(device)\n",
    "xb, yb = next(iter(test_dl))\n",
    "train_logit, train_loss = model(xb, yb)\n",
    "train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# trainer class for pytorch model that encapsulate training loop\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        train_dl: DataLoader,\n",
    "        test_dl: DataLoader,\n",
    "        eval_interval: int = 500,\n",
    "        eval_iters: int = 200,\n",
    "        max_iters: int = 5000,\n",
    "        do_save_model: bool = True,\n",
    "        model_version: str = \"\",\n",
    "        device: str = \"cpu\",\n",
    "    ) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dl = train_dl\n",
    "        self.test_dl = test_dl\n",
    "        self.eval_interval = eval_interval\n",
    "        self.eval_iters = eval_iters\n",
    "        self.max_iters = max_iters\n",
    "        self.do_save_model = do_save_model\n",
    "        self.model_version = model_version\n",
    "        self.device = device\n",
    "        self.writer = SummaryWriter(f\"../runs/my_gpt_{self.model_version}\")\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def estimate_loss(self, i: int):\n",
    "        out = {}\n",
    "        self.model.eval()\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            dl = self.train_dl if split == \"train\" else self.test_dl\n",
    "            losses = torch.zeros(self.eval_iters)\n",
    "            for k in range(self.eval_iters):\n",
    "                X, Y = next(iter(dl))\n",
    "                logits, loss = self.model(X, Y)\n",
    "                losses[k] = loss.item()\n",
    "            loss_mean = losses.mean()\n",
    "            out[split] = loss_mean\n",
    "            self.writer.add_scalar(f\"{split} loss\", loss_mean, i)\n",
    "        self.model.train()\n",
    "        return out\n",
    "\n",
    "    def train(self):\n",
    "        for i in range(self.max_iters):\n",
    "            # every once in a while evaluate the loss on train and val sets\n",
    "            if i % self.eval_interval == 0:\n",
    "                losses = self.estimate_loss(i)\n",
    "                print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['test']:.4f}\")\n",
    "\n",
    "            # sample a batch of data\n",
    "            xb, yb = next(iter(self.train_dl))\n",
    "\n",
    "            logits, loss = self.model(xb, yb)\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        if self.do_save_model:\n",
    "            save_model(self.model, self.model_version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "model_version = \"0.3\"\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "tr = Trainer(\n",
    "    model,\n",
    "    optimizer,\n",
    "    train_dl,\n",
    "    test_dl,\n",
    "    eval_interval=500,\n",
    "    eval_iters=200,\n",
    "    max_iters=5001,\n",
    "    do_save_model=True,\n",
    "    model_version=model_version,\n",
    "    device=device,\n",
    ")\n",
    "tr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_load: bool = False\n",
    "if do_load:\n",
    "    model = load_model(model_version)\n",
    "\n",
    "print(\"-----\")\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(train_ds.decode(model.generate(context, max_new_tokens=200)[0].tolist()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export(\"./components.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 5000\n",
    "eval_interval = 500\n",
    "learning_rate = 1e-2\n",
    "eval_iters = 200\n",
    "do_save_model: bool = True\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        dl = dls[split]\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = next(iter(dl))\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(max_iters):\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {i}: train loss {losses['train']:.4f}, val loss {losses['test']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = next(iter(train_dl))\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "if do_save_model:\n",
    "    model_version = \"0.2\"\n",
    "    save_model(model, model_version)\n",
    "\n",
    "\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(\"-----\")\n",
    "print(train_ds.decode(model.generate(context, max_new_tokens=100)[0].tolist()))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "class AKSortDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for the Sort problem. E.g. for problem length 6:\n",
    "    Input: 0 0 2 1 0 1 -> Output: 0 0 0 1 1 2\n",
    "    Which will feed into the transformer concatenated as:\n",
    "    input:  0 0 2 1 0 1 0 0 0 1 1\n",
    "    output: I I I I I 0 0 0 1 1 2\n",
    "    where I is \"ignore\", as the transformer is reading the input sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, split, length=6, num_digits=3):\n",
    "        assert split in {\"train\", \"test\"}\n",
    "        self.split = split\n",
    "        self.length = length\n",
    "        self.num_digits = num_digits\n",
    "\n",
    "    def __len__(self):\n",
    "        return 10000  # ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return self.num_digits\n",
    "\n",
    "    def get_block_size(self):\n",
    "        # the length of the sequence that will feed into transformer,\n",
    "        # containing concatenated input and the output, but -1 because\n",
    "        # the transformer starts making predictions at the last input element\n",
    "        return self.length * 2 - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # use rejection sampling to generate an input example from the desired split\n",
    "        while True:\n",
    "            # generate some random integers\n",
    "            inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n",
    "            # half of the time let's try to boost the number of examples that\n",
    "            # have a large number of repeats, as this is what the model seems to struggle\n",
    "            # with later in training, and they are kind of rate\n",
    "            if torch.rand(1).item() < 0.5:\n",
    "                if inp.unique().nelement() > self.length // 2:\n",
    "                    # too many unqiue digits, re-sample\n",
    "                    continue\n",
    "            # figure out if this generated example is train or test based on its hash\n",
    "            h = hash(pickle.dumps(inp.tolist()))\n",
    "            inp_split = \"test\" if h % 4 == 0 else \"train\"  # designate 25% of examples as test\n",
    "            if inp_split == self.split:\n",
    "                break  # ok\n",
    "\n",
    "        # solve the task: i.e. sort\n",
    "        sol = torch.sort(inp)[0]\n",
    "\n",
    "        # concatenate the problem specification and the solution\n",
    "        cat = torch.cat((inp, sol), dim=0)\n",
    "\n",
    "        # the inputs to the transformer will be the offset sequence\n",
    "        x = cat[:-1].clone()\n",
    "        y = cat[1:].clone()\n",
    "        # we only want to predict at output locations, mask out the loss at the input locations\n",
    "        y[: self.length - 1] = -1\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# print an example instance of the dataset\n",
    "train_dataset = AKSortDataset(\"train\")\n",
    "\n",
    "x, y = train_dataset[0]\n",
    "# for a, b in zip(x,y):\n",
    "#    print(int(a),int(b))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min gpt implem\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
    "    It is possible to use torch.nn.MultiheadAttention here but I am including an\n",
    "    explicit implementation here to show that there is nothing too scary here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n",
    "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
    "                1, 1, config.block_size, config.block_size\n",
    "            ),\n",
    "        )\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        print(B, T, C)\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        print(self.c_attn(x).shape)\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)  # takes group of n_embd along dim 2.\n",
    "        print(q.shape, k.shape, v.shape)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        # what is k.size(-1) ? d_k ?\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_dropout(att)\n",
    "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Cfg:\n",
    "    n_embd = 50\n",
    "    n_head = 5\n",
    "    attn_pdrop = 0.0\n",
    "    resid_pdrop = 0.0\n",
    "    block_size = 1\n",
    "\n",
    "\n",
    "x = torch.rand((1, seq_len, embedding_dim)).to(device)\n",
    "cs = CausalSelfAttention(Cfg()).to(device)\n",
    "cs_attn = cs(x)\n",
    "plt.imshow(cs_attn[0].cpu().detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
