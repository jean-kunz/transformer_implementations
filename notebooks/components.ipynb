{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp components\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers components to be assembled in a model\n",
    "\n",
    "Work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device('mps')\n",
    "torch.backends.mps.is_available()\n",
    "g = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "- Token embeddings, using torch embedding lookup\n",
    "- Positional embedding, which can be fixed or learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class TokenEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "embedding_dim = 50\n",
    "emb = TokenEmbeddings(vocab_size, embedding_dim=embedding_dim)\n",
    "emb.to(device)\n",
    "x = torch.tensor([0,1,2,1], dtype=torch.long, device=device)\n",
    "emb_x = emb(x)\n",
    "assert emb_x.shape==(x.shape[0],embedding_dim)\n",
    "assert torch.equal(emb_x[1],emb_x[3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word. Consequently, there’s still the need for a way to incorporate the order of the words into our model.\n",
    "So we give the model some sense of position of the token in the sequence. \n",
    "\n",
    "Either we give the position as an input to the model or the model learns it.\n",
    "\n",
    "#### Non learned positional embeddings\n",
    "\n",
    "##### Potential solutions:\n",
    "\n",
    "The first idea that might come to mind is to assign a number to each time-step within the [0, 1] range in which 0 means the first word and 1 is the last time-step. One of the problems it will introduce is that you can’t figure out how many words are present within a specific range. In other words, time-step delta doesn’t have consistent meaning across different sentences.\n",
    "\n",
    "Another idea is to assign a number to each time-step linearly. That is, the first word is given “1”, the second word is given “2”, and so on. The problem with this approach is that not only the values could get quite large, but also our model can face sentences longer than the ones in training.\n",
    "\n",
    "Ideally, the following criteria should be satisfied:\n",
    "\n",
    "- It should output a unique encoding for each time-step (word’s position in a sentence)\n",
    "- Distance between any two time-steps should be consistent across sentences with different lengths.\n",
    "- Our model should generalize to longer sentences without any efforts. Its values should be bounded.\n",
    "- It must be deterministic.\n",
    "\n",
    "##### Proposed solutions:\n",
    "\n",
    "The initial solution that was proposed isn’t a single number. Instead, it’s a d-dimensional vector that contains information about a specific position in a sentence. This vector, if not learned, is not integrated in the model.\n",
    "\n",
    "$\\begin{align}\n",
    "  \\vec{p_t}^{(i)} = f(t)^{(i)} & := \n",
    "  \\begin{cases}\n",
    "      \\sin({\\omega_k} . t),  & \\text{if}\\  i = 2k \\\\\n",
    "      \\cos({\\omega_k} . t),  & \\text{if}\\  i = 2k + 1\n",
    "  \\end{cases}\n",
    "\\end{align} $\n",
    "\n",
    "where $\\omega_k = \\frac{1}{n^{2k / d}}$\n",
    "\n",
    "where : \n",
    "- L: sequence length\n",
    "- t: position of token in input sequence\n",
    "- d: dimension of positon embedding (same as token embedding)\n",
    "- P(t,j): position function to map a position t in sequence to index (t,i) in positional matrix\n",
    "- n: user defined scalar (ex: 10'000)\n",
    "- i: index on the position embedding\n",
    "- k: floor division (partie entière) of i by 2, so i = 2k (i is even - pair) or i=2k+1 (i is odd - impair), because we add a sin/cos pair on every two embedding slot\n",
    "\n",
    "The frequency of sinusoidal is decreasing with the vector dimension (as i grows)\n",
    "\n",
    "So we got a vector with pairs of sin/cosines for each frequency.\n",
    "\n",
    "$\\vec{p_t} = \\begin{bmatrix} \n",
    "\\sin({\\omega_1}.t)\\\\ \n",
    "\\cos({\\omega_1}.t)\\\\ \n",
    "\\\\\n",
    "\\sin({\\omega_2}.t)\\\\ \n",
    "\\cos({\\omega_2}.t)\\\\ \n",
    "\\\\\n",
    "\\vdots\\\\ \n",
    "\\\\\n",
    "\\sin({\\omega_{d/2}}.t)\\\\ \n",
    "\\cos({\\omega_{d/2}}.t) \n",
    "\\end{bmatrix}_{d \\times 1}$\n",
    "\n",
    "It's like the encoding of numbers in binary format: \n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "  0: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  8: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  1: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  9: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  2: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  10: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\ \n",
    "  3: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  11: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{0}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\ \n",
    "  4: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  12: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  5: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  13: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{0}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "  6: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} & & \n",
    "  14: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{0}} \\\\\n",
    "  7: \\ \\ \\ \\ \\color{orange}{\\texttt{0}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} & & \n",
    "  15: \\ \\ \\ \\ \\color{orange}{\\texttt{1}} \\ \\ \\color{green}{\\texttt{1}} \\ \\ \\color{blue}{\\texttt{1}} \\ \\ \\color{red}{\\texttt{1}} \\\\\n",
    "\\end{align}$\n",
    "\n",
    "where last bit is alternating on every number and previous on every 2 numbers, and so on. So instead of using bits (which would be a waste of space), we use continuous float variant with sinuosidal functions..\n",
    "\n",
    "## References\n",
    "\n",
    "- https://erdem.pl/2021/05/understanding-positional-encoding-in-transformers\n",
    "- https://kazemnejad.com/blog/transformer_architecture_positional_encoding/\n",
    "- https://kikaben.com/transformers-positional-encoding/\n",
    "- http://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding\n",
    "- https://towardsdatascience.com/master-positional-encoding-part-i-63c05d90a0c3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class PositionalEncoder(nn.Module):\n",
    "    ''' Module to encode position in a transformer like model. \n",
    "    \n",
    "\n",
    "    Args:\n",
    "        max_seq_len (int): max length of sequence, aka L\n",
    "        embedding_dim (int): dimension of embeddings in model, aka d\n",
    "        dropout (float):  dropout rate. 0. for no dropout. \n",
    "        is_learned (bool): true if the position is learned through gradient descent or given (non differentiable) as defined in original paper Attention is all you need, https://arxiv.org/abs/1706.03762        \n",
    "        n (int): user defined scalar set by default to 10000 as in paper        \n",
    "    '''    \n",
    "        \n",
    "    def positional_encoding(self):\n",
    "        pos = torch.arange(0,self.max_seq_len).repeat(self.embedding_dim, 1)\n",
    "        i = torch.arange(0,self.embedding_dim)\n",
    "        k = i // 2        \n",
    "        wt = pos.T/(self.n**(2*k/self.embedding_dim))\n",
    "        sin = torch.sin(wt)\n",
    "        cos = torch.cos(wt)\n",
    "        pe = torch.zeros((self.max_seq_len,self.embedding_dim))\n",
    "        pe[:,0::2]= sin[:,0::2]\n",
    "        pe[:,1::2]=cos[:,1::2]\n",
    "        return pe\n",
    "\n",
    "    def __init__(self, max_seq_len: int, embedding_dim: int, dropout: float = 0.,is_learned: bool = True, n:int=10000) -> None:\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.is_learned = is_learned\n",
    "        self.n = n\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        if self.is_learned:            \n",
    "            pos = torch.arange(0, self.max_seq_len, dtype=torch.long).unsqueeze(0) # shape (1, max_seq_len)\n",
    "            self.register_buffer('pos', pos)\n",
    "            self.pos_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        else:\n",
    "            pos_encodings = self.positional_encoding().unsqueeze(0)\n",
    "            pos_encodings.requires_grad_(False)\n",
    "            # a buffer is a state in module which is not a parameter (learned)\n",
    "            self.register_buffer('pos_encodings',pos_encodings)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if self.is_learned:            \n",
    "            x = x + self.pos_embedding(self.pos)\n",
    "        else:\n",
    "            x= x + self.pos_encodings\n",
    "        return self.dropout(x)            \n",
    "                        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "seq_len = 20\n",
    "embedding_dim = 50\n",
    "pos_enc = PositionalEncoder(max_seq_len=seq_len, embedding_dim=embedding_dim, dropout=0.1, is_learned=False) \n",
    "pos_enc.to(device)\n",
    "pos_enc.positional_encoding()       \n",
    "x = torch.zeros(1, seq_len, embedding_dim, device=device)\n",
    "pe = pos_enc.forward(x)\n",
    "np_pe = pe.cpu().detach().numpy()\n",
    "plt.pcolormesh(np_pe[0].T, cmap='RdBu')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Depth')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanisms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def unidirectional_mask(seq_len:int):\n",
    "    inverse_mask = torch.triu(torch.ones((1, seq_len,seq_len)), diagonal=1).type(torch.uint8)\n",
    "    mask = (inverse_mask==0)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = unidirectional_mask(20)\n",
    "plt.imshow(mask[0])\n",
    "plt.colorbar()\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#| export\n",
    "def attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, mask: torch.Tensor=None, dropout: torch.nn.Module=None):\n",
    "    d_k = query.size(-1)    \n",
    "    scores = (query @ key.transpose(-2,-1)) /(torch.sqrt(torch.tensor(d_k, requires_grad=False)))\n",
    "    if mask is not None:\n",
    "        # mask scores with -inf. If we masked_fill(mask, -inf), it would mask when mask ==1 (is true). We want to mask when it's 0.\n",
    "        scores = scores.masked_fill(mask==0, float('-inf'))     \n",
    "    soft = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:        \n",
    "        soft = dropout(soft)        \n",
    "    atn = soft @ value\n",
    "    assert atn.shape == query.shape, \"atn shape should be the same as input tensors key, query, value\"\n",
    "    return atn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Q = torch.randn((1, seq_len, embedding_dim),generator=g, device=device)    \n",
    "K = torch.randn((1, seq_len, embedding_dim),generator=g, device=device)    \n",
    "V = torch.randn((1, seq_len, embedding_dim),generator=g, device=device)    \n",
    "print(seq_len, embedding_dim)\n",
    "mask = unidirectional_mask(seq_len).to(device)\n",
    "atn = attention(Q,K,V, mask=mask, dropout=torch.nn.Dropout(0.1))\n",
    "plt.imshow(atn[0].cpu().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"\"\"Object for holding a batch of data with mask during training.\"\"\"\n",
    "\n",
    "    def __init__(self, src, tgt=None, pad=2):  # 2 = <blank>\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if tgt is not None:\n",
    "            self.tgt = tgt[:, :-1]\n",
    "            self.tgt_y = tgt[:, 1:]\n",
    "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
    "            self.ntokens = (self.tgt_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & unidirectional_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        \n",
    "        return tgt_mask\n",
    "\n",
    "def data_gen(V, batch_size, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.randint(1, V, size=(batch_size, 10))\n",
    "        data[:, 0] = 1\n",
    "        src = data.requires_grad_(False).clone().detach()\n",
    "        tgt = data.requires_grad_(False).clone().detach()\n",
    "        yield Batch(src, tgt, 0)\n",
    "        \n",
    "\n",
    "for b in data_gen(V=5, batch_size=3, nbatches=2):\n",
    "    print(b.src)\n",
    "    print(b.tgt)\n",
    "    print(b.tgt_mask.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention mechanism\n",
    "Based on formal algorithm for transformers (https://arxiv.org/abs/2207.09238) where it can be used for different attention architectures like \n",
    "- encoder-decoder/seq-to-seq (very first transformer), \n",
    "- encoder-only (bert), \n",
    "- decoder-only (gpt-*, gopher). \n",
    "\n",
    "### Encoder-decoder architecture use 2 sequences as input\n",
    "- context sequence of tokens (z), which is used to encode context as a vector per token with bidirectional attention\n",
    "- primary sequence of tokens (x), which is used to decode both the encoded context and a masked primary sequence (tokens in primary sequence that precedes current). This is used to train a translator from FR to EN for instance. FR tokens are the context sequence and EN tokens are the primary sequence (target)\n",
    " \n",
    "### Encoder only architecture use 1 primary sequence as input\n",
    "Given a primary input sequence (x) with some tokens masked out, the goal is to recover the masked tokens. the goal is to learn a generally usefull representation of text. Uses a bidirectional attention\n",
    "\n",
    "### Decoder only architecture use 1 primary sequence as input\n",
    "Autoregressive language modelling where the goal is to predict the next token of a primary token sequence (x). Uses a unidirectional attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, d:int, dropout: float=0., bias: bool=True) -> None:\n",
    "        super().__init__()   \n",
    "        self.d = d        \n",
    "        self.wq = nn.Linear(d,d, bias=bias)\n",
    "        self.wk = nn.Linear(d,d, bias=bias)\n",
    "        self.wv = nn.Linear(d,d, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor, z: torch.Tensor=None, mask: torch.Tensor=None):\n",
    "            q = self.wq(x)\n",
    "            k = self.wk(z)\n",
    "            v = self.wk(z)\n",
    "            atn = attention(q, k, v, mask=mask, dropout=self.dropout)\n",
    "            return atn\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "e = torch.rand((1,seq_len, embedding_dim),generator=g)\n",
    "sh = SingleHeadAttention(d=embedding_dim, dropout=0.)\n",
    "satn = sh(e,e, mask=unidirectional_mask(seq_len))\n",
    "plt.imshow(satn[0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------\n",
    "# Test my implentation with pytorch implementation, based on multihead with one head.\n",
    "# pytorch  has 2 seq len: 1 for the source (context or z), 1 for the target (primary or x)\n",
    "# In our implementation, seqlen is for both context and primary sequence. \n",
    "# mask correspond to attn_mask in pytorch, because its a mask on the target(primary,x)\n",
    "# \n",
    "e = torch.rand((1,seq_len, embedding_dim), generator=g)\n",
    "sh = SingleHeadAttention(d=embedding_dim, dropout=0.)\n",
    "satn_mask = sh(e,e, mask=unidirectional_mask(seq_len))\n",
    "satn_no_mask = sh(e, e)\n",
    "# torch implemen could not give the same results due to the fact that weight init is not the same (different random nb gen. )\n",
    "\n",
    "torch_sh = torch.nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=1, dropout=0.,bias=True, batch_first=True)\n",
    "torch_satn_mask,_ = torch_sh(e,e,e, attn_mask=unidirectional_mask(seq_len))\n",
    "torch_satn_no_mask, _ = torch_sh(e, e, e)\n",
    "\n",
    "plt.imshow(torch_satn_mask[0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(satn_mask[0].detach().numpy())\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_satn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    ''' Multihead attention module as defined in Formal algorithm for transformers (https://arxiv.org/abs/2207.09238)\n",
    "    It can be used for different attention architectures like encoder-decoder/seq-to-seq (very first transformer), \n",
    "    encoder-only (bert), decoder-only (gpt-*, gopher). \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, d:int, h: int, dropout: float=0., bias:bool=True):\n",
    "        super().__init__()\n",
    "        g = torch.Generator().manual_seed(42)        \n",
    "        self.d = d\n",
    "        self.h = h\n",
    "        #self.w_weights = torch.randn((d, d_in), generator=g)\n",
    "        #self.k_weights = torch.randn((d, d_in), generator=g)\n",
    "        #self.v_weights = torch.randn((d, d_in), generator=g)\n",
    "        \n",
    "\n",
    "    def forward(self, x: torch.Tensor, z: torch.Tensor=None, mask: torch.Tensor=None):\n",
    "        '''_summary_\n",
    "        \n",
    "        \n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): primary sequence \n",
    "            z (torch.Tensor): context sequence, only for encoder-decoder architecture. \n",
    "            mask (torch.Tensor, optional): _description_. Defaults to None.\n",
    "        '''\n",
    "        ...\n",
    "\n",
    "\n",
    "e = torch.rand((1,seq_len, embedding_dim))\n",
    "attn = MultiHeadAttention(d=embedding_dim, h=2)\n",
    "attn(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test with multihead_attn = nn.MultiheadAttention(embed_dim, num_heads) so that result is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export(\"./components.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02e373710fe8ee5e4fefe94e05d12897bb1dce14bb31cd26162c8283164a7cc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
