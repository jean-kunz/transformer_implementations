{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp components\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers components to be assembled in a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\\ export\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "device = torch.device('mps')\n",
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "- Token embeddings, using torch embedding lookup\n",
    "- Positional embedding, which can be fixed or learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\ export\n",
    "\n",
    "class TokenEmbeddings(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim=embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 6\n",
    "embedding_dim = 50\n",
    "emb = TokenEmbeddings(vocab_size, embedding_dim=embedding_dim)\n",
    "emb.to(device)\n",
    "x = torch.tensor([0,1,2,1], dtype=torch.long, device=device)\n",
    "emb_x = emb(x)\n",
    "assert emb_x.shape==(x.shape[0],embedding_dim)\n",
    "assert torch.equal(emb_x[1],emb_x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "As each word in a sentence simultaneously flows through the Transformer’s encoder/decoder stack, The model itself doesn’t have any sense of position/order for each word. Consequently, there’s still the need for a way to incorporate the order of the words into our model.\n",
    "So we give the model some sense of position of the token in the sequence. \n",
    "\n",
    "#### Potential solutions:\n",
    "\n",
    "The first idea that might come to mind is to assign a number to each time-step within the [0, 1] range in which 0 means the first word and 1 is the last time-step. One of the problems it will introduce is that you can’t figure out how many words are present within a specific range. In other words, time-step delta doesn’t have consistent meaning across different sentences.\n",
    "\n",
    "Another idea is to assign a number to each time-step linearly. That is, the first word is given “1”, the second word is given “2”, and so on. The problem with this approach is that not only the values could get quite large, but also our model can face sentences longer than the ones in training.\n",
    "\n",
    "Ideally, the following criteria should be satisfied:\n",
    "\n",
    "- It should output a unique encoding for each time-step (word’s position in a sentence)\n",
    "- Distance between any two time-steps should be consistent across sentences with different lengths.\n",
    "- Our model should generalize to longer sentences without any efforts. Its values should be bounded.\n",
    "- It must be deterministic.\n",
    "\n",
    "Proposed solutions:\n",
    "\n",
    "The initial solution that was proposed isn’t a single number. Instead, it’s a d-dimensional vector that contains information about a specific position in a sentence. And  this vector is used to equip each word with information about its position in a sentence. \n",
    "\n",
    "$P(k,2i) = sin(\\frac{k}{n^{\\frac{2i}{d}}})$\n",
    "\n",
    "$P(k,2i+1) = cos(\\frac{k}{n^{\\frac{2i}{d}}})$\n",
    "\n",
    "where : \n",
    "- L: sequence length\n",
    "- k: position of token in input sequence, $0<=k<L/2$\n",
    "- d: dimension of embedding\n",
    "- P(k,j): position function to map a position k in sequence to index (k,j) in positional matrix\n",
    "- n: user defined scalar (ex: 10'000)\n",
    "- i: column indice in positional matrix $0<=i<d/2$\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\\ export\n",
    "\n",
    "class PositionalEmbeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_seq_len: int, embedding_dim: int, is_learned: bool = True) -> None:\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.is_learned = is_learned\n",
    "        if self.is_learned:\n",
    "            self.pos_embedding = nn.Embedding(max_seq_len, embedding_dim)\n",
    "        else:\n",
    "            self.pos_embedding = torch.zeros(max_seq_len, embedding_dim)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        ...\n",
    "        \n",
    "\n",
    "\n",
    "pos_emb = PositionalEmbeddings(max_seq_len=5, embedding_dim=embedding_dim, is_learned=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('transformers')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02e373710fe8ee5e4fefe94e05d12897bb1dce14bb31cd26162c8283164a7cc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
