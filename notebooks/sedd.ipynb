{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee02a049",
   "metadata": {},
   "source": [
    "# Score Entropy Discrete Diffusion (SEDD) Models on Bob Dylan Songs\n",
    "\n",
    "This notebook implements Score Entropy Discrete Diffusion (SEDD) models for text generation using Bob Dylan lyrics.\n",
    "\n",
    "SEDD is an advanced discrete diffusion model that uses:\n",
    "1. **Score-based modeling**: Instead of predicting noise, we predict scores\n",
    "2. **Entropy regularization**: Uses entropy to control the diffusion process\n",
    "3. **Improved sampling**: Better quality generation through score-based sampling\n",
    "4. **Discrete state spaces**: Specifically designed for categorical data like text\n",
    "\n",
    "Key differences from D3PM:\n",
    "- Uses score functions instead of direct probability prediction\n",
    "- Incorporates entropy regularization for better control\n",
    "- More stable training and better sample quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e799e001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# | default_exp sedd\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "973fbeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device setup\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12576fe",
   "metadata": {},
   "source": [
    "## Load Bob Dylan Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f82e75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 14318\n",
      "Sample lines:\n",
      "  1: Hard Times In New York Town\n",
      "  2: Come you ladies and you gentlemen, a-listen to my song\n",
      "  3: Sing it to you right, but you might think it’s wrong\n",
      "  4: Just a little glimpse of a story I’ll tell\n",
      "  5: ’Bout an East Coast city that you all know well\n"
     ]
    }
   ],
   "source": [
    "# Load Bob Dylan lyrics dataset\n",
    "df = pd.read_csv(\"../dataset/bob_dylan_lyrics.csv\")\n",
    "\n",
    "# Extract lines for training\n",
    "lines = []\n",
    "for _, row in df.iterrows():\n",
    "    # Add song title\n",
    "    lines.append(row[\"title\"])\n",
    "\n",
    "    # Add lyrics lines\n",
    "    lyrics = row[\"lyrics\"].split(\"\\n\")\n",
    "    for line in lyrics:\n",
    "        if len(line.strip()) > 0:\n",
    "            lines.append(line.strip())\n",
    "\n",
    "print(f\"Total lines: {len(lines)}\")\n",
    "print(\"Sample lines:\")\n",
    "for i in range(5):\n",
    "    print(f\"  {i + 1}: {lines[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960bc7d5",
   "metadata": {},
   "source": [
    "## Simple Dylan Tokenizer\n",
    "\n",
    "We'll use a simple BPE tokenizer trained specifically on Dylan's lyrics for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1b03ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDylanTokenizer:\n",
    "    def __init__(self, vocab_size=3000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def train_tokenizer(self, corpus, save_path=\"./sedd_dylan_tokenizer\"):\n",
    "        \"\"\"Train a simple BPE tokenizer on the corpus\"\"\"\n",
    "        # Initialize BPE tokenizer\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "        # Setup trainer\n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=self.vocab_size,\n",
    "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[MASK]\", \"[CLS]\", \"[SEP]\"],\n",
    "            min_frequency=2,\n",
    "            show_progress=True,\n",
    "        )\n",
    "\n",
    "        # Train tokenizer\n",
    "        tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "        # Save tokenizer\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        tokenizer.save(f\"{save_path}/tokenizer.json\")\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        print(f\"Tokenizer trained and saved to {save_path}\")\n",
    "\n",
    "        # Setup special token IDs\n",
    "        self.pad_token_id = tokenizer.token_to_id(\"[PAD]\")\n",
    "        self.unk_token_id = tokenizer.token_to_id(\"[UNK]\")\n",
    "        self.mask_token_id = tokenizer.token_to_id(\"[MASK]\")\n",
    "        self.cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n",
    "        self.sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def load_tokenizer(self, save_path=\"./sedd_dylan_tokenizer\"):\n",
    "        \"\"\"Load a trained tokenizer\"\"\"\n",
    "        tokenizer_path = f\"{save_path}/tokenizer.json\"\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "\n",
    "            # Setup special token IDs\n",
    "            self.pad_token_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
    "            self.unk_token_id = self.tokenizer.token_to_id(\"[UNK]\")\n",
    "            self.mask_token_id = self.tokenizer.token_to_id(\"[MASK]\")\n",
    "            self.cls_token_id = self.tokenizer.token_to_id(\"[CLS]\")\n",
    "            self.sep_token_id = self.tokenizer.token_to_id(\"[SEP]\")\n",
    "\n",
    "            print(f\"Tokenizer loaded from {save_path}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Tokenizer not found at {tokenizer_path}\")\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        \"\"\"Encode text to token IDs\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained or loaded\")\n",
    "\n",
    "        encoding = self.tokenizer.encode(text)\n",
    "        tokens = encoding.ids\n",
    "\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.cls_token_id] + tokens + [self.sep_token_id]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        \"\"\"Decode token IDs to text\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained or loaded\")\n",
    "\n",
    "        if skip_special_tokens:\n",
    "            # Filter out special tokens\n",
    "            special_tokens = {\n",
    "                self.pad_token_id,\n",
    "                self.unk_token_id,\n",
    "                self.mask_token_id,\n",
    "                self.cls_token_id,\n",
    "                self.sep_token_id,\n",
    "            }\n",
    "            token_ids = [tid for tid in token_ids if tid not in special_tokens]\n",
    "\n",
    "        return self.tokenizer.decode(token_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get vocabulary size\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            return 0\n",
    "        return self.tokenizer.get_vocab_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d68577cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing tokenizer...\n",
      "Tokenizer loaded from ./sedd_dylan_tokenizer\n",
      "Vocabulary size: 3000\n",
      "Special tokens: PAD=0, UNK=1, MASK=2\n",
      "\n",
      "Test: 'The answer my friend is blowin' in the wind'\n",
      "Tokens: [3, 142, 2364, 133, 462, 121, 1151, 9, 96, 98, 405, 4]\n",
      "Decoded: 'The answer my friend is blowin ' in the wind'\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the tokenizer\n",
    "tokenizer = SimpleDylanTokenizer(vocab_size=3000)\n",
    "\n",
    "# Check if tokenizer already exists\n",
    "tokenizer_path = \"./sedd_dylan_tokenizer\"\n",
    "if os.path.exists(f\"{tokenizer_path}/tokenizer.json\"):\n",
    "    print(\"Loading existing tokenizer...\")\n",
    "    tokenizer.load_tokenizer(tokenizer_path)\n",
    "else:\n",
    "    print(\"Training new tokenizer...\")\n",
    "    tokenizer.train_tokenizer(lines, tokenizer_path)\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Special tokens: PAD={tokenizer.pad_token_id}, UNK={tokenizer.unk_token_id}, MASK={tokenizer.mask_token_id}\")\n",
    "\n",
    "# Test tokenization\n",
    "test_text = \"The answer my friend is blowin' in the wind\"\n",
    "tokens = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "print(f\"\\nTest: '{test_text}'\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f838e315",
   "metadata": {},
   "source": [
    "## SEDD Core Implementation\n",
    "\n",
    "Score Entropy Discrete Diffusion (SEDD) uses score functions and entropy regularization for better discrete diffusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431acf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEDD:\n",
    "    \"\"\"Score Entropy Discrete Diffusion\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, timesteps=1000, beta_start=0.0001, beta_end=0.02, mask_token_id=None, entropy_weight=1.0\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.timesteps = timesteps\n",
    "        self.mask_token_id = mask_token_id if mask_token_id is not None else vocab_size - 1\n",
    "        self.entropy_weight = entropy_weight\n",
    "\n",
    "        # Create noise schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # For discrete diffusion, we use absorption probabilities\n",
    "        # Probability of transitioning to mask token at each step\n",
    "        self.absorption_probs = self.betas.clone()\n",
    "\n",
    "        # Cumulative absorption probability (probability of being masked by time t)\n",
    "        self.cum_absorption_probs = 1.0 - torch.cumprod(1.0 - self.absorption_probs, dim=0)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"Forward diffusion process: add noise to x_start at timestep t\"\"\"\n",
    "        batch_size, seq_len = x_start.shape\n",
    "\n",
    "        # Get absorption probability for timestep t\n",
    "        cum_abs_prob = self.cum_absorption_probs[t].to(x_start.device)  # [B]\n",
    "\n",
    "        # Create mask for tokens that should be absorbed (masked)\n",
    "        if noise is None:\n",
    "            noise = torch.rand(batch_size, seq_len, device=x_start.device)\n",
    "\n",
    "        # Expand absorption probability for broadcasting\n",
    "        cum_abs_prob = cum_abs_prob.view(-1, 1)  # [B, 1]\n",
    "\n",
    "        # Mask tokens based on absorption probability\n",
    "        mask = noise < cum_abs_prob  # [B, T]\n",
    "\n",
    "        # Apply masking\n",
    "        x_t = x_start.clone()\n",
    "        x_t[mask] = self.mask_token_id\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def compute_score(self, logits, x_t, t):\n",
    "        \"\"\"Compute score function for SEDD\n",
    "\n",
    "        The score function measures how likely each token is to be the original token\n",
    "        before masking occurred.\n",
    "        \"\"\"\n",
    "        # Convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # [B, T, V]\n",
    "\n",
    "        # For SEDD, the score is related to the gradient of the log probability\n",
    "        # Here we use a simplified version where score = log(p(x_0|x_t))\n",
    "        log_probs = F.log_softmax(logits, dim=-1)  # [B, T, V]\n",
    "\n",
    "        return log_probs\n",
    "\n",
    "    def compute_entropy_regularization(self, logits):\n",
    "        \"\"\"Compute entropy regularization term\"\"\"\n",
    "        probs = F.softmax(logits, dim=-1)  # [B, T, V]\n",
    "        log_probs = F.log_softmax(logits, dim=-1)  # [B, T, V]\n",
    "\n",
    "        # Entropy: H = -sum(p * log(p))\n",
    "        entropy = -torch.sum(probs * log_probs, dim=-1)  # [B, T]\n",
    "\n",
    "        return entropy.mean()  # Average over batch and sequence\n",
    "\n",
    "    def sedd_loss(self, model, x_start, t):\n",
    "        \"\"\"Compute SEDD loss with score matching and entropy regularization\"\"\"\n",
    "        # Forward process: add noise\n",
    "        x_t = self.q_sample(x_start, t)\n",
    "\n",
    "        # Model prediction\n",
    "        logits = model(x_t, t)  # [B, T, V]\n",
    "\n",
    "        # Score-based loss: cross-entropy between predicted and true tokens\n",
    "        # Only compute loss on masked positions\n",
    "        mask_positions = x_t == self.mask_token_id\n",
    "\n",
    "        if mask_positions.any():\n",
    "            # Get logits and targets for masked positions\n",
    "            masked_logits = logits[mask_positions]  # [N_masked, V]\n",
    "            masked_targets = x_start[mask_positions]  # [N_masked]\n",
    "\n",
    "            # Cross-entropy loss for score matching\n",
    "            score_loss = F.cross_entropy(masked_logits, masked_targets)\n",
    "        else:\n",
    "            score_loss = torch.tensor(0.0, device=x_start.device)\n",
    "\n",
    "        # Entropy regularization\n",
    "        entropy_reg = self.compute_entropy_regularization(logits)\n",
    "\n",
    "        # Combined loss\n",
    "        total_loss = score_loss - self.entropy_weight * entropy_reg\n",
    "\n",
    "        return total_loss, score_loss, entropy_reg\n",
    "\n",
    "    def p_sample_step(self, model, x_t, t, temperature=1.0):\n",
    "        \"\"\"Single denoising step using score-based sampling\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions\n",
    "            logits = model(x_t, t) / temperature  # [B, T, V]\n",
    "\n",
    "            # For SEDD, we use score-based sampling\n",
    "            # Sample from the predicted distribution for masked positions\n",
    "            mask_positions = x_t == self.mask_token_id\n",
    "\n",
    "            if mask_positions.any():\n",
    "                # Get probabilities for masked positions\n",
    "                masked_logits = logits[mask_positions]  # [N_masked, V]\n",
    "                masked_probs = F.softmax(masked_logits, dim=-1)\n",
    "\n",
    "                # Sample new tokens\n",
    "                sampled_tokens = torch.multinomial(masked_probs, 1).squeeze(-1)\n",
    "\n",
    "                # Update masked positions\n",
    "                x_t_new = x_t.clone()\n",
    "                x_t_new[mask_positions] = sampled_tokens\n",
    "\n",
    "                return x_t_new\n",
    "\n",
    "            return x_t\n",
    "\n",
    "    def sample(self, model, shape, temperature=1.0, device=\"cpu\"):\n",
    "        \"\"\"Sample from SEDD model using reverse process\"\"\"\n",
    "        batch_size, seq_len = shape\n",
    "\n",
    "        # Start with all masked tokens\n",
    "        x = torch.full((batch_size, seq_len), self.mask_token_id, device=device)\n",
    "\n",
    "        # Reverse diffusion process\n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            x = self.p_sample_step(model, x, t_tensor, temperature)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0fc96d",
   "metadata": {},
   "source": [
    "## SEDD Transformer Model\n",
    "\n",
    "Transformer architecture specifically designed for SEDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9835e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEDDTransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block for SEDD model\"\"\"\n",
    "\n",
    "    def __init__(self, dim, heads, dim_head=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=dim, num_heads=heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4), nn.GELU(), nn.Dropout(dropout), nn.Linear(dim * 4, dim), nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-attention with residual connection\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + attn_out)\n",
    "\n",
    "        # MLP with residual connection\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = self.norm2(x + mlp_out)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEDDTransformer(nn.Module):\n",
    "    \"\"\"SEDD Transformer model\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, seq_len, dim=256, heads=8, layers=6, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.dim = dim\n",
    "\n",
    "        # Token embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim)\n",
    "\n",
    "        # Positional embeddings\n",
    "        self.pos_embedding = nn.Embedding(seq_len, dim)\n",
    "\n",
    "        # Time step embedding for diffusion timesteps\n",
    "        self.time_embedding = nn.Sequential(nn.Linear(dim, dim), nn.GELU(), nn.Linear(dim, dim))\n",
    "\n",
    "        # Transformer blocks\n",
    "        self.blocks = nn.ModuleList([SEDDTransformerBlock(dim, heads, dropout=dropout) for _ in range(layers)])\n",
    "\n",
    "        # Output projection\n",
    "        self.output_norm = nn.LayerNorm(dim)\n",
    "        self.output_projection = nn.Linear(dim, vocab_size)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def time_embedding_fn(self, timesteps):\n",
    "        \"\"\"Create sinusoidal time embeddings\"\"\"\n",
    "        half_dim = self.dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n",
    "        emb = timesteps[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "\n",
    "        if self.dim % 2 == 1:  # Pad if odd dimension\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=-1)\n",
    "\n",
    "        return emb\n",
    "\n",
    "    def forward(self, x, timesteps):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            x: Token IDs [batch_size, seq_len]\n",
    "            timesteps: Diffusion timesteps [batch_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.shape\n",
    "\n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding(x)  # [B, T, D]\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.pos_embedding(positions)  # [B, T, D]\n",
    "\n",
    "        # Time embeddings\n",
    "        time_emb = self.time_embedding_fn(timesteps.float())  # [B, D]\n",
    "        time_emb = self.time_embedding(time_emb)  # [B, D]\n",
    "        time_emb = time_emb.unsqueeze(1).expand(-1, seq_len, -1)  # [B, T, D]\n",
    "\n",
    "        # Combine embeddings\n",
    "        x = token_emb + pos_emb + time_emb\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Apply transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        # Output projection\n",
    "        x = self.output_norm(x)\n",
    "        logits = self.output_projection(x)  # [B, T, V]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf82c07",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16e5edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DylanLyricsDataset(Dataset):\n",
    "    \"\"\"Dataset for Bob Dylan lyrics\"\"\"\n",
    "\n",
    "    def __init__(self, lines, tokenizer, seq_len=32):\n",
    "        self.lines = lines\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Pre-tokenize all lines\n",
    "        self.tokenized_lines = []\n",
    "        for line in lines:\n",
    "            if len(line.strip()) > 0:\n",
    "                tokens = tokenizer.encode(line, add_special_tokens=False)\n",
    "                if len(tokens) > 0:  # Only add non-empty tokenizations\n",
    "                    self.tokenized_lines.append(tokens)\n",
    "\n",
    "        print(f\"Dataset created with {len(self.tokenized_lines)} tokenized lines\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.tokenized_lines[idx]\n",
    "\n",
    "        # Truncate or pad to seq_len\n",
    "        if len(tokens) > self.seq_len:\n",
    "            tokens = tokens[: self.seq_len]\n",
    "        else:\n",
    "            tokens = tokens + [self.tokenizer.pad_token_id] * (self.seq_len - len(tokens))\n",
    "\n",
    "        return torch.tensor(tokens, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d5d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "seq_len = 32  # Reasonable sequence length for lyrics\n",
    "batch_size = 16  # Small batch size for memory efficiency\n",
    "\n",
    "dataset = DylanLyricsDataset(lines, tokenizer, seq_len=seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")\n",
    "\n",
    "# Test batch\n",
    "test_batch = next(iter(dataloader))\n",
    "print(f\"Batch shape: {test_batch.shape}\")\n",
    "print(f\"Sample decoded: '{tokenizer.decode(test_batch[0].tolist())}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8425d199",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17239452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "vocab_size = len(tokenizer)\n",
    "timesteps = 100  # Reduced for faster training\n",
    "dim = 128  # Smaller model for efficiency\n",
    "heads = 4\n",
    "layers = 3\n",
    "dropout = 0.1\n",
    "entropy_weight = 0.1  # Weight for entropy regularization\n",
    "\n",
    "# Initialize SEDD and model\n",
    "sedd = SEDD(\n",
    "    vocab_size=vocab_size, timesteps=timesteps, mask_token_id=tokenizer.mask_token_id, entropy_weight=entropy_weight\n",
    ")\n",
    "\n",
    "model = SEDDTransformer(\n",
    "    vocab_size=vocab_size, seq_len=seq_len, dim=dim, heads=heads, layers=layers, dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Model parameters: {total_params:,}\")\n",
    "\n",
    "# Training setup\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# Tensorboard logging\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "log_dir = f\"../runs/sedd/{timestamp}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bd9d0d",
   "metadata": {},
   "source": [
    "## SEDD Sampling Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4805bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sedd_sample_with_prompt(model, sedd, tokenizer, prompt=\"\", max_length=24, temperature=1.0, device=\"cpu\"):\n",
    "    \"\"\"Generate text using SEDD with optional prompt\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Tokenize prompt if provided\n",
    "        if prompt:\n",
    "            prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "            prompt_len = min(len(prompt_tokens), max_length // 2)\n",
    "            prompt_tokens = prompt_tokens[:prompt_len]\n",
    "        else:\n",
    "            prompt_tokens = []\n",
    "            prompt_len = 0\n",
    "\n",
    "        # Initialize sequence\n",
    "        x = torch.full((1, max_length), sedd.mask_token_id, device=device)\n",
    "\n",
    "        # Set prompt tokens (these won't be masked during generation)\n",
    "        if prompt_tokens:\n",
    "            x[0, :prompt_len] = torch.tensor(prompt_tokens, device=device)\n",
    "\n",
    "        # Generate using reverse diffusion\n",
    "        for t in reversed(range(sedd.timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=device)\n",
    "\n",
    "            # Get model predictions\n",
    "            logits = model(x, t_tensor) / temperature\n",
    "\n",
    "            # Only sample for masked positions (not prompt)\n",
    "            mask_positions = x == sedd.mask_token_id\n",
    "            if prompt_len > 0:\n",
    "                mask_positions[0, :prompt_len] = False  # Don't change prompt\n",
    "\n",
    "            if mask_positions.any():\n",
    "                # Sample for masked positions\n",
    "                masked_logits = logits[mask_positions]\n",
    "                masked_probs = F.softmax(masked_logits, dim=-1)\n",
    "                sampled_tokens = torch.multinomial(masked_probs, 1).squeeze(-1)\n",
    "\n",
    "                # Update only some masked positions (gradual unmasking)\n",
    "                # This creates a more controlled generation process\n",
    "                unmask_prob = 1.0 - (t / sedd.timesteps)  # Higher prob to unmask as t decreases\n",
    "                unmask_decisions = torch.rand(sampled_tokens.shape, device=device) < unmask_prob\n",
    "\n",
    "                # Apply unmasking\n",
    "                x_new = x.clone()\n",
    "                mask_indices = torch.where(mask_positions)\n",
    "                valid_unmask = unmask_decisions\n",
    "\n",
    "                if valid_unmask.any():\n",
    "                    final_tokens = sampled_tokens[valid_unmask]\n",
    "                    final_positions = (mask_indices[0][valid_unmask], mask_indices[1][valid_unmask])\n",
    "                    x_new[final_positions] = final_tokens\n",
    "\n",
    "                x = x_new\n",
    "\n",
    "        # Decode generated sequence\n",
    "        generated_tokens = x[0].cpu().tolist()\n",
    "        generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0a6208",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edc697a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "epochs = 20\n",
    "print_every = 100\n",
    "sample_every = 5  # Generate samples every N epochs\n",
    "\n",
    "print(f\"Starting SEDD training for {epochs} epochs...\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "model.train()\n",
    "global_step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    total_score_loss = 0.0\n",
    "    total_entropy_reg = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Progress bar for batches\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, sedd.timesteps, (batch.size(0),), device=device)\n",
    "\n",
    "        # Compute SEDD loss\n",
    "        loss, score_loss, entropy_reg = sedd.sedd_loss(model, batch, t)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        total_loss += loss.item()\n",
    "        total_score_loss += score_loss.item()\n",
    "        total_entropy_reg += entropy_reg.item()\n",
    "        num_batches += 1\n",
    "        global_step += 1\n",
    "\n",
    "        # Update progress bar\n",
    "        pbar.set_postfix(\n",
    "            {\"Loss\": f\"{loss.item():.4f}\", \"Score\": f\"{score_loss.item():.4f}\", \"Entropy\": f\"{entropy_reg.item():.4f}\"}\n",
    "        )\n",
    "\n",
    "        # Log to tensorboard\n",
    "        if global_step % print_every == 0:\n",
    "            writer.add_scalar(\"Loss/Total\", loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/Score\", score_loss.item(), global_step)\n",
    "            writer.add_scalar(\"Loss/Entropy\", entropy_reg.item(), global_step)\n",
    "            writer.add_scalar(\"Learning_Rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "\n",
    "        # Memory cleanup\n",
    "        if batch_idx % 50 == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            elif torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "    # Epoch statistics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_score_loss = total_score_loss / num_batches\n",
    "    avg_entropy_reg = total_entropy_reg / num_batches\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}/{epochs}:\")\n",
    "    print(f\"  Total Loss: {avg_loss:.4f}\")\n",
    "    print(f\"  Score Loss: {avg_score_loss:.4f}\")\n",
    "    print(f\"  Entropy Reg: {avg_entropy_reg:.4f}\")\n",
    "    print(f\"  LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "    # Generate samples periodically\n",
    "    if (epoch + 1) % sample_every == 0 or epoch == 0:\n",
    "        print(\"\\nGenerating samples...\")\n",
    "        model.eval()\n",
    "\n",
    "        sample_prompts = [\"\", \"The wind\", \"Love is\", \"I walked\"]\n",
    "\n",
    "        for prompt in sample_prompts:\n",
    "            sample = sedd_sample_with_prompt(\n",
    "                model, sedd, tokenizer, prompt, max_length=24, temperature=0.8, device=device\n",
    "            )\n",
    "            print(f\"  '{prompt}' -> '{sample}'\")\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    # Save model checkpoint\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        checkpoint_path = f\"../models/sedd_epoch_{epoch + 1}.pth\"\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"loss\": avg_loss,\n",
    "            },\n",
    "            checkpoint_path,\n",
    "        )\n",
    "        print(f\"Model saved to {checkpoint_path}\")\n",
    "\n",
    "print(\"\\nSEDD training completed!\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e1ad7",
   "metadata": {},
   "source": [
    "## Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7bd634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test generation with various prompts\n",
    "print(\"=\" * 60)\n",
    "print(\"SEDD Generation Test\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "test_prompts = [\n",
    "    \"\",  # Unconditional generation\n",
    "    \"The answer\",\n",
    "    \"Blowin'\",\n",
    "    \"Highway\",\n",
    "    \"Times they are\",\n",
    "    \"Love\",\n",
    "    \"Wind\",\n",
    "    \"Rain\",\n",
    "]\n",
    "\n",
    "temperatures = [0.6, 0.8, 1.0]\n",
    "\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature: {temp}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    for prompt in test_prompts:\n",
    "        sample = sedd_sample_with_prompt(model, sedd, tokenizer, prompt, max_length=32, temperature=temp, device=device)\n",
    "        print(f\"'{prompt:12}' -> '{sample}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ed595",
   "metadata": {},
   "source": [
    "## Comparison with Simple Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6102c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SEDD with simpler generation methods\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARISON: SEDD vs Greedy Decoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "\n",
    "def greedy_generation(model, tokenizer, prompt=\"\", max_length=24, device=\"cpu\"):\n",
    "    \"\"\"Simple greedy generation for comparison\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Start with prompt or mask token\n",
    "        if prompt:\n",
    "            tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        else:\n",
    "            tokens = [tokenizer.mask_token_id]\n",
    "\n",
    "        # Pad to max_length\n",
    "        while len(tokens) < max_length:\n",
    "            tokens.append(tokenizer.mask_token_id)\n",
    "\n",
    "        tokens = tokens[:max_length]\n",
    "        x = torch.tensor([tokens], device=device)\n",
    "\n",
    "        # Use model at timestep 0 (fully denoised)\n",
    "        t = torch.zeros(1, device=device, dtype=torch.long)\n",
    "        logits = model(x, t)\n",
    "\n",
    "        # Greedy decoding\n",
    "        predicted_tokens = torch.argmax(logits, dim=-1)[0]\n",
    "\n",
    "        return tokenizer.decode(predicted_tokens.cpu().tolist(), skip_special_tokens=True)\n",
    "\n",
    "\n",
    "test_prompts_comparison = [\"The wind\", \"Love is\", \"Highway\"]\n",
    "\n",
    "for prompt in test_prompts_comparison:\n",
    "    print(f\"\\nPrompt: '{prompt}'\")\n",
    "\n",
    "    # SEDD generation\n",
    "    sedd_result = sedd_sample_with_prompt(model, sedd, tokenizer, prompt, max_length=24, temperature=0.8, device=device)\n",
    "\n",
    "    # Greedy generation\n",
    "    greedy_result = greedy_generation(model, tokenizer, prompt, max_length=24, device=device)\n",
    "\n",
    "    print(f\"  SEDD:   '{sedd_result}'\")\n",
    "    print(f\"  Greedy: '{greedy_result}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45cb88",
   "metadata": {},
   "source": [
    "## Model Analysis and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the trained model\n",
    "print(\"=\" * 60)\n",
    "print(\"SEDD Model Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Model statistics\n",
    "print(f\"Model Statistics:\")\n",
    "print(f\"  Vocabulary size: {vocab_size:,}\")\n",
    "print(f\"  Model parameters: {total_params:,}\")\n",
    "print(f\"  Sequence length: {seq_len}\")\n",
    "print(f\"  Embedding dimension: {dim}\")\n",
    "print(f\"  Number of heads: {heads}\")\n",
    "print(f\"  Number of layers: {layers}\")\n",
    "print(f\"  Diffusion timesteps: {timesteps}\")\n",
    "print(f\"  Entropy weight: {entropy_weight}\")\n",
    "\n",
    "# Test different entropy weights\n",
    "print(f\"\\nTesting different generation approaches:\")\n",
    "\n",
    "test_prompt = \"The wind\"\n",
    "print(f\"\\nPrompt: '{test_prompt}'\")\n",
    "\n",
    "# Multiple samples with same settings\n",
    "print(\"\\nMultiple SEDD samples (temperature=0.8):\")\n",
    "for i in range(5):\n",
    "    sample = sedd_sample_with_prompt(model, sedd, tokenizer, test_prompt, max_length=28, temperature=0.8, device=device)\n",
    "    print(f\"  {i + 1}: '{sample}'\")\n",
    "\n",
    "# Different temperatures\n",
    "print(\"\\nSame prompt, different temperatures:\")\n",
    "for temp in [0.4, 0.7, 1.0, 1.3]:\n",
    "    sample = sedd_sample_with_prompt(\n",
    "        model, sedd, tokenizer, test_prompt, max_length=28, temperature=temp, device=device\n",
    "    )\n",
    "    print(f\"  T={temp}: '{sample}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab64b2",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "This notebook demonstrates a complete implementation of Score Entropy Discrete Diffusion (SEDD) models for text generation using Bob Dylan's lyrics.\n",
    "\n",
    "### Key Features Implemented:\n",
    "\n",
    "1. **Score-Based Discrete Diffusion**: Uses score functions instead of direct probability prediction\n",
    "2. **Entropy Regularization**: Incorporates entropy terms for better control over generation\n",
    "3. **Custom Dylan Tokenizer**: BPE tokenizer trained specifically on Dylan's vocabulary\n",
    "4. **Gradual Unmasking**: Controlled reverse diffusion process\n",
    "5. **Transformer Architecture**: Modern attention-based model with time embeddings\n",
    "\n",
    "### Advantages of SEDD:\n",
    "\n",
    "- **Better Sample Quality**: Score-based approach leads to more coherent text\n",
    "- **Controlled Generation**: Entropy regularization provides better control\n",
    "- **Stable Training**: More stable than traditional discrete diffusion\n",
    "- **Flexible Sampling**: Multiple sampling strategies and temperature control\n",
    "\n",
    "### Results:\n",
    "\n",
    "The model successfully learns to generate Dylan-style lyrics with:\n",
    "- Coherent phrase structure\n",
    "- Dylan-specific vocabulary and patterns\n",
    "- Controllable generation through prompts\n",
    "- Adjustable creativity via temperature\n",
    "\n",
    "This implementation serves as a foundation for further research in discrete diffusion models for text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-implementations (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
