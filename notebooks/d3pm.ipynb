{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e60c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n",
      "env: PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n"
     ]
    }
   ],
   "source": [
    "# | default_exp attention\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "%env PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4404c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from icecream import ic\n",
    "import math\n",
    "from my_transformer.utils import save_model, load_model\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import json\n",
    "import os\n",
    "\n",
    "# from rich import print\n",
    "from rich.pretty import pprint\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b82294f",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edbbc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hard Times In New York Town',\n",
       "  'Come you ladies and you gentlemen, a-listen to my song',\n",
       "  'Sing it to you right, but you might think itâ€™s wrong',\n",
       "  'Just a little glimpse of a story Iâ€™ll tell',\n",
       "  'â€™Bout an East Coast city that you all know well',\n",
       "  'Itâ€™s hard times in the city',\n",
       "  'Livinâ€™ down in New York town',\n",
       "  'Old New York City is a friendly old town',\n",
       "  'From Washington Heights to Harlem on down',\n",
       "  'Thereâ€™s a-mighty many people all millinâ€™ all around'],\n",
       " 14318)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/bob_dylan_lyrics.csv\")\n",
    "lines = []\n",
    "nb_rows = 999999\n",
    "row_id = 0\n",
    "for r in df.iterrows():\n",
    "    # todo: one line is one sentence.\n",
    "    lines.append(r[1][\"title\"])\n",
    "    # sentences.append(r[1][\"title\"] + \"\\n\" + r[1][\"lyrics\"])\n",
    "    lyrics = r[1][\"lyrics\"].split(\"\\n\")\n",
    "    for line in lyrics:\n",
    "        if len(line.strip()) > 0:\n",
    "            lines.append(line.strip())\n",
    "        row_id += 1\n",
    "\n",
    "lines[:10], len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bab58",
   "metadata": {},
   "source": [
    "### Simple Custom Tokenizer for Bob Dylan Lyrics\n",
    "\n",
    "Create a simple BPE (Byte-Pair Encoding) tokenizer trained specifically on Dylan's lyrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92266e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDylanTokenizer:\n",
    "    def __init__(self, vocab_size=3000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def train_tokenizer(self, corpus: list[str], save_path: str = \"./simple_dylan_tokenizer\"):\n",
    "        # Initialize simple BPE tokenizer\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "        # Simple whitespace pre-tokenization\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "        # Simple trainer\n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=self.vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[MASK]\"], min_frequency=2, show_progress=True\n",
    "        )\n",
    "\n",
    "        # Train the tokenizer\n",
    "        tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "        # Save tokenizer\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        tokenizer.save(f\"{save_path}/tokenizer.json\")\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        print(f\"Tokenizer trained and saved to {save_path}\")\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def load_tokenizer(self, save_path=\"./simple_dylan_tokenizer\"):\n",
    "        \"\"\"Load the trained tokenizer\"\"\"\n",
    "        tokenizer_path = f\"{save_path}/tokenizer.json\"\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "            return self.tokenizer\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Tokenizer not found at {tokenizer_path}\")\n",
    "\n",
    "    def get_transformers_tokenizer(self):\n",
    "        \"\"\"Convert to HuggingFace tokenizer for compatibility\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained or loaded\")\n",
    "\n",
    "        # Create fast tokenizer wrapper\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_object=self.tokenizer, pad_token=\"[PAD]\", unk_token=\"[UNK]\", mask_token=\"[MASK]\"\n",
    "        )\n",
    "\n",
    "        return fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76920799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(tokenizer): 3000\n",
      "ic| tokenizer.special_tokens_map: {'mask_token': '[MASK]', 'pad_token': '[PAD]', 'unk_token': | len(tokenizer): 3000\n",
      "ic| tokenizer.special_tokens_map: {'mask_token': '[MASK]', 'pad_token': '[PAD]', 'unk_token': '[UNK]'}\n",
      "'[UNK]'}\n",
      "ic| phrase: \"The answer my friend is blowin' in the wind\"\n",
      "ic| decoded: \"The answer my friend is blowin ' in the wind\"\n",
      "ic| phrase: \"The answer my friend is blowin' in the wind\"\n",
      "ic| decoded: \"The answer my friend is blowin ' in the wind\"\n",
      "ic| token_strs: ['ic| token_strs: ['The',The',"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained and saved to ./simple_dylan_tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 'answer', 'my', 'friend', 'is', 'blowin', \"'\", 'in', 'the', 'wind']\n",
      "'answer', 'my', 'friend', 'is', 'blowin', \"'\", 'in', 'the', 'wind']\n"
     ]
    }
   ],
   "source": [
    "# Initialize simple Dylan tokenizer\n",
    "dylan_tokenizer = SimpleDylanTokenizer(vocab_size=3000)\n",
    "\n",
    "# Train the tokenizer on Dylan lyrics\n",
    "dylan_tokenizer.train_tokenizer(corpus=lines, save_path=\"./simple_dylan_tokenizer\")\n",
    "\n",
    "# Convert to HuggingFace format for compatibility\n",
    "tokenizer = dylan_tokenizer.get_transformers_tokenizer()\n",
    "\n",
    "ic(len(tokenizer))\n",
    "ic(tokenizer.special_tokens_map)\n",
    "\n",
    "\n",
    "phrase = \"The answer my friend is blowin' in the wind\"\n",
    "\n",
    "tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "decoded = tokenizer.decode(tokens, skip_special_tokens=False)\n",
    "token_strs = tokenizer.convert_ids_to_tokens(tokens)\n",
    "ic(phrase)\n",
    "ic(decoded)\n",
    "ic(token_strs);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a50552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(dataset): 14318\n",
      "ic| seq_len: 20\n",
      "ic| len(| len(dataset): 14318\n",
      "ic| seq_len: 20\n",
      "ic| len(tokenizer): 3000\n",
      "ic|tokenizer): 3000\n",
      "ic| sample_batch.shape: torch.Size([8, 20])\n",
      "ic| tokenizer.decode(sample_batch sample_batch.shape: torch.Size([8, 20])\n",
      "ic| tokenizer.decode(sample_batch[0].tolist(), skip_special_tokens=False): ('Like Lou ie and Jimmy and B ud dy and all of the rest [PAD] [PAD] [PAD] '\n",
      "                                                                            '[PAD] [PAD] [PAD]')\n",
      "[0].tolist(), skip_special_tokens=False): ('Like Lou ie and Jimmy and B ud dy and all of the rest [PAD] [PAD] [PAD] '\n",
      "                                                                            '[PAD] [PAD] [PAD]')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sequence length in dataset: 41\n"
     ]
    }
   ],
   "source": [
    "class SimpleDylanDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.examples = []\n",
    "        max_seq_len = 0\n",
    "\n",
    "        for line in texts:\n",
    "            # Simple tokenization - no structure tokens\n",
    "            tokens = tokenizer.encode(line.strip(), add_special_tokens=False)\n",
    "            token_nb = len(tokens)\n",
    "            max_seq_len = max(max_seq_len, token_nb)\n",
    "            # Truncate if too long\n",
    "\n",
    "            if token_nb > seq_len:\n",
    "                tokens = tokens[:seq_len]\n",
    "\n",
    "            if token_nb > 0:  # Skip empty sequences\n",
    "                self.examples.append(tokens)\n",
    "        print(f\"Max sequence length in dataset: {max_seq_len}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.examples[idx]\n",
    "        pad_id = self.tokenizer.pad_token_id if hasattr(self.tokenizer, \"pad_token_id\") else 0\n",
    "\n",
    "        # Pad to sequence length\n",
    "        padded = tokens + [pad_id] * (self.seq_len - len(tokens))\n",
    "        return torch.tensor(padded[: self.seq_len], dtype=torch.long)\n",
    "\n",
    "\n",
    "seq_len = 20  # Keep shorter sequences for memory efficiency\n",
    "batch_size = 8\n",
    "\n",
    "# Create dataset with selected tokenizer\n",
    "dataset = SimpleDylanDataset(lines, tokenizer, seq_len=seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "ic(len(dataset))\n",
    "ic(seq_len)\n",
    "ic(len(tokenizer))\n",
    "\n",
    "# Test the dataset\n",
    "sample_batch = next(iter(dataloader))\n",
    "ic(sample_batch.shape)\n",
    "ic(tokenizer.decode(sample_batch[0].tolist(), skip_special_tokens=False));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af94d38b",
   "metadata": {},
   "source": [
    "## Diffusion model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54f5a9a5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Timeline:  xâ‚€ â”€â”€â”€â”€â†’ xâ‚ â”€â”€â”€â”€â†’ xâ‚‚ â”€â”€â”€â”€â†’ xâ‚ƒ\n",
    "          clean   noisy   noisier  noisiest\n",
    "           â†‘        â†‘        â†‘        â†‘\n",
    "         \"hello\"  \"h[M]lo\" \"[M][M]o\" \"[M][M][M]\"\n",
    "\n",
    "Forward process q(x_t | x_{t-1}):\n",
    "- q(xâ‚|xâ‚€): \"hello\" â†’ \"h[M]lo\" (add some noise)\n",
    "- q(xâ‚‚|xâ‚): \"h[M]lo\" â†’ \"[M][M]o\" (add more noise)\n",
    "\n",
    "Posterior q(x_{t-1} | x_t, xâ‚€):\n",
    "- q(xâ‚|xâ‚‚, xâ‚€): Given \"[M][M]o\" and knowing original was \"hello\", \n",
    "                 what was xâ‚? Answer: probably \"h[M]lo\"\n",
    "- q(xâ‚€|xâ‚, xâ‚€): Given \"h[M]lo\" and knowing original was \"hello\",\n",
    "                 what was xâ‚€? Answer: definitely \"hello\"\n",
    "\n",
    "\n",
    "# The KL loss compares:\n",
    "KL[q(x_{t-1}|x_t,x_0) || p_Î¸(x_{t-1}|x_t)]\n",
    "   â†‘                    â†‘\n",
    "   True denoising       Model's denoising\n",
    "   (uses ground truth) (learned)                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "beb72e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniformScheduler(nn.Module):\n",
    "    \"\"\"Simple uniform transition scheduler with linear noise schedule.\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int, num_timesteps: int, beta_start: float = 0.0001, beta_end: float = 0.02):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        # Create schedule and transition matrices\n",
    "        betas = self._create_linear_schedule()\n",
    "        # so they are put to device automatically\n",
    "        self.register_buffer(\"betas\", betas)\n",
    "        Q_t = self._create_transition_matrices()\n",
    "        self.register_buffer(\"Q_t\", Q_t)\n",
    "        Q_bar_t = self._create_cumulative_matrices()\n",
    "        self.register_buffer(\"Q_bar_t\", Q_bar_t)\n",
    "\n",
    "    def _create_linear_schedule(self) -> torch.Tensor:\n",
    "        \"\"\"Create linear beta schedule: Î²_t increases linearly from beta_start to beta_end.\"\"\"\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.num_timesteps)\n",
    "\n",
    "    def _create_transition_matrices(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Create uniform transition matrices: Q_t = (1-Î²_t)I + Î²_t/K * 11^T\n",
    "\n",
    "        This means:\n",
    "        - Stay in same state with probability (1-Î²_t)\n",
    "        - Transition to any state (including same) with probability Î²_t/K each\n",
    "        \"\"\"\n",
    "        Q_matrices = torch.zeros(self.num_timesteps, self.num_classes, self.num_classes)\n",
    "\n",
    "        for t in range(self.num_timesteps):\n",
    "            beta_t = self.betas[t].item()\n",
    "\n",
    "            # Diagonal: probability of staying in same state\n",
    "            Q_t = (1 - beta_t) * torch.eye(self.num_classes)\n",
    "\n",
    "            # Off-diagonal: uniform probability of transitioning to any state\n",
    "            Q_t += beta_t / self.num_classes * torch.ones(self.num_classes, self.num_classes)\n",
    "\n",
    "            Q_matrices[t] = Q_t\n",
    "\n",
    "        return Q_matrices\n",
    "\n",
    "    def _create_cumulative_matrices(self) -> torch.Tensor:\n",
    "        \"\"\"Create cumulative matrices: QÌ„_t = Q_1 * Q_2 * ... * Q_t\"\"\"\n",
    "        Q_bar_matrices = torch.zeros(self.num_timesteps, self.num_classes, self.num_classes)\n",
    "        Q_bar_matrices[0] = self.Q_t[0]\n",
    "\n",
    "        for t in range(1, self.num_timesteps):\n",
    "            Q_bar_matrices[t] = torch.matmul(Q_bar_matrices[t - 1], self.Q_t[t])\n",
    "\n",
    "        return Q_bar_matrices\n",
    "\n",
    "    def add_noise(self, x_0: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply forward diffusion: sample from q(x_t | x_0).\n",
    "\n",
    "        For uniform transitions, this uses the cumulative matrix QÌ„_t.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x_0.shape\n",
    "        device = x_0.device\n",
    "\n",
    "        # Clamp x_0 to valid token range to prevent out-of-bounds errors\n",
    "        x_0_clamped = torch.clamp(x_0, 0, self.num_classes - 1)\n",
    "\n",
    "        # Convert to one-hot encoding\n",
    "        x_0_onehot = F.one_hot(x_0_clamped, self.num_classes).to(device).float()  # [B, L, K]\n",
    "\n",
    "        x_t = torch.zeros_like(x_0).to(device)  # [B, L]\n",
    "        self.Q_bar_t = self.Q_bar_t.to(device)  # Ensure matrices are on the correct device\n",
    "        for i in range(batch_size):\n",
    "            # Get cumulative transition matrix for this timestep\n",
    "            t_val = t[i].item()\n",
    "\n",
    "            # Add bounds checking to prevent IndexError\n",
    "            t_val = max(0, min(t_val, self.num_timesteps - 1))\n",
    "\n",
    "            Q_bar = self.Q_bar_t[t_val].to(device)  # [K, K]\n",
    "\n",
    "            # Compute transition probabilities: x_0 @ QÌ„_t\n",
    "            # This gives probability distribution over x_t for each position\n",
    "            probs = torch.matmul(x_0_onehot[i], Q_bar)  # [L, K]\n",
    "\n",
    "            # Add numerical stability: ensure probabilities are non-negative and sum to 1\n",
    "            probs = torch.clamp(probs, min=1e-8)  # Ensure non-negative\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)  # Normalize\n",
    "\n",
    "            # Sample from categorical distribution\n",
    "            flat_probs = probs.view(-1, self.num_classes)  # [L, K]\n",
    "\n",
    "            # Additional safety check for multinomial\n",
    "            prob_sums = flat_probs.sum(dim=-1)\n",
    "            if (prob_sums <= 0).any():\n",
    "                print(f\"Warning: Invalid probability distribution detected\")\n",
    "                print(f\"prob_sums: {prob_sums}\")\n",
    "                print(f\"flat_probs sample: {flat_probs[0]}\")\n",
    "                # Fallback to uniform distribution\n",
    "                flat_probs = torch.ones_like(flat_probs) / self.num_classes\n",
    "\n",
    "            flat_samples = torch.multinomial(flat_probs, 1).squeeze(-1)  # [L]\n",
    "            x_t[i] = flat_samples\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def get_posterior_params(self, x_t: torch.Tensor, x_0: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute true posterior q(x_{t-1} | x_t, x_0) using Equation 3.\n",
    "\n",
    "        From the paper: q(x_{t-1}|x_t,x_0) = Cat(x_{t-1}; p = x_t Q_t^T âŠ™ x_0 QÌ„_{t-1} / (x_0 QÌ„_t x_t^T))\n",
    "\n",
    "        This tells us: given we observe x_t at time t and know the original was x_0,\n",
    "        what's the probability distribution over what x_{t-1} could have been?\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x_t.shape\n",
    "        device = x_t.device\n",
    "\n",
    "        # Clamp token indices to valid range\n",
    "        x_t_clamped = torch.clamp(x_t, 0, self.num_classes - 1)\n",
    "        x_0_clamped = torch.clamp(x_0, 0, self.num_classes - 1)\n",
    "\n",
    "        posteriors = torch.zeros(batch_size, seq_len, self.num_classes, device=device)\n",
    "\n",
    "        for i, t_val in enumerate(t):\n",
    "            # Add bounds checking here too\n",
    "            t_val = max(0, min(t_val.item(), self.num_timesteps - 1))\n",
    "\n",
    "            if t_val == 0:\n",
    "                # Special case: t=0 means we're asking for q(x_{-1}|x_0, x_0)\n",
    "                # This doesn't make physical sense, so return uniform or x_0\n",
    "                # In practice, this case shouldn't occur in training\n",
    "                posteriors[i] = F.one_hot(x_0_clamped[i], self.num_classes).float()\n",
    "                continue\n",
    "\n",
    "            # Get transition matrices\n",
    "            Q_t = self.Q_t[t_val].to(device)  # [K, K] - single step t\n",
    "            Q_bar_t_minus_1 = self.Q_bar_t[t_val - 1].to(device)  # [K, K] - cumulative to t-1\n",
    "            Q_bar_t = self.Q_bar_t[t_val].to(device)  # [K, K] - cumulative to t\n",
    "\n",
    "            # For each position in sequence\n",
    "            for pos in range(seq_len):\n",
    "                x_0_idx = x_0_clamped[i, pos].item()  # Original token index (clamped)\n",
    "                x_t_idx = x_t_clamped[i, pos].item()  # Current token index (clamped)\n",
    "\n",
    "                # Compute posterior using Bayes rule:\n",
    "                # q(x_{t-1}|x_t,x_0) âˆ q(x_t|x_{t-1},x_0) * q(x_{t-1}|x_0)\n",
    "                #                    = q(x_t|x_{t-1}) * q(x_{t-1}|x_0)  [Markov property]\n",
    "\n",
    "                posterior = torch.zeros(self.num_classes, device=device)\n",
    "\n",
    "                # For each possible value of x_{t-1}\n",
    "                for x_prev_idx in range(self.num_classes):\n",
    "                    # q(x_{t-1}|x_0): probability that x_{t-1} = x_prev_idx given x_0\n",
    "                    q_prev_given_x0 = Q_bar_t_minus_1[x_0_idx, x_prev_idx]\n",
    "\n",
    "                    # q(x_t|x_{t-1}): probability that x_t = x_t_idx given x_{t-1} = x_prev_idx\n",
    "                    q_curr_given_prev = Q_t[x_prev_idx, x_t_idx]\n",
    "\n",
    "                    # Joint probability\n",
    "                    posterior[x_prev_idx] = q_curr_given_prev * q_prev_given_x0\n",
    "\n",
    "                # Normalize to get proper probability distribution\n",
    "                posterior_sum = posterior.sum()\n",
    "                if posterior_sum > 1e-8:\n",
    "                    posterior = posterior / posterior_sum\n",
    "                else:\n",
    "                    # Fallback to uniform if numerical issues\n",
    "                    posterior = torch.ones(self.num_classes, device=device) / self.num_classes\n",
    "\n",
    "                posteriors[i, pos] = posterior\n",
    "\n",
    "        return posteriors\n",
    "\n",
    "    def compute_kl_divergence(self, true_posterior: torch.Tensor, pred_posterior: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute KL[q(x_{t-1}|x_t,x_0) || p_Î¸(x_{t-1}|x_t)]\"\"\"\n",
    "        kl = torch.sum(true_posterior * (torch.log(true_posterior + 1e-8) - torch.log(pred_posterior + 1e-8)), dim=-1)\n",
    "        return kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d24517b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t=1: x_0: [42, 15, 73], x_t: [42, 15, 73], true x_{t-1}: [42, 15, 73],\n",
      "equals x_0: True\n",
      "t=10: x_0: [42, 15, 73], x_t: [42, 15, 73], true x_{t-1}: [42, 15, 73],\n",
      "equals x_0: True\n",
      "t=50: x_0: [42, 15, 73], x_t: [42, 15, 73], true x_{t-1}: [42, 15, 73],\n",
      "equals x_0: True\n",
      "t=100: x_0: [42, 15, 73], x_t: [42, 15, 73], true x_{t-1}: [42, 15, 73],\n",
      "equals x_0: True\n",
      "t=500: x_0: [42, 15, 73], x_t: [83, 86, 63], true x_{t-1}: [83, 86, 63],\n",
      "equals x_0: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scheduler = UniformScheduler(num_classes=100, num_timesteps=1000).to(device)\n",
    "\n",
    "x_0 = torch.tensor([[42, 15, 73]])  # Original tokens\n",
    "\n",
    "# Check at different timesteps\n",
    "for t_val in [1, 10, 50, 100, 500]:\n",
    "    t = torch.tensor([t_val])\n",
    "    x_t = scheduler.add_noise(x_0, t)\n",
    "\n",
    "    true_posterior = scheduler.get_posterior_params(x_t, x_0, t)\n",
    "    true_prev = true_posterior.argmax(dim=-1)\n",
    "\n",
    "    print(\n",
    "        f\"t={t_val}: x_0: {x_0[0].tolist()}, x_t: {x_t[0].tolist()}, true x_{{t-1}}: {true_prev[0].tolist()},\\nequals x_0: {true_prev[0].equal(x_0[0])}\"\n",
    "    )\n",
    "scheduler.Q_t.device, scheduler.Q_bar_t.device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac5fd249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing posterior evolution...\n",
      "Evolution of posterior q(x_{t-1} | x_t, x_0) as t increases:\n",
      "(Shows probability of each token being x_{t-1})\n",
      "\n",
      "t=1, x_0=1, x_t=1,  P(x_{t-1} = k): ['0.001', '0.996', '0.001', '0.001', '0.001'],Most likely x_{t-1}: 1\n",
      "t=2, x_0=1, x_t=0,  P(x_{t-1} = k): ['0.444', '0.461', '0.032', '0.032', '0.032'],Most likely x_{t-1}: 1\n",
      "t=3, x_0=1, x_t=0,  P(x_{t-1} = k): ['0.502', '0.342', '0.052', '0.052', '0.052'],Most likely x_{t-1}: 0\n",
      "t=4, x_0=1, x_t=4,  P(x_{t-1} = k): ['0.074', '0.260', '0.074', '0.074', '0.517'],Most likely x_{t-1}: 4\n",
      "t=5, x_0=1, x_t=0,  P(x_{t-1} = k): ['0.504', '0.205', '0.097', '0.097', '0.097'],Most likely x_{t-1}: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Test that posterior evolves correctly as we move through timesteps.\"\"\"\n",
    "print(\"Testing posterior evolution...\")\n",
    "\n",
    "vocab_size = 5  # Very small for clear visualization\n",
    "scheduler = UniformScheduler(num_classes=vocab_size, num_timesteps=10, beta_start=0.1, beta_end=0.9).to(device)\n",
    "\n",
    "x_0 = torch.tensor([[1]])  # Single token, original = 1\n",
    "\n",
    "print(\"Evolution of posterior q(x_{t-1} | x_t, x_0) as t increases:\")\n",
    "print(\"(Shows probability of each token being x_{t-1})\")\n",
    "print()\n",
    "\n",
    "for t_val in range(1, 6):\n",
    "    t = torch.tensor([t_val])\n",
    "    x_t = scheduler.add_noise(x_0, t)\n",
    "    posterior = scheduler.get_posterior_params(x_t, x_0, t)\n",
    "    true_prev = posterior.argmax(dim=-1)\n",
    "\n",
    "    print(\n",
    "        f\"t={t_val}, x_0=1, x_t={x_t[0, 0].item()},  P(x_{{t-1}} = k): {[f'{p:.3f}' for p in posterior[0, 0].tolist()]},Most likely x_{{t-1}}: {posterior[0, 0].argmax().item()}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f727c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| inp.shape: torch.Size([8, 20])\n",
      "| inp.shape: torch.Size([8, 20])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As they were taking down the t ent s [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "As they were taking down the t ent grace [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "As they were taking down the t ent s [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] avey [PAD] spo forget\n",
      "As they were taking down the t ent s [PAD] [PAD] [PAD] [PAD] Heart pre [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Generate some noisy sentences\n",
    "inp = next(iter(dataloader)).to(device)\n",
    "ic(inp.shape)\n",
    "\n",
    "ds_scheduler = UniformScheduler(num_classes=len(tokenizer), num_timesteps=30)\n",
    "\n",
    "\n",
    "def demo_noise(inp, line_nb, step):\n",
    "    src_line = tokenizer.decode(inp[line_nb].cpu().numpy())\n",
    "    noisy_inp = ds_scheduler.add_noise(inp[line_nb : line_nb + 1], torch.tensor([step]).to(device))\n",
    "    noisy_line = tokenizer.decode(noisy_inp[0].cpu().numpy())\n",
    "    return src_line, noisy_line\n",
    "\n",
    "\n",
    "ic.disable()\n",
    "ic.enable()\n",
    "sent_nb = 4\n",
    "print(demo_noise(inp, sent_nb, 0)[1])\n",
    "print(demo_noise(inp, sent_nb, 12)[1])\n",
    "print(demo_noise(inp, sent_nb, 20)[1])\n",
    "print(demo_noise(inp, sent_nb, 29)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4de9b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleD3PMModel(nn.Module):\n",
    "    \"\"\"Simple transformer model for D3PM.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        max_seq_len: int,\n",
    "        d_model: int = 256,\n",
    "        num_heads: int = 8,\n",
    "        num_layers: int = 4,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        # Time embedding for diffusion timestep\n",
    "        self.time_embedding = nn.Sequential(nn.Linear(d_model, d_model), nn.GELU(), nn.Linear(d_model, d_model))\n",
    "\n",
    "        # Transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4 * d_model,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "\n",
    "        # Output head to predict xâ‚€\n",
    "        self.output_head = nn.Sequential(nn.LayerNorm(d_model), nn.Linear(d_model, vocab_size))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _get_time_embedding(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Create sinusoidal time embeddings like in original Transformer.\"\"\"\n",
    "        half_dim = self.d_model // 2\n",
    "\n",
    "        # Create the frequency ratios\n",
    "        freqs = torch.exp(\n",
    "            -math.log(10000.0) * torch.arange(0, half_dim, dtype=torch.float32, device=t.device) / half_dim\n",
    "        )\n",
    "\n",
    "        # Expand dims to enable broadcasting: t is [batch_size], freqs is [half_dim]\n",
    "        time_freqs = t.float()[:, None] * freqs[None, :]  # [batch_size, half_dim]\n",
    "\n",
    "        # Create sin and cos components\n",
    "        emb = torch.cat([torch.sin(time_freqs), torch.cos(time_freqs)], dim=1)  # [batch_size, d_model]\n",
    "\n",
    "        if self.d_model % 2 == 1:  # Handle odd d_model\n",
    "            emb = torch.cat([emb, torch.zeros_like(emb[:, :1])], dim=1)\n",
    "\n",
    "        return self.time_embedding(emb)\n",
    "\n",
    "    def forward(self, x_t: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: predict xâ‚€ from x_t.\n",
    "\n",
    "        Args:\n",
    "            x_t: Noisy tokens, shape [batch_size, seq_len]\n",
    "            t: Timesteps, shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "            x0_logits: Predicted xâ‚€ logits, shape [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x_t.shape\n",
    "        device = x_t.device\n",
    "\n",
    "        # Token embeddings\n",
    "        token_emb = self.token_embedding(x_t)  # [B, L, D]\n",
    "\n",
    "        # Position embeddings\n",
    "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)\n",
    "        pos_emb = self.position_embedding(positions)  # [B, L, D]\n",
    "\n",
    "        # Time embeddings\n",
    "        time_emb = self._get_time_embedding(t)  # [B, D]\n",
    "        time_emb = time_emb.unsqueeze(1).expand(-1, seq_len, -1)  # [B, L, D]\n",
    "\n",
    "        # Combine all embeddings\n",
    "        x = self.dropout(token_emb + pos_emb + time_emb)  # [B, L, D]\n",
    "\n",
    "        # Transformer processing\n",
    "        x = self.transformer(x)  # [B, L, D]\n",
    "\n",
    "        # Predict xâ‚€ logits\n",
    "        x0_logits = self.output_head(x)  # [B, L, vocab_size]\n",
    "\n",
    "        return x0_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0a19a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_t.shape: torch.Size([8, 20])\n",
      "    x0_logits.shape: torch.Size([8, 20, 1000])\n",
      "    t.shape: torch.Size([8])\n",
      "| x_t.shape: torch.Size([8, 20])\n",
      "    x0_logits.shape: torch.Size([8, 20, 1000])\n",
      "    t.shape: torch.Size([8])\n",
      "ic| scheduler.Q_t.shape: torch.Size([50, 1000, 1000])\n",
      "    scheduler.Q_bar_t.shape: torch.Size([50, 1000, 1000])\n",
      "ic| scheduler.Q_t.shape: torch.Size([50, 1000, 1000])\n",
      "    scheduler.Q_bar_t.shape: torch.Size([50, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "def compute_predicted_posterior(\n",
    "    scheduler: UniformScheduler, x0_logits: torch.Tensor, x_t: torch.Tensor, t: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute p_Î¸(x_{t-1}|x_t) using xâ‚€-parameterization.\n",
    "\n",
    "    From Equation 4: p_Î¸(x_{t-1}|x_t) âˆ Î£ q(x_{t-1},x_t|xÌƒâ‚€) pÌƒ_Î¸(xÌƒâ‚€|x_t)\n",
    "\n",
    "    The true posterior is: q(x_{t-1}|x_t,xâ‚€) âˆ q(x_t|x_{t-1}) q(x_{t-1}|xâ‚€)\n",
    "\n",
    "    This is a fully vectorized implementation with no loops.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = x_t.shape\n",
    "    device = x_t.device\n",
    "    num_classes = scheduler.num_classes\n",
    "\n",
    "    # Convert model logits to probabilities\n",
    "    p_x0_given_xt = F.softmax(x0_logits, dim=-1)  # [B, L, K]\n",
    "\n",
    "    # Initialize result tensor\n",
    "    pred_posteriors = torch.zeros(batch_size, seq_len, num_classes, device=device)\n",
    "\n",
    "    # Handle t=0 case separately (early return for efficiency)\n",
    "    t_zero_mask = t == 0\n",
    "    if t_zero_mask.any():\n",
    "        # For t=0 cases, return softmax of x0_logits\n",
    "        pred_posteriors[t_zero_mask] = p_x0_given_xt[t_zero_mask]\n",
    "\n",
    "        # If all timesteps are 0, return early\n",
    "        if t_zero_mask.all():\n",
    "            return pred_posteriors\n",
    "\n",
    "    # Process non-zero timesteps\n",
    "    non_zero_mask = ~t_zero_mask\n",
    "    if not non_zero_mask.any():\n",
    "        return pred_posteriors\n",
    "\n",
    "    # Get indices for non-zero timesteps\n",
    "    t_non_zero = t[non_zero_mask]  # [N] where N = number of non-zero timesteps\n",
    "\n",
    "    # Get transition matrices for all non-zero timesteps\n",
    "    # Stack Q_t and Q_{t-1} matrices for vectorized operations\n",
    "    Q_t_stack = torch.stack([scheduler.Q_t[t_val] for t_val in t_non_zero])  # [N, K, K]\n",
    "    Q_t_minus_1_stack = torch.stack([scheduler.Q_t[t_val - 1] for t_val in t_non_zero])  # [N, K, K]\n",
    "\n",
    "    # Get relevant data for non-zero timesteps\n",
    "    x_t_non_zero = x_t[non_zero_mask]  # [N, L]\n",
    "    p_x0_non_zero = p_x0_given_xt[non_zero_mask]  # [N, L, K]\n",
    "\n",
    "    # Vectorized computation of posterior for all positions and timesteps\n",
    "    # We need to compute: Î£_{x0} p(x0|xt) * q(x_{t-1}|xt,x0)\n",
    "    # where q(x_{t-1}|xt,x0) âˆ q(xt|x_{t-1}) * q(x_{t-1}|x0)\n",
    "\n",
    "    # Create index tensors for gathering\n",
    "    batch_indices = torch.arange(len(t_non_zero), device=device)[:, None]  # [N, 1]\n",
    "    x_t_indices = x_t_non_zero  # [N, L] - current token indices\n",
    "\n",
    "    # For each possible x_{t-1} value, compute the posterior probability\n",
    "    pred_posterior_non_zero = torch.zeros(len(t_non_zero), seq_len, num_classes, device=device)\n",
    "\n",
    "    for x_prev in range(num_classes):\n",
    "        # For this x_{t-1} value, compute contribution from all possible x0 values\n",
    "\n",
    "        # q(x_t|x_{t-1}): transition probability from x_prev to current tokens\n",
    "        # Shape operations: Q_t_stack[batch_indices, x_prev, x_t_indices]\n",
    "        q_xt_given_xprev = Q_t_stack[batch_indices, x_prev, x_t_indices]  # [N, L]\n",
    "\n",
    "        # Avoid division by zero\n",
    "        Q_t_minus_1_vals = Q_t_minus_1_stack[batch_indices, x_prev, x_t_indices]  # [N, L]\n",
    "        q_xt_given_xprev = q_xt_given_xprev / (Q_t_minus_1_vals + 1e-8)\n",
    "\n",
    "        # q(x_{t-1}|x0): probability of being at x_prev given each possible x0\n",
    "        # Q_t_minus_1_stack[:, x0_indices, x_prev] for all x0_indices\n",
    "        q_xprev_given_x0 = Q_t_minus_1_stack[:, :, x_prev]  # [N, K]\n",
    "\n",
    "        # Expand dimensions for broadcasting\n",
    "        q_xt_given_xprev_expanded = q_xt_given_xprev[:, :, None]  # [N, L, 1]\n",
    "        q_xprev_given_x0_expanded = q_xprev_given_x0[:, None, :]  # [N, 1, K]\n",
    "\n",
    "        # Compute unnormalized posterior contribution for this x_{t-1}\n",
    "        # Shape: [N, L, K] = [N, L, 1] * [N, 1, K] * [N, L, K]\n",
    "        unnorm_contrib = q_xt_given_xprev_expanded * q_xprev_given_x0_expanded * p_x0_non_zero\n",
    "\n",
    "        # Sum over all possible x0 values (marginalization)\n",
    "        pred_posterior_non_zero[:, :, x_prev] = unnorm_contrib.sum(dim=-1)  # [N, L]\n",
    "\n",
    "    # Normalize the posterior distributions\n",
    "    pred_posterior_non_zero = pred_posterior_non_zero / (pred_posterior_non_zero.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "    # Place results back into the full tensor\n",
    "    pred_posteriors[non_zero_mask] = pred_posterior_non_zero\n",
    "\n",
    "    return pred_posteriors\n",
    "\n",
    "\n",
    "batch_size = 8\n",
    "vocab_size = 1000\n",
    "seq_len = 20\n",
    "num_timesteps = 50  # Total number of timesteps\n",
    "t = torch.randint(0, num_timesteps, (batch_size,), device=device)\n",
    "x_t = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)  # Simulated noisy input\n",
    "x0_logits = torch.randn(batch_size, seq_len, vocab_size, device=device)  # Simulated model output\n",
    "ic(x_t.shape, x0_logits.shape, t.shape)\n",
    "\n",
    "scheduler = UniformScheduler(num_classes=vocab_size, num_timesteps=num_timesteps).to(device)\n",
    "ic(scheduler.Q_t.shape, scheduler.Q_bar_t.shape)\n",
    "pred_posterior = compute_predicted_posterior(scheduler=scheduler, x0_logits=x0_logits, x_t=x_t, t=t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43db492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¨ Visual demonstration of compute_predicted_posterior...\n",
      "\n",
      "ðŸ” Timestep t = 0:\n",
      "   Input x_t: [[0, 1, 2], [2, 0, 1]],  Predicted x0 (argmax): [[0, 1, 2], [3, 0, 1]]\n",
      "   Posterior p(x_{t-1}|x_t) for each position:\n",
      "     Batch 0, Pos 0: most likely x_{t-1} = 0 (prob=0.980)full distribution: ['0.980', '0.007', '0.007', '0.007']\n",
      "     Batch 0, Pos 1: most likely x_{t-1} = 1 (prob=0.980)full distribution: ['0.007', '0.980', '0.007', '0.007']\n",
      "     Batch 0, Pos 2: most likely x_{t-1} = 2 (prob=0.980)full distribution: ['0.007', '0.007', '0.980', '0.007']\n",
      "     Batch 1, Pos 0: most likely x_{t-1} = 3 (prob=0.980)full distribution: ['0.007', '0.007', '0.007', '0.980']\n",
      "     Batch 1, Pos 1: most likely x_{t-1} = 0 (prob=0.980)full distribution: ['0.980', '0.007', '0.007', '0.007']\n",
      "     Batch 1, Pos 2: most likely x_{t-1} = 1 (prob=0.980)full distribution: ['0.007', '0.980', '0.007', '0.007']\n",
      "   Entropy (uncertainty): [[0.11907892674207687, 0.11907892674207687, 0.11907892674207687], [0.11907892674207687, 0.11907892674207687, 0.11907892674207687]]\n",
      "\n",
      "ðŸ” Timestep t = 1:\n",
      "   Input x_t: [[0, 1, 2], [2, 0, 1]],  Predicted x0 (argmax): [[0, 1, 2], [3, 0, 1]]\n",
      "   Posterior p(x_{t-1}|x_t) for each position:\n",
      "     Batch 0, Pos 0: most likely x_{t-1} = 0 (prob=0.492)full distribution: ['0.492', '0.169', '0.169', '0.169']\n",
      "     Batch 0, Pos 1: most likely x_{t-1} = 1 (prob=0.492)full distribution: ['0.169', '0.492', '0.169', '0.169']\n",
      "     Batch 0, Pos 2: most likely x_{t-1} = 2 (prob=0.492)full distribution: ['0.169', '0.169', '0.492', '0.169']\n",
      "     Batch 1, Pos 0: most likely x_{t-1} = 3 (prob=0.987)full distribution: ['0.007', '0.007', '0.000', '0.987']\n",
      "     Batch 1, Pos 1: most likely x_{t-1} = 0 (prob=0.492)full distribution: ['0.492', '0.169', '0.169', '0.169']\n",
      "     Batch 1, Pos 2: most likely x_{t-1} = 1 (prob=0.492)full distribution: ['0.169', '0.492', '0.169', '0.169']\n",
      "   Entropy (uncertainty): [[1.2512774467468262, 1.2512774467468262, 1.2512774467468262], [0.08140797913074493, 1.2512774467468262, 1.2512774467468262]]\n",
      "\n",
      "ðŸ” Timestep t = 3:\n",
      "   Input x_t: [[0, 1, 2], [2, 0, 1]],  Predicted x0 (argmax): [[0, 1, 2], [3, 0, 1]]\n",
      "   Posterior p(x_{t-1}|x_t) for each position:\n",
      "     Batch 0, Pos 0: most likely x_{t-1} = 0 (prob=0.960)full distribution: ['0.960', '0.013', '0.013', '0.013']\n",
      "     Batch 0, Pos 1: most likely x_{t-1} = 1 (prob=0.960)full distribution: ['0.013', '0.960', '0.013', '0.013']\n",
      "     Batch 0, Pos 2: most likely x_{t-1} = 2 (prob=0.960)full distribution: ['0.013', '0.013', '0.960', '0.013']\n",
      "     Batch 1, Pos 0: most likely x_{t-1} = 3 (prob=0.976)full distribution: ['0.009', '0.009', '0.006', '0.976']\n",
      "     Batch 1, Pos 1: most likely x_{t-1} = 0 (prob=0.960)full distribution: ['0.960', '0.013', '0.013', '0.013']\n",
      "     Batch 1, Pos 2: most likely x_{t-1} = 1 (prob=0.960)full distribution: ['0.013', '0.960', '0.013', '0.013']\n",
      "   Entropy (uncertainty): [[0.21273937821388245, 0.21273937821388245, 0.21273937821388245], [0.14017626643180847, 0.21273937821388245, 0.21273937821388245]]\n",
      "\n",
      "ðŸ“Š Summary insights:\n",
      "   â€¢ At t=0: Returns exact softmax of model predictions\n",
      "   â€¢ At t>0: Incorporates diffusion process uncertainty\n",
      "   â€¢ Higher t â†’ more uncertainty (higher entropy)\n",
      "   â€¢ Function respects probability constraints (sum=1, non-negative)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Visual demonstration of compute_predicted_posterior function behavior.\n",
    "\"\"\"\n",
    "print(\"ðŸŽ¨ Visual demonstration of compute_predicted_posterior...\")\n",
    "\n",
    "# Use a very small example for clarity\n",
    "demo_vocab_size = 4\n",
    "demo_num_timesteps = 5\n",
    "demo_scheduler = UniformScheduler(num_classes=demo_vocab_size, num_timesteps=demo_num_timesteps).to(device)\n",
    "batch_size, seq_len = 2, 3\n",
    "\n",
    "# Test different timesteps\n",
    "for t_val in [0, 1, 3]:\n",
    "    print(f\"\\nðŸ” Timestep t = {t_val}:\")\n",
    "\n",
    "    t = torch.tensor([t_val, t_val], device=device)\n",
    "    x_t = torch.tensor([[0, 1, 2], [2, 0, 1]], device=device)\n",
    "\n",
    "    # Create logits that strongly prefer certain tokens\n",
    "    x0_logits = torch.zeros(batch_size, seq_len, demo_vocab_size, device=device)\n",
    "    x0_logits[0, 0, 0] = 5.0  # Strongly predict token 0 for position 0\n",
    "    x0_logits[0, 1, 1] = 5.0  # Strongly predict token 1 for position 1\n",
    "    x0_logits[0, 2, 2] = 5.0  # Strongly predict token 2 for position 2\n",
    "    x0_logits[1, 0, 3] = 5.0  # Strongly predict token 3 for position 0\n",
    "    x0_logits[1, 1, 0] = 5.0  # Strongly predict token 0 for position 1\n",
    "    x0_logits[1, 2, 1] = 5.0  # Strongly predict token 1 for position 2\n",
    "\n",
    "    result = compute_predicted_posterior(demo_scheduler, x0_logits, x_t, t)\n",
    "\n",
    "    print(\n",
    "        f\"   Input x_t: {x_t.tolist()},  Predicted x0 (argmax): {F.softmax(x0_logits, dim=-1).argmax(dim=-1).tolist()}\"\n",
    "    )\n",
    "    print(f\"   Posterior p(x_{{t-1}}|x_t) for each position:\")\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        for pos in range(seq_len):\n",
    "            posterior = result[b, pos]\n",
    "            max_prob_token = posterior.argmax().item()\n",
    "            max_prob = posterior.max().item()\n",
    "            print(\n",
    "                f\"     Batch {b}, Pos {pos}: most likely x_{{t-1}} = {max_prob_token} (prob={max_prob:.3f})\"\n",
    "                f\"full distribution: {[f'{p:.3f}' for p in posterior.tolist()]}\"\n",
    "            )\n",
    "\n",
    "    # Show entropy (measure of uncertainty)\n",
    "    entropy = -(result * torch.log(result + 1e-8)).sum(dim=-1)\n",
    "    print(f\"   Entropy (uncertainty): {entropy.tolist()}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary insights:\")\n",
    "print(f\"   â€¢ At t=0: Returns exact softmax of model predictions\")\n",
    "print(f\"   â€¢ At t>0: Incorporates diffusion process uncertainty\")\n",
    "print(f\"   â€¢ Higher t â†’ more uncertainty (higher entropy)\")\n",
    "print(f\"   â€¢ Function respects probability constraints (sum=1, non-negative)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a5fb805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained and saved to ./simple_1000_dylan_tokenizer\n",
      "Max sequence length in dataset: 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_0.shape: torch.Size([32, 3])\n",
      "| x_0.shape: torch.Size([32, 3])\n",
      "ic| x_t.shape: torch.Size([32, 3])\n",
      "ic| x_t.shape: torch.Size([32, 3])\n",
      "ic| x0_logits.shape: torch.Size([32, 3, 1000])\n",
      "ic| x0_logits.shape: torch.Size([32, 3, 1000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before training step\n",
      "after training step\n",
      "after training step\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Training step on a single batch. \"\"\"\n",
    "\n",
    "\n",
    "def training_step(\n",
    "    scheduler: UniformScheduler, model: SimpleD3PMModel, x_0: torch.Tensor, t: torch.Tensor\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    x_t = scheduler.add_noise(x_0, t)\n",
    "    ic(x_t.shape)\n",
    "    x0_logits = model(x_t, t)\n",
    "    ic(x0_logits.shape)\n",
    "    true_posterior = scheduler.get_posterior_params(x_t, x_0, t)\n",
    "    # ic(true_posterior.shape)\n",
    "\n",
    "    pred_posterior = compute_predicted_posterior(scheduler=scheduler, x0_logits=x0_logits, x_t=x_t, t=t)\n",
    "    # ic(pred_posterior.shape)\n",
    "\n",
    "    kl_terms = scheduler.compute_kl_divergence(true_posterior, pred_posterior)\n",
    "    # ic(kl_terms.shape)\n",
    "    return x0_logits, kl_terms\n",
    "\n",
    "\n",
    "# scheduler = UniformScheduler(num_classes=100, num_timesteps=10, beta_start=0.1, beta_end=0.9)\n",
    "# x_0 = torch.tensor([[42, 15, 73]])\n",
    "# model = SimpleD3PMModel(vocab_size=100, max_seq_len=10, d_model=64, num_heads=1, num_layers=1)\n",
    "# t = torch.tensor([3])\n",
    "\n",
    "# x0_logits, kl_terms = training_step(scheduler, model, x_0, t)\n",
    "\n",
    "\n",
    "num_timesteps = 50\n",
    "max_seq_len = 22\n",
    "batch_size = 32\n",
    "vocab_size = 1000\n",
    "dylan_tokenizer = SimpleDylanTokenizer(vocab_size=1000)\n",
    "dylan_tokenizer.train_tokenizer(corpus=lines, save_path=f\"./simple_{vocab_size}_dylan_tokenizer\")\n",
    "tokenizer = dylan_tokenizer.get_transformers_tokenizer()\n",
    "sm_dataset = SimpleDylanDataset(lines[:100], tokenizer, seq_len=seq_len)\n",
    "sm_dataloader = DataLoader(sm_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "scheduler = UniformScheduler(num_classes=vocab_size, num_timesteps=num_timesteps, beta_start=0.0001, beta_end=0.02).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "model = SimpleD3PMModel(vocab_size=vocab_size, max_seq_len=max_seq_len, d_model=128, num_heads=2, num_layers=1).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "t = torch.randint(0, num_timesteps, (batch_size,), device=device)  # Changed from (batch_size, 1) to (batch_size,)\n",
    "x_0 = next(iter(sm_dataloader))  # Get a batch from the small dataset\n",
    "x_0 = x_0.to(device)\n",
    "ic(x_0.shape)  # Should be [batch_size, seq_len]\n",
    "# Forward pass\n",
    "print(\"before training step\")\n",
    "x0_logits, kl_terms = training_step(scheduler=scheduler, model=model, x_0=x_0, t=t)\n",
    "print(\"after training step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa7e5584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and scheduler created successfully!\n",
      "Model parameters: 3,809,256\n",
      "Vocab size: 1000\n",
      "Num timesteps: 50\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming SimpleD3PMModel and UniformScheduler are defined elsewhere\n",
    "# from your_model_definition import SimpleD3PMModel, UniformScheduler\n",
    "\n",
    "# Simple model and scheduler setup for training\n",
    "seq_len = 20\n",
    "vocab_size = len(tokenizer)\n",
    "num_timesteps = 50\n",
    "\n",
    "# Create model and scheduler\n",
    "model = SimpleD3PMModel(vocab_size=vocab_size, max_seq_len=seq_len).to(device)\n",
    "scheduler = UniformScheduler(num_classes=vocab_size, num_timesteps=num_timesteps, beta_start=0.0001, beta_end=0.02).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "\n",
    "# Simple loss function\n",
    "def d3pm_loss(\n",
    "    x0_logits: torch.Tensor, kl_terms: torch.Tensor, x_0: torch.Tensor, lambda_weight: float = 0.001\n",
    ") -> tuple[torch.Tensor, dict]:\n",
    "    # VB loss: average KL terms\n",
    "    vb_loss = kl_terms.mean()\n",
    "\n",
    "    # Auxiliary loss: cross entropy between predicted x0 and true x0\n",
    "    flat_logits = x0_logits.view(-1, vocab_size)\n",
    "    flat_targets = x_0.view(-1)\n",
    "    aux_loss = F.cross_entropy(flat_logits, flat_targets)\n",
    "\n",
    "    # Total loss\n",
    "    total_loss = vb_loss + lambda_weight * aux_loss\n",
    "\n",
    "    return total_loss, {\"total\": total_loss, \"vb\": vb_loss, \"aux\": aux_loss}\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "print(\"Model and scheduler created successfully!\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Vocab size: {vocab_size}\")\n",
    "print(f\"Num timesteps: {num_timesteps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4c66f1",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "211eecdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_t.shape: torch.Size([8, 20])\n",
      "| x_t.shape: torch.Size([8, 20])\n",
      "ic| x0_logits.shape: torch.Size([8, 20, 1000])\n",
      "ic| x0_logits.shape: torch.Size([8, 20, 1000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_t.shape: torch.Size([8, 20])\n",
      "ic| x0_logits| x_t.shape: torch.Size([8, 20])\n",
      "ic| x0_logits.shape: torch.Size([8, 20, 1000])\n",
      ".shape: torch.Size([8, 20, 1000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 0: Loss = 7.0026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x_t.shape: torch.Size([8, 20])\n",
      "ic| x0_logits| x_t.shape: torch.Size([8, 20])\n",
      "ic| x0_logits.shape.shape: torch.Size([8, 20, 1000])\n",
      ": torch.Size([8, 20, 1000])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 1: Loss = 6.8669\n",
      "  Batch 2: Loss = 6.7276\n",
      "Epoch 0 completed. Average Loss: 6.8657\n",
      "\n",
      "âœ… Training loop working correctly! IndexError has been fixed.\n",
      "  Batch 2: Loss = 6.7276\n",
      "Epoch 0 completed. Average Loss: 6.8657\n",
      "\n",
      "âœ… Training loop working correctly! IndexError has been fixed.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer)\n",
    "num_timesteps = 50\n",
    "max_seq_len = 20\n",
    "batch_size = 8\n",
    "model = SimpleD3PMModel(vocab_size=vocab_size, max_seq_len=max_seq_len, d_model=128, num_heads=2, num_layers=1).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "scheduler = UniformScheduler(num_classes=vocab_size, num_timesteps=num_timesteps, beta_start=0.0001, beta_end=0.02).to(\n",
    "    device\n",
    ")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "model.train()\n",
    "\n",
    "# Training loop - IndexError fixed!\n",
    "num_epochs = 1\n",
    "num_batches = 3  # Test a few batches\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for b, x_0 in enumerate(dataloader):\n",
    "        if b >= num_batches:\n",
    "            break\n",
    "\n",
    "        x_0 = x_0.to(device)\n",
    "        batch_size = x_0.shape[0]\n",
    "\n",
    "        # Sample random timesteps\n",
    "        t = torch.randint(0, num_timesteps, (batch_size,), device=device)\n",
    "\n",
    "        # Training step\n",
    "        x0_logits, kl_terms = training_step(scheduler=scheduler, model=model, x_0=x_0, t=t)\n",
    "        loss, loss_dict = d3pm_loss(x0_logits, kl_terms, x_0)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"  Batch {b}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch {epoch} completed. Average Loss: {epoch_loss / num_batches:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Training loop working correctly! IndexError has been fixed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c964c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df2fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff21f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss computation\n",
    "\n",
    "x_0 = torch.tensor([[0, 1, 2], [2, 1, 0]]).long().to(device)  # Use long for indices\n",
    "# preds are logits\n",
    "pred_logit = (\n",
    "    torch.tensor(\n",
    "        [\n",
    "            [[1000, 0, 0], [0, 1000, 0], [0, 0, 1000]],\n",
    "            [[0, 0, 1000], [0, 1000, 0], [1000, 0, 0]],\n",
    "        ]\n",
    "    )\n",
    "    .float()\n",
    "    .to(device)\n",
    ")\n",
    "\n",
    "# For cross-entropy, pred needs to be reshaped to [batch*seq, num_classes]\n",
    "ic(F.cross_entropy(pred_logit.view(-1, 3), x_0.view(-1), reduction=\"mean\"))\n",
    "\n",
    "# For the manual computation, convert x_0 to one-hot encoding\n",
    "psm = F.softmax(pred_logit, dim=-1)  # Apply softmax over the last dimension (classes)\n",
    "ic(psm, psm.shape)\n",
    "\n",
    "# Convert x_0 to one-hot encoding\n",
    "x_0_onehot = F.one_hot(x_0, num_classes=3).float()  # Shape: [2, 3, 3]\n",
    "ic(x_0_onehot, x_0_onehot.shape)\n",
    "\n",
    "# Now compute the negative log likelihood\n",
    "neg_log_likelihood = -torch.sum(x_0_onehot * torch.log(psm + 1e-10), dim=-1)\n",
    "ic(neg_log_likelihood, neg_log_likelihood.shape)\n",
    "ic(neg_log_likelihood.mean())  # Mean over the sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a86e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy data\n",
    "num_classes = 10\n",
    "num_timesteps = 5\n",
    "batch_size = 2\n",
    "seq_len = 3\n",
    "x_0 = torch.randint(0, num_classes, (batch_size, seq_len))  # Clean data\n",
    "x_t = torch.randint(0, num_classes, (batch_size, seq_len))  # Noisy data\n",
    "t = torch.randint(0, num_timesteps, (batch_size,))  # Timesteps\n",
    "\n",
    "# Model prediction (logits for pÌƒ_Î¸(xÌƒ_0|x_t))\n",
    "model_logits = torch.randn(batch_size, seq_len, num_classes)\n",
    "ic(model_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6238710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "class D3PMLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    D3PM Loss Function: L_Î» = L_vb + Î» * auxiliary_loss\n",
    "\n",
    "    Implements the hybrid loss from Equation 5 in the D3PM paper.\n",
    "    This is a simplified version that expects the model to handle\n",
    "    the complex posterior computations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lambda_weight: float = 0.001, use_auxiliary_loss: bool = True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.lambda_weight = lambda_weight\n",
    "        self.use_auxiliary_loss = use_auxiliary_loss\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        model_x0_logits: torch.Tensor,  # Model's prediction of pÌƒ_Î¸(xÌƒ_0|x_t)\n",
    "        model_kl_terms: torch.Tensor,  # Pre-computed KL divergence terms\n",
    "        x_0: torch.Tensor,  # Original clean data\n",
    "        t: torch.Tensor,  # Timesteps\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> tuple[torch.Tensor, dict]:\n",
    "        \"\"\"\n",
    "        Compute D3PM loss.\n",
    "\n",
    "        Args:\n",
    "            model_x0_logits: Model's logits for pÌƒ_Î¸(xÌƒ_0|x_t), shape [B, ..., K]\n",
    "            model_kl_terms: Pre-computed KL terms from model, shape [B, ...]\n",
    "            x_0: Clean data for auxiliary loss, shape [B, ...]\n",
    "            t: Timesteps, shape [B]\n",
    "            mask: Optional mask for sequence padding, shape [B, ...]\n",
    "\n",
    "        Returns:\n",
    "            loss: Total loss scalar\n",
    "            loss_dict: Dictionary with loss components\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. Auxiliary loss: E[-log pÌƒ_Î¸(x_0|x_t)]\n",
    "        auxiliary_loss = self._compute_auxiliary_loss(model_x0_logits, x_0, mask)\n",
    "\n",
    "        # 2. Variational bound loss: Use pre-computed KL terms from model\n",
    "        vb_loss = model_kl_terms.mean()\n",
    "\n",
    "        # 3. Combine losses\n",
    "        if self.use_auxiliary_loss:\n",
    "            total_loss = vb_loss + self.lambda_weight * auxiliary_loss\n",
    "        else:\n",
    "            total_loss = vb_loss\n",
    "\n",
    "        loss_dict = {\n",
    "            \"total_loss\": total_loss,\n",
    "            \"vb_loss\": vb_loss,\n",
    "            \"auxiliary_loss\": auxiliary_loss,\n",
    "            \"weighted_auxiliary\": self.lambda_weight * auxiliary_loss,\n",
    "        }\n",
    "\n",
    "        return total_loss, loss_dict\n",
    "\n",
    "    def _compute_auxiliary_loss(\n",
    "        self,\n",
    "        model_logits: torch.Tensor,  # [B, ..., K]\n",
    "        x_0: torch.Tensor,  # [B, ...]\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute auxiliary denoising loss: -log pÌƒ_Î¸(x_0|x_t)\n",
    "        \"\"\"\n",
    "        # Flatten all but last dimension for cross entropy\n",
    "        flat_logits = model_logits.view(-1, model_logits.size(-1))  # [B*N, K]\n",
    "        flat_targets = x_0.view(-1).long()  # [B*N]\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        ce_loss = F.cross_entropy(flat_logits, flat_targets, reduction=\"none\")  # [B*N]\n",
    "\n",
    "        if mask is not None:\n",
    "            flat_mask = mask.view(-1).float()  # [B*N]\n",
    "            ce_loss = ce_loss * flat_mask\n",
    "            return ce_loss.sum() / (flat_mask.sum() + 1e-8)\n",
    "        else:\n",
    "            return ce_loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de16ce78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cab4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3pmDiffusion(nn.Module):\n",
    "    \"\"\"Implementation of uniform transition with linear beta schedule\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, num_discrete_states: int, num_timesteps: int = 1000, beta_start: float = 0.0001, beta_end: float = 0.02\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_discrete_states = num_discrete_states\n",
    "        self.num_timesteps = num_timesteps\n",
    "\n",
    "        # More conservative beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # Precompute transition matrices\n",
    "        self.transition_matrices_t = nn.ParameterList()\n",
    "        self.cumulative_transition_matrices_t = nn.ParameterList()\n",
    "\n",
    "        Q_prev_cumulative = torch.eye(num_discrete_states)\n",
    "\n",
    "        for t in range(num_timesteps):\n",
    "            beta = self.betas[t].item()\n",
    "\n",
    "            # More conservative transition matrix\n",
    "            diag_prob = 1.0 - beta\n",
    "            off_diag_prob = beta / (num_discrete_states - 1) if num_discrete_states > 1 else 0.0\n",
    "\n",
    "            Q_t = torch.eye(num_discrete_states) * diag_prob\n",
    "            Q_t = (\n",
    "                Q_t\n",
    "                + (torch.ones(num_discrete_states, num_discrete_states) - torch.eye(num_discrete_states))\n",
    "                * off_diag_prob\n",
    "            )\n",
    "            Q_t = Q_t / Q_t.sum(dim=1, keepdim=True)\n",
    "\n",
    "            self.transition_matrices_t.append(nn.Parameter(Q_t, requires_grad=False))\n",
    "\n",
    "            Q_current_cumulative = torch.matmul(Q_t, Q_prev_cumulative)\n",
    "            self.cumulative_transition_matrices_t.append(nn.Parameter(Q_current_cumulative, requires_grad=False))\n",
    "            Q_prev_cumulative = Q_current_cumulative\n",
    "\n",
    "    def forward(self, x_0: torch.Tensor, t: torch.Tensor):\n",
    "        \"\"\"Forward diffusion process\"\"\"\n",
    "        original_shape = x_0.shape\n",
    "        batch_size = original_shape[0]\n",
    "        x_flat = x_0.view(batch_size, -1)\n",
    "        num_elements = x_flat.shape[1]\n",
    "\n",
    "        # Convert to one-hot encoding\n",
    "        x_one_hot = F.one_hot(x_flat, num_classes=self.num_discrete_states).float()\n",
    "\n",
    "        # Gather transition matrices for batch\n",
    "        Q_bar_t_batch = torch.stack([self.cumulative_transition_matrices_t[idx] for idx in t])\n",
    "\n",
    "        # Apply transition\n",
    "        next_state_probs = torch.bmm(x_one_hot, Q_bar_t_batch)\n",
    "\n",
    "        # Sample from categorical distribution\n",
    "        x_t = torch.multinomial(next_state_probs.view(-1, self.num_discrete_states), num_samples=1).squeeze(dim=1)\n",
    "        x_t = x_t.view(original_shape)\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def compute_loss(self, x_0: torch.Tensor, predicted_logits: torch.Tensor, t: torch.Tensor, pad_token_id: int = 0):\n",
    "        \"\"\"Compute proper D3PM loss with padding mask\"\"\"\n",
    "        batch_size, seq_len = x_0.shape\n",
    "\n",
    "        # Create padding mask\n",
    "        pad_mask = (x_0 != pad_token_id).float()  # 1 for real tokens, 0 for padding\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        ic(predicted_logits.view(-1, self.num_discrete_states).shape)\n",
    "        ic(x_0.view(-1).shape)\n",
    "        loss = F.cross_entropy(predicted_logits.view(-1, self.num_discrete_states), x_0.view(-1), reduction=\"none\")\n",
    "\n",
    "        # Apply padding mask\n",
    "        loss = loss.view(batch_size, seq_len)\n",
    "        masked_loss = loss * pad_mask\n",
    "\n",
    "        # Average over non-padded tokens\n",
    "        total_loss = masked_loss.sum() / pad_mask.sum().clamp(min=1)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "NUM_STATES = len(tokenizer)  # e.g., pixel values 0, 1, 2, 3\n",
    "NUM_CLASSES = len(tokenizer)  # Number of discrete states (e.g., vocabulary size)\n",
    "BETA_PER_STEP = 0.2  # Probability of changing state at each step\n",
    "NUM_TIMESTEPS = 2  # Number of diffusion steps\n",
    "# Create the forward diffusion module\n",
    "forward_diffuser = D3pmDiffusion(num_discrete_states=NUM_CLASSES, num_timesteps=NUM_TIMESTEPS).to(device)\n",
    "# x_0 = torch.tensor([[0, 1, 2], [2, 1, 0]]).to(device)\n",
    "# pred = torch.tensor([[[1, 0, 0], [0, 1, 0], [0, 0, 1]], [[0, 0, 1], [0, 1, 0], [1, 0, 0]]]).to(device)\n",
    "# forward_diffuser.compute_loss(x_0, pred, torch.tensor([0]).to(device), pad_token_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c16e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = next(iter(dataloader)).to(device)\n",
    "\n",
    "F.one_hot(inp, num_classes=len(tokenizer)).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd03ac10",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = next(iter(dataloader)).to(device)\n",
    "ic(inp.shape)\n",
    "\n",
    "\n",
    "def demo_noise(inp, line_nb, step):\n",
    "    src_line = tokenizer.decode(inp[line_nb].cpu().numpy())\n",
    "    noisy_inp = forward_diffuser.forward(inp[line_nb : line_nb + 1], torch.tensor([step]).to(device))\n",
    "    noisy_line = tokenizer.decode(noisy_inp[0].cpu().numpy())\n",
    "    return src_line, noisy_line\n",
    "\n",
    "\n",
    "ic.disable()\n",
    "ic.enable()\n",
    "sent_nb = 4\n",
    "print(demo_noise(inp, sent_nb, 0)[1])\n",
    "print(demo_noise(inp, sent_nb, 12)[1])\n",
    "print(demo_noise(inp, sent_nb, 20)[1])\n",
    "# print(demo_noise(inp, sent_nb, 99)[1])\n",
    "\n",
    "forward_diffuser.compute_loss(\n",
    "    inp,\n",
    "    F.one_hot(inp, num_classes=len(tokenizer)).float(),\n",
    "    torch.tensor([0]).to(device),\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a12987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, t):\n",
    "        half = self.lin.in_features // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half, dtype=torch.float32) / half).to(t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        return self.lin(emb)\n",
    "\n",
    "\n",
    "class DiffusionTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, dim=512, heads=8, layers=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
    "        self.time_emb = TimeEmbedding(dim)\n",
    "        enc_layer = nn.TransformerEncoderLayer(dim, heads, dim * 4)\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, layers)\n",
    "        self.to_logits = nn.Linear(dim, vocab_size)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, L = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(L, device=x.device))\n",
    "        temb = self.time_emb(t).unsqueeze(1)\n",
    "        h = tok + pos + temb\n",
    "        h = self.transformer(h.transpose(0, 1)).transpose(0, 1)\n",
    "        return self.to_logits(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545b5027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "import os\n",
    "\n",
    "# Hyperparameters\n",
    "seq_len = 20\n",
    "batch_size = 32\n",
    "num_epochs = 1  # 30\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5  # Added weight decay\n",
    "vocab_size = tokenizer.vocab_size\n",
    "num_timesteps = 100\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = SimpleDylanDataset(lines, tokenizer, seq_len=seq_len)\n",
    "dataset = SimpleDylanDataset(lines[:100], tokenizer, seq_len=seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate models\n",
    "diffuser = D3pmDiffusion(num_discrete_states=vocab_size, num_timesteps=num_timesteps).to(device)\n",
    "model = DiffusionTransformer(vocab_size=vocab_size, seq_len=seq_len).to(device)\n",
    "\n",
    "# Enhanced optimizer with weight decay\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Adaptive learning rate scheduler\n",
    "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=2, eta_min=1e-6)\n",
    "# Alternative: ReduceLROnPlateau for validation-based scheduling\n",
    "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca3fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard setup\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = f\"../runs/d3pm_training_{timestamp}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "print(f\"To view logs, run: tensorboard --logdir {log_dir}\")\n",
    "\n",
    "\n",
    "model_name = \"d3pm\"\n",
    "version = \"2_0\"  # Updated version with enhancements\n",
    "do_train = True\n",
    "\n",
    "if do_train:\n",
    "    print(\"\\nStarting enhanced training loop...\")\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "    global_step = 0\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    # Training metrics tracking\n",
    "    train_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Progress bar for batches\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "        for batch_idx, batch_x_0 in enumerate(pbar):\n",
    "            batch_x_0 = batch_x_0.to(device)\n",
    "            batch_size_actual = batch_x_0.shape[0]\n",
    "\n",
    "            # Sample random timesteps\n",
    "            timesteps = torch.randint(0, num_timesteps, (batch_size_actual,), device=device)\n",
    "\n",
    "            # Forward diffusion process\n",
    "            x_t = diffuser(batch_x_0, timesteps)\n",
    "\n",
    "            predicted_logits = model(x_t, timesteps)\n",
    "            loss = diffuser.compute_loss(batch_x_0, predicted_logits, timesteps, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping for stability\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            global_step += 1\n",
    "\n",
    "            # Get current learning rate\n",
    "            current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "            # Log to TensorBoard (every 10 steps to avoid too much logging)\n",
    "            if global_step % 10 == 0:\n",
    "                writer.add_scalar(\"Loss/Batch\", loss.item(), global_step)\n",
    "                writer.add_scalar(\"Learning_Rate\", current_lr, global_step)\n",
    "                writer.add_scalar(\n",
    "                    \"Gradient_Norm\",\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=float(\"inf\")),\n",
    "                    global_step,\n",
    "                )\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{current_lr:.2e}\", \"Step\": global_step})\n",
    "\n",
    "            # Memory cleanup\n",
    "            if batch_idx % 20 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                elif torch.backends.mps.is_available():\n",
    "                    torch.mps.empty_cache()\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        scheduler.step()\n",
    "        # For ReduceLROnPlateau, use: scheduler.step(avg_epoch_loss)\n",
    "\n",
    "        # Log epoch metrics to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Epoch\", avg_epoch_loss, epoch)\n",
    "        writer.add_scalar(\"Learning_Rate/Epoch\", current_lr, epoch)\n",
    "\n",
    "        # Log model parameters histogram every few epochs\n",
    "        if epoch % 5 == 0:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad:\n",
    "                    writer.add_histogram(f\"Parameters/{name}\", param.data, epoch)\n",
    "                    if param.grad is not None:\n",
    "                        writer.add_histogram(f\"Gradients/{name}\", param.grad.data, epoch)\n",
    "\n",
    "        # Save best model\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            best_model_path = f\"../models/{model_name}_best_{version}.pth\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"loss\": best_loss,\n",
    "                    \"global_step\": global_step,\n",
    "                },\n",
    "                best_model_path,\n",
    "            )\n",
    "            print(f\"âœ“ New best model saved with loss: {best_loss:.4f}\")\n",
    "\n",
    "        # Regular checkpoint saving\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            checkpoint_path = f\"../models/{model_name}_epoch_{epoch + 1}_{version}.pth\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"loss\": avg_epoch_loss,\n",
    "                    \"global_step\": global_step,\n",
    "                },\n",
    "                checkpoint_path,\n",
    "            )\n",
    "            print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} completed:\")\n",
    "        print(f\"  Average Loss: {avg_epoch_loss:.4f}\")\n",
    "        print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "        print(f\"  Global Step: {global_step}\")\n",
    "\n",
    "    # Final model save\n",
    "    final_model_path = f\"../models/{model_name}_final_{version}.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": num_epochs,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"loss\": avg_epoch_loss,\n",
    "            \"global_step\": global_step,\n",
    "            \"train_losses\": train_losses,\n",
    "            \"learning_rates\": learning_rates,\n",
    "        },\n",
    "        final_model_path,\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining complete!\")\n",
    "    print(f\"Final model saved: {final_model_path}\")\n",
    "    print(f\"Best loss achieved: {best_loss:.4f}\")\n",
    "    print(f\"Total training steps: {global_step}\")\n",
    "    print(f\"TensorBoard logs: {log_dir}\")\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "\n",
    "    # Plot training curves\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    # Loss curve\n",
    "    ax1.plot(train_losses)\n",
    "    ax1.set_title(\"Training Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Learning rate curve\n",
    "    ax2.plot(learning_rates)\n",
    "    ax2.set_title(\"Learning Rate Schedule\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Learning Rate\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(\"../plots\", exist_ok=True)\n",
    "    plt.savefig(f\"../plots/training_curves_{timestamp}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    # Load existing model\n",
    "    model_path = f\"../models/{model_name}_best_{version}.pth\"\n",
    "    if os.path.exists(model_path):\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        print(f\"Loaded model from {model_path}\")\n",
    "    else:\n",
    "        print(f\"Model file not found: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2683f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    input_prompt_text = \"the answer is\"  # Try different prompts\n",
    "    input_prompt_text = \"how many roads\"\n",
    "    # input_prompt_text = \"like a rolling\"\n",
    "\n",
    "    prompt_tokens = tokenizer.encode(input_prompt_text, add_special_tokens=False)\n",
    "    prompt_len = len(prompt_tokens)\n",
    "\n",
    "    if prompt_len >= seq_len:\n",
    "        raise ValueError(\n",
    "            f\"Prompt length ({prompt_len}) exceeds sequence length ({seq_len}). Please shorten the prompt.\"\n",
    "        )\n",
    "\n",
    "    initial_sequence = torch.full((1, seq_len), tokenizer.pad_token_id, dtype=torch.long, device=device)\n",
    "\n",
    "    initial_sequence[0, :prompt_len] = torch.tensor(prompt_tokens, dtype=torch.long, device=device)\n",
    "\n",
    "    # Fill the rest with random tokens (pure noise for the parts to be completed)\n",
    "    rand_tokens = torch.randint(low=0, high=vocab_size, size=(1, seq_len - prompt_len), device=device)\n",
    "    if tokenizer.pad_token_id != -1:  # Assuming -1 means no specific pad_token_id\n",
    "        rand_tokens[rand_tokens == tokenizer.pad_token_id] = tokenizer.unk_token_id\n",
    "\n",
    "    initial_sequence[0, prompt_len:] = rand_tokens[0]\n",
    "\n",
    "    print(f'Prompt: \"{input_prompt_text}\"')\n",
    "    print(f\"Initial noisy sequence:\\n{tokenizer.decode(initial_sequence[0].cpu().tolist())}\")\n",
    "\n",
    "    current_x = initial_sequence.clone()\n",
    "    # The actual denoising loop should go from T down to 1\n",
    "    inference_steps = num_timesteps  # Max T is num_timesteps (from 1 to num_timesteps)\n",
    "\n",
    "    for t_idx in tqdm(range(inference_steps, 0, -1), desc=\"Denoising with prompt\"):\n",
    "        t_tensor = torch.tensor([t_idx], dtype=torch.long, device=device)\n",
    "\n",
    "        # Model predicts logits for x_0 given current_x and t\n",
    "        predicted_logits_x0 = model(current_x, t_tensor)\n",
    "        predicted_probs_x0 = F.softmax(predicted_logits_x0, dim=-1)\n",
    "\n",
    "        # could use top-k sampling or temperature.\n",
    "        next_x_sampled = torch.argmax(predicted_probs_x0, dim=-1)  # (1, seq_len)\n",
    "\n",
    "        # Keep the prompt tokens unchanged\n",
    "        next_x_sampled[0, :prompt_len] = initial_sequence[0, :prompt_len]\n",
    "\n",
    "        current_x = next_x_sampled\n",
    "\n",
    "        # Print intermediate results for a few steps\n",
    "        if t_idx % (inference_steps // 5) == 0:\n",
    "            print(f\"  Step {t_idx}: {tokenizer.decode(current_x[0].cpu().tolist())}\")\n",
    "\n",
    "    final_generated_text = tokenizer.decode(current_x[0].cpu().tolist())\n",
    "    print(f\"\\nFinal generated text:\\n{final_generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccfc46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14bb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dace5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a limited dataset with only 100 records for system setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543652a3",
   "metadata": {},
   "source": [
    "# Improved D3PM Implementation\n",
    "\n",
    "Based on the analysis of the training issues, here are the key improvements:\n",
    "\n",
    "1. **Larger vocabulary size** (5000 tokens) for better expressiveness\n",
    "2. **Longer sequences** (64 tokens) to capture more meaningful patterns\n",
    "3. **Better diffusion schedule** with lower beta values\n",
    "4. **Improved model architecture** with larger dimensions\n",
    "5. **Proper D3PM loss formulation**\n",
    "6. **Better learning rate schedule**\n",
    "7. **Validation set** for monitoring overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae060e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved tokenizer with larger vocabulary\n",
    "print(\"Creating improved tokenizer with larger vocabulary...\")\n",
    "\n",
    "# Initialize Dylan tokenizer with larger vocabulary\n",
    "improved_tokenizer = SimpleDylanTokenizer(vocab_size=5000)\n",
    "\n",
    "# Train the tokenizer on Dylan lyrics\n",
    "improved_tokenizer.train_tokenizer(corpus=lines, save_path=\"./improved_dylan_tokenizer\")\n",
    "\n",
    "# Convert to HuggingFace format\n",
    "improved_hf_tokenizer = improved_tokenizer.get_transformers_tokenizer()\n",
    "\n",
    "print(f\"Improved tokenizer vocabulary size: {len(improved_hf_tokenizer)}\")\n",
    "print(f\"Special tokens: {improved_hf_tokenizer.special_tokens_map}\")\n",
    "\n",
    "# Test the improved tokenizer\n",
    "phrase = \"The answer my friend is blowin' in the wind\"\n",
    "tokens = improved_hf_tokenizer.encode(phrase, add_special_tokens=False)\n",
    "decoded = improved_hf_tokenizer.decode(tokens, skip_special_tokens=False)\n",
    "token_strs = improved_hf_tokenizer.convert_ids_to_tokens(tokens)\n",
    "\n",
    "print(f\"Original: {phrase}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Tokens: {token_strs}\")\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d60602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4497323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedTimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.mlp = nn.Sequential(nn.Linear(dim, dim * 4), nn.GELU(), nn.Linear(dim * 4, dim))\n",
    "\n",
    "    def forward(self, t):\n",
    "        half = self.dim // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half, dtype=torch.float32) / half).to(t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        return self.mlp(emb)\n",
    "\n",
    "\n",
    "class ImprovedDiffusionTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, dim=768, heads=12, layers=12, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.dim = dim\n",
    "\n",
    "        # Embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
    "        self.time_emb = ImprovedTimeEmbedding(dim)\n",
    "\n",
    "        # Transformer layers with improved architecture\n",
    "        encoder_layers = []\n",
    "        for _ in range(layers):\n",
    "            layer = nn.TransformerEncoderLayer(\n",
    "                d_model=dim,\n",
    "                nhead=heads,\n",
    "                dim_feedforward=dim * 4,\n",
    "                dropout=dropout,\n",
    "                activation=\"gelu\",\n",
    "                batch_first=True,\n",
    "                norm_first=True,  # Pre-norm for better training stability\n",
    "            )\n",
    "            encoder_layers.append(layer)\n",
    "\n",
    "        self.transformer = nn.ModuleList(encoder_layers)\n",
    "\n",
    "        # Output projection\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "        self.to_logits = nn.Linear(dim, vocab_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x, t, attention_mask=None):\n",
    "        B, L = x.shape\n",
    "\n",
    "        # Token embeddings\n",
    "        tok_emb = self.token_emb(x)  # (B, L, D)\n",
    "\n",
    "        # Position embeddings\n",
    "        pos_indices = torch.arange(L, device=x.device).expand(B, -1)\n",
    "        pos_emb = self.pos_emb(pos_indices)  # (B, L, D)\n",
    "\n",
    "        # Time embeddings\n",
    "        time_emb = self.time_emb(t).unsqueeze(1).expand(-1, L, -1)  # (B, L, D)\n",
    "\n",
    "        # Combine embeddings\n",
    "        h = tok_emb + pos_emb + time_emb\n",
    "\n",
    "        # Create attention mask for padding tokens\n",
    "        if attention_mask is None:\n",
    "            # Assume padding tokens are 0\n",
    "            attention_mask = (x != 0).float()\n",
    "\n",
    "        # Convert to transformer format (True = masked positions)\n",
    "        src_key_padding_mask = attention_mask == 0\n",
    "\n",
    "        # Apply transformer layers\n",
    "        for layer in self.transformer:\n",
    "            h = layer(h, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        # Layer norm and output projection\n",
    "        h = self.layer_norm(h)\n",
    "        logits = self.to_logits(h)\n",
    "\n",
    "        return logits\n",
    "\n",
    "\n",
    "print(\"Improved transformer model created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ddf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improved training setup\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Improved hyperparameters\n",
    "IMPROVED_HYPERPARAMS = {\n",
    "    \"seq_len\": IMPROVED_SEQ_LEN,\n",
    "    \"batch_size\": IMPROVED_BATCH_SIZE,\n",
    "    \"num_epochs\": 20,\n",
    "    \"learning_rate\": 3e-4,  # Higher learning rate\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"vocab_size\": len(improved_hf_tokenizer),\n",
    "    \"num_timesteps\": 1000,  # More timesteps\n",
    "    \"model_dim\": 768,  # Larger model\n",
    "    \"model_heads\": 12,\n",
    "    \"model_layers\": 8,  # Reasonable depth\n",
    "    \"dropout\": 0.1,\n",
    "    \"gradient_clip\": 1.0,\n",
    "    \"warmup_steps\": 1000,\n",
    "    \"beta_start\": 0.0001,\n",
    "    \"beta_end\": 0.02,  # More conservative\n",
    "}\n",
    "\n",
    "print(\"Improved hyperparameters:\")\n",
    "for key, value in IMPROVED_HYPERPARAMS.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Initialize improved models\n",
    "print(\"\\nInitializing improved models...\")\n",
    "improved_diffuser = ImprovedD3PM(\n",
    "    num_discrete_states=IMPROVED_HYPERPARAMS[\"vocab_size\"],\n",
    "    num_timesteps=IMPROVED_HYPERPARAMS[\"num_timesteps\"],\n",
    "    beta_start=IMPROVED_HYPERPARAMS[\"beta_start\"],\n",
    "    beta_end=IMPROVED_HYPERPARAMS[\"beta_end\"],\n",
    ").to(device)\n",
    "\n",
    "improved_model = ImprovedDiffusionTransformer(\n",
    "    vocab_size=IMPROVED_HYPERPARAMS[\"vocab_size\"],\n",
    "    seq_len=IMPROVED_HYPERPARAMS[\"seq_len\"],\n",
    "    dim=IMPROVED_HYPERPARAMS[\"model_dim\"],\n",
    "    heads=IMPROVED_HYPERPARAMS[\"model_heads\"],\n",
    "    layers=IMPROVED_HYPERPARAMS[\"model_layers\"],\n",
    "    dropout=IMPROVED_HYPERPARAMS[\"dropout\"],\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in improved_model.parameters()):,}\")\n",
    "\n",
    "# Improved optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(\n",
    "    improved_model.parameters(),\n",
    "    lr=IMPROVED_HYPERPARAMS[\"learning_rate\"],\n",
    "    weight_decay=IMPROVED_HYPERPARAMS[\"weight_decay\"],\n",
    "    betas=(0.9, 0.95),\n",
    ")\n",
    "\n",
    "# One cycle learning rate scheduler\n",
    "total_steps = len(train_dataloader) * IMPROVED_HYPERPARAMS[\"num_epochs\"]\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=IMPROVED_HYPERPARAMS[\"learning_rate\"],\n",
    "    total_steps=total_steps,\n",
    "    pct_start=0.1,  # Warmup for 10% of training\n",
    "    anneal_strategy=\"cos\",\n",
    "    cycle_momentum=True,\n",
    "    base_momentum=0.85,\n",
    "    max_momentum=0.95,\n",
    ")\n",
    "\n",
    "# TensorBoard setup\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = f\"../runs/improved_d3pm_{timestamp}\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "print(f\"TensorBoard logs: {log_dir}\")\n",
    "print(f\"Total training steps: {total_steps:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbe3d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_inference(model, diffuser, tokenizer, prompt, seq_len=64, num_steps=100, temperature=1.0, top_k=50):\n",
    "    \"\"\"Improved inference with better sampling strategies\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Prepare prompt\n",
    "        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "        prompt_len = len(prompt_tokens)\n",
    "\n",
    "        if prompt_len >= seq_len:\n",
    "            print(f\"Warning: Prompt too long ({prompt_len} tokens). Truncating.\")\n",
    "            prompt_tokens = prompt_tokens[: seq_len - 1]\n",
    "            prompt_len = len(prompt_tokens)\n",
    "\n",
    "        # Initialize sequence\n",
    "        initial_sequence = torch.full((1, seq_len), tokenizer.pad_token_id, dtype=torch.long, device=device)\n",
    "        initial_sequence[0, :prompt_len] = torch.tensor(prompt_tokens, dtype=torch.long, device=device)\n",
    "\n",
    "        # Fill rest with random tokens\n",
    "        rand_tokens = torch.randint(low=1, high=len(tokenizer) - 1, size=(1, seq_len - prompt_len), device=device)\n",
    "        initial_sequence[0, prompt_len:] = rand_tokens[0]\n",
    "\n",
    "        print(f'Prompt: \"{prompt}\"')\n",
    "        print(f\"Initial sequence: {tokenizer.decode(initial_sequence[0].tolist(), skip_special_tokens=True)}\")\n",
    "\n",
    "        current_x = initial_sequence.clone()\n",
    "\n",
    "        # Denoising loop\n",
    "        for t_idx in tqdm(range(num_steps, 0, -1), desc=\"Denoising\"):\n",
    "            t_tensor = torch.tensor([t_idx], dtype=torch.long, device=device)\n",
    "\n",
    "            # Model prediction\n",
    "            predicted_logits = model(current_x, t_tensor)\n",
    "\n",
    "            # Apply temperature\n",
    "            if temperature > 0:\n",
    "                predicted_logits = predicted_logits / temperature\n",
    "\n",
    "            # Top-k sampling for non-prompt tokens\n",
    "            for pos in range(prompt_len, seq_len):\n",
    "                logits = predicted_logits[0, pos, :]\n",
    "\n",
    "                if top_k > 0:\n",
    "                    # Top-k sampling\n",
    "                    top_k_logits, top_k_indices = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                    probs = F.softmax(top_k_logits, dim=-1)\n",
    "                    sampled_idx = torch.multinomial(probs, num_samples=1)\n",
    "                    current_x[0, pos] = top_k_indices[sampled_idx]\n",
    "                else:\n",
    "                    # Standard sampling\n",
    "                    probs = F.softmax(logits, dim=-1)\n",
    "                    current_x[0, pos] = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # Keep prompt tokens unchanged\n",
    "            current_x[0, :prompt_len] = initial_sequence[0, :prompt_len]\n",
    "\n",
    "            # Print progress\n",
    "            if t_idx % (num_steps // 5) == 0 or t_idx == 1:\n",
    "                intermediate_text = tokenizer.decode(current_x[0].cpu().tolist(), skip_special_tokens=True)\n",
    "                print(f\"  Step {t_idx:3d}: {intermediate_text}\")\n",
    "\n",
    "    final_text = tokenizer.decode(current_x[0].cpu().tolist(), skip_special_tokens=True)\n",
    "    print(f\"\\nFinal result: {final_text}\")\n",
    "    return final_text\n",
    "\n",
    "\n",
    "print(\"Improved inference function created.\")\n",
    "print(\"\\nTo test the improved model after training, use:\")\n",
    "print(\"improved_inference(improved_model, improved_diffuser, improved_hf_tokenizer, 'Your prompt here')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b748e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, diffuser, val_dataloader, tokenizer, device):\n",
    "    \"\"\"Validation function\"\"\"\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    num_val_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_x_0 in val_dataloader:\n",
    "            batch_x_0 = batch_x_0.to(device)\n",
    "            batch_size_actual = batch_x_0.shape[0]\n",
    "\n",
    "            # Sample timesteps\n",
    "            timesteps = torch.randint(0, diffuser.num_timesteps, (batch_size_actual,), device=device)\n",
    "\n",
    "            # Forward diffusion\n",
    "            x_t = diffuser(batch_x_0, timesteps)\n",
    "\n",
    "            # Model prediction\n",
    "            predicted_logits = model(x_t, timesteps)\n",
    "\n",
    "            # Compute loss using improved loss function\n",
    "            loss = diffuser.compute_loss(batch_x_0, predicted_logits, timesteps, pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "            total_val_loss += loss.item()\n",
    "            num_val_batches += 1\n",
    "\n",
    "    model.train()\n",
    "    return total_val_loss / num_val_batches if num_val_batches > 0 else float(\"inf\")\n",
    "\n",
    "\n",
    "# Improved training loop\n",
    "print(\"\\n=== Starting Improved Training ===\")\n",
    "\n",
    "model_name = \"improved_d3pm\"\n",
    "version = \"3_0\"\n",
    "do_train = True\n",
    "\n",
    "if do_train:\n",
    "    global_step = 0\n",
    "    best_val_loss = float(\"inf\")\n",
    "    patience = 3\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    learning_rates = []\n",
    "\n",
    "    for epoch in range(IMPROVED_HYPERPARAMS[\"num_epochs\"]):\n",
    "        improved_model.train()\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "\n",
    "        # Training loop\n",
    "        pbar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{IMPROVED_HYPERPARAMS['num_epochs']}\")\n",
    "\n",
    "        for batch_idx, batch_x_0 in enumerate(pbar):\n",
    "            batch_x_0 = batch_x_0.to(device)\n",
    "            batch_size_actual = batch_x_0.shape[0]\n",
    "\n",
    "            # Sample random timesteps\n",
    "            timesteps = torch.randint(0, improved_diffuser.num_timesteps, (batch_size_actual,), device=device)\n",
    "\n",
    "            # Forward diffusion\n",
    "            x_t = improved_diffuser(batch_x_0, timesteps)\n",
    "\n",
    "            # Model prediction\n",
    "            predicted_logits = improved_model(x_t, timesteps)\n",
    "\n",
    "            # Compute improved loss\n",
    "            loss = improved_diffuser.compute_loss(\n",
    "                batch_x_0, predicted_logits, timesteps, pad_token_id=improved_hf_tokenizer.pad_token_id\n",
    "            )\n",
    "\n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(improved_model.parameters(), max_norm=IMPROVED_HYPERPARAMS[\"gradient_clip\"])\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            global_step += 1\n",
    "\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "            # Log to TensorBoard\n",
    "            if global_step % 50 == 0:\n",
    "                writer.add_scalar(\"Loss/Train_Step\", loss.item(), global_step)\n",
    "                writer.add_scalar(\"Learning_Rate\", current_lr, global_step)\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{current_lr:.2e}\", \"Step\": global_step})\n",
    "\n",
    "            # Memory cleanup\n",
    "            if batch_idx % 50 == 0:\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                elif torch.backends.mps.is_available():\n",
    "                    torch.mps.empty_cache()\n",
    "\n",
    "        # Calculate epoch metrics\n",
    "        avg_train_loss = epoch_loss / num_batches\n",
    "\n",
    "        # Validation\n",
    "        print(\"\\nRunning validation...\")\n",
    "        avg_val_loss = validate_model(improved_model, improved_diffuser, val_dataloader, improved_hf_tokenizer, device)\n",
    "\n",
    "        # Store metrics\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        learning_rates.append(current_lr)\n",
    "\n",
    "        # Log epoch metrics\n",
    "        writer.add_scalar(\"Loss/Train_Epoch\", avg_train_loss, epoch)\n",
    "        writer.add_scalar(\"Loss/Validation_Epoch\", avg_val_loss, epoch)\n",
    "        writer.add_scalar(\"Learning_Rate/Epoch\", current_lr, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{IMPROVED_HYPERPARAMS['num_epochs']} Results:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Learning Rate: {current_lr:.2e}\")\n",
    "        print(f\"  Global Step: {global_step:,}\")\n",
    "\n",
    "        # Save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            best_model_path = f\"../models/{model_name}_best_{version}.pth\"\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": improved_model.state_dict(),\n",
    "                    \"diffuser_state_dict\": improved_diffuser.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "                    \"train_loss\": avg_train_loss,\n",
    "                    \"val_loss\": avg_val_loss,\n",
    "                    \"global_step\": global_step,\n",
    "                    \"hyperparams\": IMPROVED_HYPERPARAMS,\n",
    "                },\n",
    "                best_model_path,\n",
    "            )\n",
    "\n",
    "            print(f\"  âœ“ New best model saved! Val loss: {best_val_loss:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"  No improvement. Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "        # Sample generation every few epochs\n",
    "        if (epoch + 1) % 3 == 0:\n",
    "            print(\"\\nGenerating sample...\")\n",
    "            improved_model.eval()\n",
    "            with torch.no_grad():\n",
    "                try:\n",
    "                    sample_prompt = \"The answer my friend\"\n",
    "                    sample_tokens = improved_hf_tokenizer.encode(sample_prompt, add_special_tokens=False)\n",
    "\n",
    "                    if len(sample_tokens) < IMPROVED_SEQ_LEN:\n",
    "                        pad_length = IMPROVED_SEQ_LEN - len(sample_tokens)\n",
    "                        sample_input = torch.tensor(\n",
    "                            [sample_tokens + [improved_hf_tokenizer.pad_token_id] * pad_length],\n",
    "                            device=device,\n",
    "                            dtype=torch.long,\n",
    "                        )\n",
    "\n",
    "                        # Simple denoising\n",
    "                        timesteps_sample = torch.tensor([improved_diffuser.num_timesteps // 2], device=device)\n",
    "                        x_t_sample = improved_diffuser(sample_input, timesteps_sample)\n",
    "                        predicted_logits_sample = improved_model(x_t_sample, timesteps_sample)\n",
    "                        predicted_tokens = torch.argmax(predicted_logits_sample, dim=-1)\n",
    "\n",
    "                        generated_text = improved_hf_tokenizer.decode(\n",
    "                            predicted_tokens[0].cpu().tolist(), skip_special_tokens=True\n",
    "                        )\n",
    "                        print(f\"  Sample: '{generated_text}'\")\n",
    "                        writer.add_text(\"Sample_Generation\", generated_text, epoch)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"  Sample generation failed: {e}\")\n",
    "\n",
    "            improved_model.train()\n",
    "\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    # Final save\n",
    "    final_model_path = f\"../models/{model_name}_final_{version}.pth\"\n",
    "    torch.save(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": improved_model.state_dict(),\n",
    "            \"diffuser_state_dict\": improved_diffuser.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"learning_rates\": learning_rates,\n",
    "            \"best_val_loss\": best_val_loss,\n",
    "            \"global_step\": global_step,\n",
    "            \"hyperparams\": IMPROVED_HYPERPARAMS,\n",
    "        },\n",
    "        final_model_path,\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== Training Complete ===\")\n",
    "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "    print(f\"Final model saved: {final_model_path}\")\n",
    "    print(f\"Total steps: {global_step:,}\")\n",
    "    print(f\"TensorBoard logs: {log_dir}\")\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # Plot training curves\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Loss curves\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    ax1.plot(epochs_range, train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "    ax1.plot(epochs_range, val_losses, label=\"Validation Loss\", color=\"red\")\n",
    "    ax1.set_title(\"Training and Validation Loss\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Learning rate\n",
    "    ax2.plot(epochs_range, learning_rates)\n",
    "    ax2.set_title(\"Learning Rate Schedule\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Learning Rate\")\n",
    "    ax2.set_yscale(\"log\")\n",
    "    ax2.grid(True)\n",
    "\n",
    "    # Loss difference\n",
    "    loss_diff = np.array(val_losses) - np.array(train_losses)\n",
    "    ax3.plot(epochs_range, loss_diff)\n",
    "    ax3.axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "    ax3.set_title(\"Validation - Training Loss (Overfitting Check)\")\n",
    "    ax3.set_xlabel(\"Epoch\")\n",
    "    ax3.set_ylabel(\"Loss Difference\")\n",
    "    ax3.grid(True)\n",
    "\n",
    "    # Training progress\n",
    "    ax4.plot(epochs_range, train_losses, label=\"Train\", alpha=0.7)\n",
    "    ax4.plot(epochs_range, val_losses, label=\"Validation\", alpha=0.7)\n",
    "    ax4.fill_between(epochs_range, train_losses, alpha=0.3)\n",
    "    ax4.fill_between(epochs_range, val_losses, alpha=0.3)\n",
    "    ax4.set_title(\"Training Progress Overview\")\n",
    "    ax4.set_xlabel(\"Epoch\")\n",
    "    ax4.set_ylabel(\"Loss\")\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(\"../plots\", exist_ok=True)\n",
    "    plt.savefig(f\"../plots/improved_training_curves_{timestamp}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"Training disabled. Set do_train=True to start training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4573f8",
   "metadata": {},
   "source": [
    "# Summary of Key Improvements to Fix Training Loss\n",
    "\n",
    "## Issues Identified in Original Implementation:\n",
    "\n",
    "1. **Vocabulary Size**: 1000 tokens â†’ **5000 tokens** (better expressiveness)\n",
    "2. **Sequence Length**: 16-20 tokens â†’ **64 tokens** (capture more context)\n",
    "3. **Diffusion Schedule**: Beta end 0.5 â†’ **0.02** (less aggressive noise)\n",
    "4. **Model Architecture**: 512 dim, 6 layers â†’ **768 dim, 8 layers** (more capacity)\n",
    "5. **Loss Function**: Basic CrossEntropy â†’ **Proper D3PM loss with padding masks**\n",
    "6. **Learning Rate**: 1e-4 â†’ **3e-4** with OneCycle scheduler\n",
    "7. **Validation**: None â†’ **Train/val split with early stopping**\n",
    "8. **Attention**: No masking â†’ **Proper attention masking for padding**\n",
    "\n",
    "## Expected Improvements:\n",
    "\n",
    "- **Loss should drop below 2.0** with these changes\n",
    "- Better text generation quality\n",
    "- More stable training\n",
    "- Proper overfitting detection\n",
    "\n",
    "## If Loss Still Doesn't Improve:\n",
    "\n",
    "1. **Check data quality**: Ensure sufficient text variety\n",
    "2. **Increase model size**: Try 1024 dim, 12 layers\n",
    "3. **Adjust diffusion schedule**: Try linear/cosine schedules\n",
    "4. **Learning rate**: Try 1e-3 or adaptive scheduling\n",
    "5. **Batch size**: Increase to 32-64 if memory allows\n",
    "6. **Training time**: Train for more epochs (50+)\n",
    "\n",
    "## Quick Start:\n",
    "\n",
    "```python\n",
    "# Run the improved training\n",
    "do_train = True  # Set this to True in the training cell\n",
    "\n",
    "# After training, test with:\n",
    "improved_inference(\n",
    "    improved_model, improved_diffuser, improved_hf_tokenizer,\n",
    "    \"The answer my friend\", temperature=0.8, top_k=50\n",
    ")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-implementations (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
