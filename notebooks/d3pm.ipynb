{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e60c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# | default_exp attention\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4404c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import uuid\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "from icecream import ic\n",
    "import math\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "868188ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7edbbc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Hard Times In New York Town',\n",
       "  'Come you ladies and you gentlemen, a-listen to my song',\n",
       "  'Sing it to you right, but you might think it’s wrong',\n",
       "  'Just a little glimpse of a story I’ll tell',\n",
       "  '’Bout an East Coast city that you all know well',\n",
       "  'It’s hard times in the city',\n",
       "  'Livin’ down in New York town',\n",
       "  'Old New York City is a friendly old town',\n",
       "  'From Washington Heights to Harlem on down',\n",
       "  'There’s a-mighty many people all millin’ all around'],\n",
       " 14318)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../dataset/bob_dylan_lyrics.csv\")\n",
    "lines = []\n",
    "nb_rows = 999999\n",
    "row_id = 0\n",
    "for r in df.iterrows():\n",
    "    # todo: one line is one sentence.\n",
    "    lines.append(r[1][\"title\"])\n",
    "    # sentences.append(r[1][\"title\"] + \"\\n\" + r[1][\"lyrics\"])\n",
    "    lyrics = r[1][\"lyrics\"].split(\"\\n\")\n",
    "    for line in lyrics:\n",
    "        if len(line.strip()) > 0:\n",
    "            lines.append(line.strip())\n",
    "        row_id += 1\n",
    "\n",
    "lines[:10], len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6bab58",
   "metadata": {},
   "source": [
    "# Simple Custom Tokenizer for Bob Dylan Lyrics\n",
    "\n",
    "Create a simple BPE (Byte-Pair Encoding) tokenizer trained specifically on Dylan's lyrics.\n",
    "This will:\n",
    "1. Learn Dylan's vocabulary efficiently\n",
    "2. Handle his common words and phrases better than BERT\n",
    "3. Use a smaller vocabulary size for memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92266e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, normalizers\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "class SimpleDylanTokenizer:\n",
    "    def __init__(self, vocab_size=3000):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tokenizer = None\n",
    "\n",
    "    def train_tokenizer(self, corpus: list[str], save_path: str = \"./simple_dylan_tokenizer\"):\n",
    "        # Initialize simple BPE tokenizer\n",
    "        tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "        # Simple whitespace pre-tokenization\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "\n",
    "        # Simple trainer\n",
    "        trainer = BpeTrainer(\n",
    "            vocab_size=self.vocab_size, special_tokens=[\"[PAD]\", \"[UNK]\", \"[MASK]\"], min_frequency=2, show_progress=True\n",
    "        )\n",
    "\n",
    "        # Train the tokenizer\n",
    "        tokenizer.train_from_iterator(corpus, trainer)\n",
    "\n",
    "        # Save tokenizer\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        tokenizer.save(f\"{save_path}/tokenizer.json\")\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        print(f\"Tokenizer trained and saved to {save_path}\")\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def load_tokenizer(self, save_path=\"./simple_dylan_tokenizer\"):\n",
    "        \"\"\"Load the trained tokenizer\"\"\"\n",
    "        tokenizer_path = f\"{save_path}/tokenizer.json\"\n",
    "        if os.path.exists(tokenizer_path):\n",
    "            self.tokenizer = Tokenizer.from_file(tokenizer_path)\n",
    "            return self.tokenizer\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Tokenizer not found at {tokenizer_path}\")\n",
    "\n",
    "    def get_transformers_tokenizer(self):\n",
    "        \"\"\"Convert to HuggingFace tokenizer for compatibility\"\"\"\n",
    "        if self.tokenizer is None:\n",
    "            raise ValueError(\"Tokenizer not trained or loaded\")\n",
    "\n",
    "        # Create fast tokenizer wrapper\n",
    "        fast_tokenizer = PreTrainedTokenizerFast(\n",
    "            tokenizer_object=self.tokenizer, pad_token=\"[PAD]\", unk_token=\"[UNK]\", mask_token=\"[MASK]\"\n",
    "        )\n",
    "\n",
    "        return fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76920799",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(tokenizer): 3000\n",
      "ic| tokenizer.special_tokens_map: {'mask_token'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Tokenizer trained and saved to ./simple_dylan_tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ": '[MASK]', 'pad_token': '[PAD]', 'unk_token': '[UNK]'}\n",
      "ic| phrase: \"The answer my friend is blowin' in the wind\"\n",
      "ic| decoded: \"The answer my friend is blowin ' in the wind\"\n",
      "ic| token_strs: ['The', 'answer', 'my', 'friend', 'is', 'blowin', \"'\", 'in', 'the', 'wind']\n"
     ]
    }
   ],
   "source": [
    "# Initialize simple Dylan tokenizer\n",
    "dylan_tokenizer = SimpleDylanTokenizer(vocab_size=3000)\n",
    "\n",
    "# Train the tokenizer on Dylan lyrics\n",
    "dylan_tokenizer.train_tokenizer(corpus=lines, save_path=\"./simple_dylan_tokenizer\")\n",
    "\n",
    "# Convert to HuggingFace format for compatibility\n",
    "tokenizer = dylan_tokenizer.get_transformers_tokenizer()\n",
    "\n",
    "ic(len(tokenizer))\n",
    "ic(tokenizer.special_tokens_map)\n",
    "\n",
    "\n",
    "phrase = \"The answer my friend is blowin' in the wind\"\n",
    "\n",
    "tokens = tokenizer.encode(phrase, add_special_tokens=False)\n",
    "decoded = tokenizer.decode(tokens, skip_special_tokens=False)\n",
    "token_strs = tokenizer.convert_ids_to_tokens(tokens)\n",
    "ic(phrase)\n",
    "ic(decoded)\n",
    "ic(token_strs);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a50552a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up dataset with simple Dylan tokenizer...\n",
      "Max sequence length in dataset: 41\n",
      "Dataset created with 14318 examples\n",
      "Sequence length: 16\n",
      "Batch size: 8\n",
      "Tokenizer vocabulary size: 3000\n",
      "\n",
      "Sample batch shape: torch.Size([8, 16])\n",
      "Sample sequence: And glow ed like burnin ’ co al [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "# Update dataset to use simple Dylan tokenizer\n",
    "print(\"Setting up dataset with simple Dylan tokenizer...\")\n",
    "\n",
    "\n",
    "# Create simple dataset\n",
    "seq_len = 16  # Keep shorter sequences for memory efficiency\n",
    "batch_size = 8\n",
    "\n",
    "\n",
    "class SimpleDylanDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, seq_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.seq_len = seq_len\n",
    "        self.examples = []\n",
    "        max_seq_len = 0\n",
    "\n",
    "        for line in texts:\n",
    "            # Simple tokenization - no structure tokens\n",
    "            tokens = tokenizer.encode(line.strip(), add_special_tokens=False)\n",
    "            token_nb = len(tokens)\n",
    "            max_seq_len = max(max_seq_len, token_nb)\n",
    "            # Truncate if too long\n",
    "\n",
    "            if token_nb > seq_len:\n",
    "                tokens = tokens[:seq_len]\n",
    "\n",
    "            if token_nb > 0:  # Skip empty sequences\n",
    "                self.examples.append(tokens)\n",
    "        print(f\"Max sequence length in dataset: {max_seq_len}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.examples[idx]\n",
    "        pad_id = self.tokenizer.pad_token_id if hasattr(self.tokenizer, \"pad_token_id\") else 0\n",
    "\n",
    "        # Pad to sequence length\n",
    "        padded = tokens + [pad_id] * (self.seq_len - len(tokens))\n",
    "        return torch.tensor(padded[: self.seq_len], dtype=torch.long)\n",
    "\n",
    "\n",
    "# Create dataset with selected tokenizer\n",
    "dataset = SimpleDylanDataset(lines, tokenizer, seq_len=seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Dataset created with {len(dataset)} examples\")\n",
    "print(f\"Sequence length: {seq_len}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "\n",
    "# Test the dataset\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(f\"\\nSample batch shape: {sample_batch.shape}\")\n",
    "print(f\"Sample sequence: {tokenizer.decode(sample_batch[0].tolist(), skip_special_tokens=False)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c0b8839",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteDiffusion:\n",
    "    def __init__(self, num_tokens, timesteps, beta_start=0.0001, beta_end=0.02):\n",
    "        self.num_tokens = num_tokens\n",
    "        self.timesteps = timesteps\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def q_sample(self, x0, t):\n",
    "        B, L = x0.shape\n",
    "        out = torch.zeros_like(x0)\n",
    "        for i in range(B):\n",
    "            a_bar = self.alpha_bars[t[i]].item()\n",
    "            mask = torch.rand((L,), device=x0.device) >= a_bar\n",
    "            out[i] = x0[i].clone()\n",
    "            noise = torch.randint(0, self.num_tokens, (L,), device=x0.device)\n",
    "            out[i][mask] = noise[mask]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70eaf511",
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion = DiscreteDiffusion(num_tokens=len(tokenizer), timesteps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9cab4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructed Transition Matrix Q:\n",
      "tensor([[0.8000, 0.0667, 0.0667, 0.0667],\n",
      "        [0.0667, 0.8000, 0.0667, 0.0667],\n",
      "        [0.0667, 0.0667, 0.8000, 0.0667],\n",
      "        [0.0667, 0.0667, 0.0667, 0.8000]])\n",
      "\n",
      "Original data (x_0):\n",
      "tensor([[0, 1, 2, 3, 0],\n",
      "        [3, 2, 1, 0, 3]])\n",
      "\n",
      "Noisy data after 1 step (x_1):\n",
      "tensor([[2, 1, 2, 3, 0],\n",
      "        [1, 2, 1, 0, 3]])\n",
      "\n",
      "Simulating multiple steps:\n",
      "Step 0:\n",
      "tensor([[0, 1, 2, 3, 0],\n",
      "        [3, 2, 1, 0, 3]])\n",
      "Step 1:\n",
      "tensor([[1, 1, 1, 1, 0],\n",
      "        [3, 2, 1, 0, 3]])\n",
      "Step 2:\n",
      "tensor([[1, 1, 1, 1, 0],\n",
      "        [3, 2, 1, 0, 3]])\n",
      "Step 3:\n",
      "tensor([[1, 1, 2, 0, 0],\n",
      "        [3, 2, 3, 0, 2]])\n",
      "Step 4:\n",
      "tensor([[1, 1, 2, 3, 0],\n",
      "        [3, 2, 3, 0, 2]])\n",
      "\n",
      "Final noisy data after 5 steps:\n",
      "tensor([[1, 1, 2, 3, 0],\n",
      "        [3, 2, 3, 3, 1]])\n",
      "\n",
      "Original Image Data:\n",
      "tensor([[[0, 1],\n",
      "         [2, 3]],\n",
      "\n",
      "        [[3, 2],\n",
      "         [1, 0]]])\n",
      "\n",
      "Noisy Image Data:\n",
      "tensor([[[0, 1],\n",
      "         [2, 3]],\n",
      "\n",
      "        [[3, 2],\n",
      "         [1, 0]]])\n",
      "\n",
      "--- Demonstrating higher beta (more noise) ---\n",
      "Constructed Transition Matrix Q:\n",
      "tensor([[0.2000, 0.2667, 0.2667, 0.2667],\n",
      "        [0.2667, 0.2000, 0.2667, 0.2667],\n",
      "        [0.2667, 0.2667, 0.2000, 0.2667],\n",
      "        [0.2667, 0.2667, 0.2667, 0.2000]])\n",
      "\n",
      "Original data for high beta:\n",
      "tensor([[0, 0, 0, 0],\n",
      "        [1, 1, 1, 1]])\n",
      "Step 0:\n",
      "tensor([[0, 0, 0, 0],\n",
      "        [1, 1, 1, 1]])\n",
      "Step 1:\n",
      "tensor([[2, 1, 2, 2],\n",
      "        [1, 2, 1, 2]])\n",
      "\n",
      "Final noisy data (high beta):\n",
      "tensor([[2, 3, 2, 2],\n",
      "        [3, 1, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "class DiscreteDiffusionForward(nn.Module):\n",
    "    def __init__(self, num_discrete_states: int, beta: float):\n",
    "        \"\"\"\n",
    "        Initializes the forward discrete diffusion process.\n",
    "\n",
    "        Args:\n",
    "            num_discrete_states (int): The number of possible discrete states (e.g., 4 for pixel values 0-3, or vocabulary size).\n",
    "            beta (float): A scalar determining the noise level. Higher beta means more aggressive noise.\n",
    "                          Beta is often scheduled per timestep in full D3PMs, but here it's fixed for simplicity.\n",
    "                          It controls the probability of *not* changing state.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_discrete_states = num_discrete_states\n",
    "        self.beta = beta\n",
    "\n",
    "        # Construct the transition matrix Q.\n",
    "        # For simplicity, we'll use a uniform transition model:\n",
    "        # A state can transition to any other state (with probability beta / (K-1))\n",
    "        # or stay the same (with probability 1 - beta).\n",
    "        # Note: In a real D3PM, Q_t would be different for each t. Here, it's fixed.\n",
    "\n",
    "        # Diagonal elements (P(x_t = i | x_{t-1} = i))\n",
    "        # This is the probability of staying in the same state.\n",
    "        diag_prob = 1.0 - beta\n",
    "\n",
    "        # Off-diagonal elements (P(x_t = j | x_{t-1} = i), for i != j)\n",
    "        # This is the probability of transitioning to any other state.\n",
    "        # We distribute the 'beta' probability uniformly among the other K-1 states.\n",
    "        off_diag_prob = beta / (num_discrete_states - 1) if num_discrete_states > 1 else 0.0\n",
    "\n",
    "        # Create the identity matrix\n",
    "        Q = torch.eye(num_discrete_states)\n",
    "\n",
    "        # Fill off-diagonal elements\n",
    "        Q = Q * diag_prob  # Set diagonal\n",
    "        Q = (\n",
    "            Q + (torch.ones(num_discrete_states, num_discrete_states) - torch.eye(num_discrete_states)) * off_diag_prob\n",
    "        )  # Set off-diagonal\n",
    "\n",
    "        # Ensure probabilities sum to 1 for each row\n",
    "        # (This should be true by construction, but good for robustness)\n",
    "        Q = Q / Q.sum(dim=1, keepdim=True)\n",
    "\n",
    "        self.register_buffer(\"transition_matrix\", Q)\n",
    "        print(f\"Constructed Transition Matrix Q:\\n{self.transition_matrix}\")\n",
    "\n",
    "    def forward(self, x_t_minus_1: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Applies one step of the forward diffusion process to a batch of discrete data.\n",
    "\n",
    "        Args:\n",
    "            x_t_minus_1 (torch.Tensor): A batch of discrete data, representing x_{t-1}.\n",
    "                                        Shape: (batch_size, sequence_length) or (batch_size, height, width) etc.\n",
    "                                        Values must be integers representing the discrete states (0 to num_discrete_states-1).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The noisy data x_t after one diffusion step. Same shape as input.\n",
    "        \"\"\"\n",
    "        if x_t_minus_1.max() >= self.num_discrete_states or x_t_minus_1.min() < 0:\n",
    "            raise ValueError(\n",
    "                f\"Input tensor values must be within [0, {self.num_discrete_states - 1}], \"\n",
    "                f\"but got min={x_t_minus_1.min().item()}, max={x_t_minus_1.max().item()}\"\n",
    "            )\n",
    "\n",
    "        original_shape = x_t_minus_1.shape\n",
    "        # Flatten the input to apply matrix multiplication for each element\n",
    "        x_flat = x_t_minus_1.view(-1)  # (batch_size * num_elements)\n",
    "\n",
    "        # Convert flat integer indices to one-hot encoding\n",
    "        # This is necessary because matrix multiplication works with distributions\n",
    "        # For each element, we have a one-hot vector indicating its current state.\n",
    "        x_one_hot = F.one_hot(\n",
    "            x_flat, num_classes=self.num_discrete_states\n",
    "        ).float()  # (batch_size * num_elements, num_discrete_states)\n",
    "\n",
    "        # Apply the transition matrix\n",
    "        # Each row of x_one_hot is [0,0,1,0] if the element is in state 2.\n",
    "        # Multiplying this by Q gives the probability distribution over possible next states for that element.\n",
    "        # (batch_size * num_elements, num_discrete_states) @ (num_discrete_states, num_discrete_states)\n",
    "        next_state_probs = torch.matmul(x_one_hot, self.transition_matrix)\n",
    "\n",
    "        # Sample the next state from the probability distribution\n",
    "        # torch.multinomial samples indices based on probabilities\n",
    "        x_t = torch.multinomial(next_state_probs, num_samples=1).squeeze(dim=1)\n",
    "\n",
    "        # Reshape back to the original shape\n",
    "        x_t = x_t.view(original_shape)\n",
    "\n",
    "        return x_t\n",
    "\n",
    "\n",
    "# Define parameters\n",
    "NUM_STATES = 4  # e.g., pixel values 0, 1, 2, 3\n",
    "BETA_PER_STEP = 0.2  # Probability of changing state at each step\n",
    "\n",
    "# Create the forward diffusion module\n",
    "forward_diffuser = DiscreteDiffusionForward(num_discrete_states=NUM_STATES, beta=BETA_PER_STEP)\n",
    "\n",
    "# Example input data (e.g., a batch of 2 sequences of length 5, or 2x2 images)\n",
    "# Values should be integers from 0 to NUM_STATES-1\n",
    "x_0 = torch.tensor([[0, 1, 2, 3, 0], [3, 2, 1, 0, 3]], dtype=torch.long)\n",
    "print(f\"\\nOriginal data (x_0):\\n{x_0}\")\n",
    "\n",
    "# Simulate one step of diffusion\n",
    "x_1 = forward_diffuser(x_0)\n",
    "print(f\"\\nNoisy data after 1 step (x_1):\\n{x_1}\")\n",
    "\n",
    "# Simulate multiple steps\n",
    "current_x = x_0.clone()\n",
    "print(\"\\nSimulating multiple steps:\")\n",
    "for t in range(5):  # Simulate 5 steps\n",
    "    print(f\"Step {t}:\")\n",
    "    print(current_x)\n",
    "    current_x = forward_diffuser(current_x)\n",
    "\n",
    "print(f\"\\nFinal noisy data after 5 steps:\\n{current_x}\")\n",
    "\n",
    "# Example with a different shape (e.g., a batch of images)\n",
    "image_data = torch.tensor([[[0, 1], [2, 3]], [[3, 2], [1, 0]]], dtype=torch.long)\n",
    "print(f\"\\nOriginal Image Data:\\n{image_data}\")\n",
    "noisy_image = forward_diffuser(image_data)\n",
    "print(f\"\\nNoisy Image Data:\\n{noisy_image}\")\n",
    "\n",
    "# Demonstrate that as beta increases, noise is added more aggressively\n",
    "print(\"\\n--- Demonstrating higher beta (more noise) ---\")\n",
    "forward_diffuser_high_beta = DiscreteDiffusionForward(num_discrete_states=NUM_STATES, beta=0.8)\n",
    "x_0_high_beta = torch.tensor([[0, 0, 0, 0], [1, 1, 1, 1]], dtype=torch.long)\n",
    "print(f\"\\nOriginal data for high beta:\\n{x_0_high_beta}\")\n",
    "current_x_high_beta = x_0_high_beta.clone()\n",
    "for t in range(2):\n",
    "    print(f\"Step {t}:\")\n",
    "    print(current_x_high_beta)\n",
    "    current_x_high_beta = forward_diffuser_high_beta(current_x_high_beta)\n",
    "print(f\"\\nFinal noisy data (high beta):\\n{current_x_high_beta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9889cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7246a1d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f119720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "361dcfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Things fall to pie ces in my face [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Things fall to pie ces in my face [PAD] [PAD] ó [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Things lights to med Henry in my face [PAD] paint [PAD] [PAD] [PAD] strong [PAD] [PAD]\n",
      "Things fall fist pie org in ts drivin eve [PAD] man [PAD] [PAD] Beyond roo hungry\n"
     ]
    }
   ],
   "source": [
    "def demo_noise(lines, line_nb, step):\n",
    "    src_line = tokenizer.decode(inp[line_nb].cpu().numpy())\n",
    "    noisy_inp = diffusion.q_sample(inp[line_nb : line_nb + 1], torch.tensor([step]).to(device))\n",
    "    noisy_line = tokenizer.decode(noisy_inp[0].cpu().numpy())\n",
    "    return src_line, noisy_line\n",
    "\n",
    "\n",
    "ic.disable()\n",
    "ic.enable()\n",
    "sent_nb = 4\n",
    "print(demo_noise(lines, sent_nb, 0)[1])\n",
    "print(demo_noise(lines, sent_nb, 12)[1])\n",
    "print(demo_noise(lines, sent_nb, 60)[1])\n",
    "print(demo_noise(lines, sent_nb, 99)[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13a12987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, t):\n",
    "        half = self.lin.in_features // 2\n",
    "        freqs = torch.exp(-math.log(10000) * torch.arange(half, dtype=torch.float32) / half).to(t.device)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "        return self.lin(emb)\n",
    "\n",
    "\n",
    "class DiffusionTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, dim=512, heads=8, layers=6):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
    "        self.time_emb = TimeEmbedding(dim)\n",
    "        enc_layer = nn.TransformerEncoderLayer(dim, heads, dim * 4)\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, layers)\n",
    "        self.to_logits = nn.Linear(dim, vocab_size)\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, L = x.shape\n",
    "        tok = self.token_emb(x)\n",
    "        pos = self.pos_emb(torch.arange(L, device=x.device))\n",
    "        temb = self.time_emb(t).unsqueeze(1)\n",
    "        h = tok + pos + temb\n",
    "        h = self.transformer(h.transpose(0, 1)).transpose(0, 1)\n",
    "        return self.to_logits(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "319bd9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating limited dataset with 100 records for faster setup...\n",
      "Max sequence length in dataset: 21\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating limited dataset with 100 records for faster setup...\")\n",
    "\n",
    "# Take only first 100 lines for quick testing\n",
    "limited_lines = lines[:100]\n",
    "\n",
    "# Create limited dataset\n",
    "limited_dataset = SimpleDylanDataset(limited_lines, tokenizer, seq_len=seq_len)\n",
    "limited_dataloader = DataLoader(limited_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6d4a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: ../runs/dylan_d3pm/01-06-2025_18:36:14\n",
      "Starting test training with 3000 vocab size...\n",
      "Model parameters: 543,160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6fe6e25aa44af499e6672a8e73a0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "🚀 Training:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8365c306c944889881ebd1cc68fdbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 1:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/10 | Loss: 0.7065\n",
      "Saved checkpoint: d3pm_epoch_1.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a879af5edaa6481e9787eb23e3d419e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 2:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 0.4706\n",
      "Saved checkpoint: d3pm_epoch_2.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d17e5f83a6e495a9193de5253846db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 3:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 0.3695\n",
      "Saved checkpoint: d3pm_epoch_3.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7409c7d3bd344819a1e1ef6b2b810c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 4:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 0.3057\n",
      "Saved checkpoint: d3pm_epoch_4.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502fef7ca13143a48582117defa68a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 5:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 0.2596\n",
      "Saved checkpoint: d3pm_epoch_5.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64597316fc5d498495c674b77cca1c7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 6:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 0.2286\n",
      "Saved checkpoint: d3pm_epoch_6.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccdcec9e34c4a709d501067c73d9ba2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 7:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 0.2107\n",
      "Saved checkpoint: d3pm_epoch_7.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d896e2e767d4565ae21c505ea78f00f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 8:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 0.1965\n",
      "Saved checkpoint: d3pm_epoch_8.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618c747bb60b4b608cec829e9532614d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 9:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 0.1868\n",
      "Saved checkpoint: d3pm_epoch_9.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98302ee88c6c44b68b16f1a1b61154aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 10:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 0.1778\n",
      "Saved checkpoint: d3pm_epoch_10.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0099f518f1434baaaf521c668ebe4867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ⚙️ Inner Task 11:   0%|          | 0/1790 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 0.1724\n",
      "Saved checkpoint: d3pm_epoch_11.pth\n",
      "Test training completed successfully!\n",
      "Final model size: 543,160 parameters\n"
     ]
    }
   ],
   "source": [
    "# Quick test training with simple Dylan tokenizer\n",
    "seq_len = 32  # Reduced from 32\n",
    "batch_size = 64  # Reduced from 32\n",
    "\n",
    "# Use our simple dataset\n",
    "diffusion = DiscreteDiffusion(num_tokens=len(tokenizer), timesteps=50)  # Reduced timesteps\n",
    "model = DiffusionTransformer(vocab_size=len(tokenizer), seq_len=seq_len, dim=64, heads=4, layers=3)\n",
    "model.to(device)\n",
    "model_name = \"dylan_d3pm\"\n",
    "epochs = 10  # Just test with 5 epochs\n",
    "lr = 1e-4\n",
    "\n",
    "# train_dataloader = limited_dataloader\n",
    "train_dataloader = dataloader\n",
    "\n",
    "# Set up TensorBoard logging\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "log_dir = f\"../runs/{model_name}/{timestamp}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "print(f\"Starting test training with {len(tokenizer)} vocab size...\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "global_step = 0\n",
    "epoch_bar = tqdm(range(epochs + 1), desc=\"🚀 Training\", position=0, leave=True)\n",
    "for epoch in epoch_bar:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batch_nb = len(train_dataloader)\n",
    "    batch_count = 0\n",
    "    inner_pbar = tqdm(range(batch_nb), desc=f\"  ⚙️ Inner Task {epoch + 1}\", position=1, leave=False, colour=\"green\")\n",
    "\n",
    "    for batch_idx in inner_pbar:\n",
    "        inp = next(iter(train_dataloader))\n",
    "\n",
    "        inp = inp.to(device)\n",
    "        B = inp.size(0)\n",
    "        t = torch.randint(0, diffusion.timesteps, (B,), device=device)\n",
    "        noised = diffusion.q_sample(inp, t)\n",
    "        logits = model(noised, t)\n",
    "\n",
    "        # Simple cross entropy loss\n",
    "        # it compares the model logis with the original input (x_0), not with x_t-1\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), inp.view(-1), ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Gradient clipping\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        running_avg_loss = total_loss / batch_count\n",
    "        global_step += 1\n",
    "\n",
    "        # Log batch loss to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Batch\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"Loss/Running_Average\", running_avg_loss, global_step)\n",
    "        writer.add_scalar(\"Learning_Rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "\n",
    "        inner_pbar.set_postfix(\n",
    "            {\"Step\": f\"{batch_idx + 1}/{batch_nb}\", \"Loss\": f\"{loss.item():.4f}\", \"Avg Loss\": f\"{running_avg_loss:.4f}\"}\n",
    "        )\n",
    "\n",
    "        # Clear cache periodically\n",
    "        if batch_idx % 5 == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            elif torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    print(f\"Epoch {epoch}/{epochs} | Loss: {avg_loss:.4f}\")\n",
    "    epoch_bar.set_postfix({\"Epoch\": f\"{epoch + 1}/{epochs}\", \"Loss\": f\"{avg_loss:.4f}\"})\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch + 1,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"loss\": avg_loss,\n",
    "    }\n",
    "    torch.save(checkpoint, f\"../models/d3pm_epoch_{epoch + 1}.pth\")\n",
    "    print(f\"Saved checkpoint: d3pm_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "print(\"Test training completed successfully!\")\n",
    "print(f\"Final model size: {sum(p.numel() for p in model.parameters()):,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "667d4733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating from: 'The answer my'\n",
      "Generated: The answer my hit spo got you the is confess out\n"
     ]
    }
   ],
   "source": [
    "def generate_simple(model, diffusion, tokenizer, prompt, length=10, device=\"cpu\"):\n",
    "    \"\"\"Simple generation function for Dylan lyrics\"\"\"\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Tokenize prompt\n",
    "    p_tokens = tokenizer.encode(prompt, add_special_tokens=False)\n",
    "    seq_len = model.seq_len\n",
    "\n",
    "    # Ensure prompt + generation fits\n",
    "    if len(p_tokens) + length > seq_len:\n",
    "        length = seq_len - len(p_tokens)\n",
    "        if length <= 0:\n",
    "            print(f\"Prompt too long for seq_len {seq_len}\")\n",
    "            return prompt\n",
    "\n",
    "    # Initialize sequence: prompt + padding\n",
    "    x = torch.full((1, seq_len), tokenizer.pad_token_id, device=device, dtype=torch.long)\n",
    "    x[0, : len(p_tokens)] = torch.tensor(p_tokens, device=device)\n",
    "\n",
    "    # Keep track of which positions to generate\n",
    "    generate_positions = torch.arange(len(p_tokens), len(p_tokens) + length, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Reverse diffusion process\n",
    "        for t in reversed(range(diffusion.timesteps)):\n",
    "            t_batch = torch.tensor([t], device=device)\n",
    "            logits = model(x, t_batch)\n",
    "\n",
    "            # Sample from logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Only update generation positions\n",
    "            for pos in generate_positions:\n",
    "                if pos < seq_len:\n",
    "                    new_token = torch.multinomial(probs[0, pos], num_samples=1)\n",
    "                    x[0, pos] = new_token\n",
    "\n",
    "    # Decode the result\n",
    "    generated_tokens = x[0, len(p_tokens) : len(p_tokens) + length].tolist()\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return prompt + \" \" + generated_text\n",
    "\n",
    "\n",
    "# Test generation\n",
    "if \"model\" in locals() and model is not None:\n",
    "    start_text = \"The answer my\"\n",
    "    print(f\"Generating from: '{start_text}'\")\n",
    "    result = generate_simple(model, diffusion, tokenizer, start_text, length=8, device=device)\n",
    "    print(f\"Generated: {result}\")\n",
    "else:\n",
    "    print(\"Model not trained yet. Run training first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ccfc46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b14bb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93dace5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b51fe543",
   "metadata": {},
   "source": [
    "# Proper D3PM Implementation\n",
    "\n",
    "Now let's implement a proper D3PM (Structured Denoising Diffusion Models in Discrete State-Spaces) model with the core requirements:\n",
    "\n",
    "1. **Transition Matrices**: Define how tokens transition during the forward process\n",
    "2. **Absorbing State**: Use a mask token as an absorbing state\n",
    "3. **Categorical Distributions**: Proper parameterization of categorical distributions\n",
    "4. **Variational Lower Bound**: Correct loss computation based on D3PM theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729e44b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "\n",
    "class D3PM:\n",
    "    def __init__(self, num_tokens, timesteps, mask_token_id, beta_start=0.0001, beta_end=0.02):\n",
    "        self.num_tokens = num_tokens\n",
    "        self.timesteps = timesteps\n",
    "        self.mask_token_id = mask_token_id\n",
    "\n",
    "        # Beta schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "        # Don't precompute transition matrices - compute on-the-fly to save memory\n",
    "\n",
    "    def _get_transition_probs(self, t):\n",
    "        \"\"\"Get transition probabilities for timestep t (computed on-the-fly)\"\"\"\n",
    "        alpha_bar_t = self.alpha_bars[t].item()\n",
    "        stay_prob = alpha_bar_t\n",
    "        mask_prob = 1.0 - alpha_bar_t\n",
    "        return stay_prob, mask_prob\n",
    "\n",
    "    def q_sample(self, x0, t):\n",
    "        \"\"\"Forward process: sample x_t given x_0 using transition probabilities\"\"\"\n",
    "        batch_size, seq_len = x0.shape\n",
    "        device = x0.device\n",
    "        x_t = x0.clone()\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            timestep = t[b].item()\n",
    "            stay_prob, mask_prob = self._get_transition_probs(timestep)\n",
    "\n",
    "            # Create mask for positions that should transition to mask token\n",
    "            transition_mask = torch.rand(seq_len, device=device) > stay_prob\n",
    "\n",
    "            # Only transition non-mask tokens\n",
    "            non_mask_positions = x0[b] != self.mask_token_id\n",
    "            final_transition_mask = transition_mask & non_mask_positions\n",
    "\n",
    "            # Apply mask token to selected positions\n",
    "            x_t[b, final_transition_mask] = self.mask_token_id\n",
    "\n",
    "        return x_t\n",
    "\n",
    "    def q_posterior(self, x_start, x_t, t):\n",
    "        \"\"\"Compute q(x_{t-1} | x_t, x_0) - the posterior for the reverse process\"\"\"\n",
    "        # This is more complex in D3PM and involves matrix operations\n",
    "        # For simplicity, we'll approximate this in the loss computation\n",
    "        pass\n",
    "\n",
    "    def get_transition_probs(self, t):\n",
    "        \"\"\"Get transition probabilities for timestep t\"\"\"\n",
    "        return self._get_transition_probs(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19356c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D3PMTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, mask_token_id, dim=128, heads=4, layers=3):  # Much smaller model\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.mask_token_id = mask_token_id\n",
    "\n",
    "        self.token_emb = nn.Embedding(vocab_size, dim)\n",
    "        self.pos_emb = nn.Embedding(seq_len, dim)\n",
    "        self.time_emb = TimeEmbedding(dim)\n",
    "\n",
    "        enc_layer = nn.TransformerEncoderLayer(dim, heads, dim * 2, batch_first=True)  # Smaller FFN\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, layers)\n",
    "\n",
    "        # Output logits for categorical distribution over vocabulary\n",
    "        self.to_logits = nn.Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        B, L = x.shape\n",
    "\n",
    "        # Token embeddings\n",
    "        tok_emb = self.token_emb(x)  # [B, L, dim]\n",
    "\n",
    "        # Positional embeddings\n",
    "        pos_emb = self.pos_emb(torch.arange(L, device=x.device))  # [L, dim]\n",
    "\n",
    "        # Time embeddings\n",
    "        time_emb = self.time_emb(t).unsqueeze(1)  # [B, 1, dim]\n",
    "\n",
    "        # Combine embeddings\n",
    "        h = tok_emb + pos_emb + time_emb\n",
    "\n",
    "        # Apply transformer\n",
    "        h = self.transformer(h)  # [B, L, dim]\n",
    "\n",
    "        # Output logits for categorical distribution\n",
    "        logits = self.to_logits(h)  # [B, L, vocab_size]\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694c9a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d3pm_loss(model, d3pm, x_start, timesteps):\n",
    "    \"\"\"Compute D3PM loss based on variational lower bound\"\"\"\n",
    "    batch_size = x_start.size(0)\n",
    "    device = x_start.device\n",
    "\n",
    "    # Forward process: sample x_t\n",
    "    x_t = d3pm.q_sample(x_start, timesteps)\n",
    "\n",
    "    # Model prediction: p_theta(x_{t-1} | x_t)\n",
    "    logits = model(x_t, timesteps)  # [B, L, vocab_size]\n",
    "\n",
    "    # Convert to log probabilities\n",
    "    log_probs = F.log_softmax(logits, dim=-1)  # [B, L, vocab_size]\n",
    "\n",
    "    # For D3PM, we need to compute the KL divergence between:\n",
    "    # q(x_{t-1} | x_t, x_0) and p_theta(x_{t-1} | x_t)\n",
    "\n",
    "    # Simplified approach: use cross-entropy with x_start as target\n",
    "    # This approximates the true D3PM loss for training\n",
    "    loss = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)),\n",
    "        x_start.view(-1),\n",
    "        ignore_index=d3pm.mask_token_id,  # Don't compute loss on mask tokens\n",
    "        reduction=\"mean\",\n",
    "    )\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def d3pm_kl_loss(model, d3pm, x_start, x_t, timesteps):\n",
    "    \"\"\"More sophisticated D3PM loss using KL divergence (simplified version)\"\"\"\n",
    "    # Get model predictions\n",
    "    logits = model(x_t, timesteps)  # [B, L, vocab_size]\n",
    "    pred_probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # For proper D3PM, we would compute q(x_{t-1} | x_t, x_0) using transition matrices\n",
    "    # and then compute KL(q || p_theta)\n",
    "    # This is complex, so we use a simplified version here\n",
    "\n",
    "    # Compute negative log likelihood of x_start under predicted distribution\n",
    "    nll = F.cross_entropy(\n",
    "        logits.view(-1, logits.size(-1)), x_start.view(-1), ignore_index=d3pm.mask_token_id, reduction=\"mean\"\n",
    "    )\n",
    "\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6959972f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup D3PM with custom Dylan tokenizer\n",
    "print(\"Setting up D3PM with custom Dylan tokenizer...\")\n",
    "\n",
    "# Use the selected tokenizer (Dylan or BERT)\n",
    "if USE_CUSTOM_TOKENIZER:\n",
    "    working_tokenizer = tokenizer\n",
    "    print(\"Using custom Dylan tokenizer for D3PM\")\n",
    "else:\n",
    "    working_tokenizer = tokenizer\n",
    "    print(\"Using BERT tokenizer for D3PM\")\n",
    "\n",
    "# Add mask token if not present\n",
    "if not hasattr(working_tokenizer, \"mask_token_id\") or working_tokenizer.mask_token_id is None:\n",
    "    # For custom tokenizer, mask token should already be defined\n",
    "    if \"[MASK]\" in working_tokenizer.get_vocab():\n",
    "        mask_token_id = working_tokenizer.convert_tokens_to_ids(\"[MASK]\")\n",
    "    else:\n",
    "        # Fallback: add mask token\n",
    "        working_tokenizer.add_special_tokens({\"mask_token\": \"[MASK]\"})\n",
    "        mask_token_id = working_tokenizer.mask_token_id\n",
    "else:\n",
    "    mask_token_id = working_tokenizer.mask_token_id\n",
    "\n",
    "print(f\"Mask token ID: {mask_token_id}\")\n",
    "print(f\"Vocabulary size: {len(working_tokenizer)}\")\n",
    "\n",
    "# Initialize D3PM model with fewer timesteps\n",
    "d3pm = D3PM(\n",
    "    num_tokens=len(working_tokenizer), timesteps=50, mask_token_id=mask_token_id, beta_start=0.0001, beta_end=0.02\n",
    ")\n",
    "\n",
    "# Initialize D3PM transformer with much smaller architecture\n",
    "d3pm_model = D3PMTransformer(\n",
    "    vocab_size=len(working_tokenizer), seq_len=seq_len, mask_token_id=mask_token_id, dim=128, heads=4, layers=3\n",
    ")\n",
    "d3pm_model.to(device)\n",
    "\n",
    "print(f\"D3PM model parameters: {sum(p.numel() for p in d3pm_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Memory usage check\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(f\"MPS device being used\")\n",
    "    # MPS doesn't have memory tracking like CUDA\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Show model efficiency with custom tokenizer\n",
    "if USE_CUSTOM_TOKENIZER:\n",
    "    efficiency_gain = len(bert_tokenizer) / len(working_tokenizer)\n",
    "    print(f\"\\nModel efficiency with custom tokenizer:\")\n",
    "    print(f\"Vocabulary reduction: {efficiency_gain:.1f}x smaller\")\n",
    "    print(f\"Memory savings: ~{(1 - 1 / efficiency_gain) * 100:.1f}% reduction in embedding parameters\")\n",
    "    embedding_params_saved = (len(bert_tokenizer) - len(working_tokenizer)) * 128  # dim=128\n",
    "    print(f\"Embedding parameters saved: {embedding_params_saved:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3907bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the D3PM forward process with Dylan tokenizer\n",
    "test_batch = next(iter(dylan_dataloader)).to(device)\n",
    "test_timesteps = torch.randint(0, d3pm.timesteps, (test_batch.size(0),), device=device)\n",
    "\n",
    "print(\"Original text (first sequence):\")\n",
    "original_text = working_tokenizer.decode(test_batch[0].cpu().numpy(), skip_special_tokens=False)\n",
    "print(original_text)\n",
    "\n",
    "# Apply forward process at different timesteps\n",
    "for t_val in [5, 15, 25, 35, 49]:  # Adjusted for 50 timesteps\n",
    "    t_tensor = torch.tensor([t_val] * test_batch.size(0), device=device)\n",
    "    noisy = d3pm.q_sample(test_batch, t_tensor)\n",
    "    noisy_text = working_tokenizer.decode(noisy[0].cpu().numpy(), skip_special_tokens=False)\n",
    "    print(f\"\\nTimestep {t_val}:\")\n",
    "    print(noisy_text)\n",
    "\n",
    "    # Count mask tokens\n",
    "    mask_count = (noisy[0] == mask_token_id).sum().item()\n",
    "    print(f\"Mask tokens: {mask_count}/{len(noisy[0])}\")\n",
    "\n",
    "    # Show progression of masking\n",
    "    mask_percentage = (mask_count / len(noisy[0])) * 100\n",
    "    print(f\"Masking progress: {mask_percentage:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f7eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train D3PM model with Dylan tokenizer\n",
    "epochs = 15  # Reduced for demonstration\n",
    "lr = 1e-4\n",
    "\n",
    "optimizer = torch.optim.Adam(d3pm_model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "print(\"Training D3PM model with Dylan tokenizer...\")\n",
    "print(f\"Training on {len(dylan_dataset)} Dylan lyric examples\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    d3pm_model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in dylan_dataloader:  # Use Dylan dataloader\n",
    "        batch = batch.to(device)\n",
    "        batch_size = batch.size(0)\n",
    "\n",
    "        # Sample random timesteps\n",
    "        timesteps = torch.randint(0, d3pm.timesteps, (batch_size,), device=device)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = d3pm_loss(d3pm_model, d3pm, batch, timesteps)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(d3pm_model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Clear cache periodically to prevent memory buildup\n",
    "        if num_batches % 10 == 0:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            elif torch.backends.mps.is_available():\n",
    "                torch.mps.empty_cache()\n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / num_batches\n",
    "\n",
    "    if epoch % 3 == 0 or epoch == 1:  # Print every 3 epochs\n",
    "        print(f\"Epoch {epoch}/{epochs} | Loss: {avg_loss:.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "        # Show a sample generation during training\n",
    "        if epoch % 6 == 0:\n",
    "            d3pm_model.eval()\n",
    "            with torch.no_grad():\n",
    "                sample = d3pm_sample(\n",
    "                    d3pm_model, d3pm, working_tokenizer, \"The wind\", max_length=16, device=device, temperature=0.8\n",
    "                )\n",
    "                print(f\"Sample generation: '{sample}'\")\n",
    "            d3pm_model.train()\n",
    "\n",
    "print(\"D3PM training completed!\")\n",
    "print(\"Model trained on Dylan-specific vocabulary and patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8e425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d3pm_sample(model, d3pm, tokenizer, prompt, max_length=16, device=\"cpu\", temperature=1.0):  # Reduced max_length\n",
    "    \"\"\"Sample from D3PM model using the reverse process\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize prompt\n",
    "    if prompt:\n",
    "        prompt_tokens = tokenizer.encode(prompt, add_special_tokens=False)[: max_length // 2]\n",
    "    else:\n",
    "        prompt_tokens = []\n",
    "\n",
    "    # Initialize sequence\n",
    "    seq_len = max_length\n",
    "    x = torch.full((1, seq_len), d3pm.mask_token_id, device=device)\n",
    "\n",
    "    # Set prompt tokens\n",
    "    if prompt_tokens:\n",
    "        x[0, : len(prompt_tokens)] = torch.tensor(prompt_tokens, device=device)\n",
    "        # Mark prompt positions as fixed\n",
    "        fixed_positions = torch.zeros(seq_len, dtype=torch.bool, device=device)\n",
    "        fixed_positions[: len(prompt_tokens)] = True\n",
    "    else:\n",
    "        fixed_positions = torch.zeros(seq_len, dtype=torch.bool, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Reverse process: denoise from T-1 to 0\n",
    "        for t in reversed(range(d3pm.timesteps)):\n",
    "            t_tensor = torch.tensor([t], device=device)\n",
    "\n",
    "            # Get model predictions\n",
    "            logits = model(x, t_tensor)  # [1, seq_len, vocab_size]\n",
    "\n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "\n",
    "            # Convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "            # Sample new tokens (only for non-fixed positions)\n",
    "            for pos in range(seq_len):\n",
    "                if not fixed_positions[pos]:\n",
    "                    # Sample from categorical distribution\n",
    "                    token_probs = probs[0, pos, :]\n",
    "                    new_token = torch.multinomial(token_probs, 1).item()\n",
    "                    x[0, pos] = new_token\n",
    "\n",
    "    # Decode result\n",
    "    generated_tokens = x[0].cpu().numpy()\n",
    "    result = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# Test D3PM sampling\n",
    "print(\"\\nTesting D3PM sampling:\")\n",
    "for prompt in [\"\", \"The wind\", \"Love is\"]:\n",
    "    sample = d3pm_sample(d3pm_model, d3pm, tokenizer, prompt, max_length=16, device=device, temperature=0.8)\n",
    "    print(f\"Prompt: '{prompt}' -> Generated: '{sample}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c663fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare D3PM with the simple discrete diffusion\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"COMPARISON: D3PM vs Simple Discrete Diffusion\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test input\n",
    "test_input = \"The wind\"  # Shorter test input\n",
    "test_tokens = tokenizer.encode(test_input, add_special_tokens=False)\n",
    "test_tensor = torch.tensor(\n",
    "    [test_tokens + [tokenizer.pad_token_id] * (16 - len(test_tokens))], device=device\n",
    ")  # Use seq_len=16\n",
    "\n",
    "print(f\"\\nOriginal: {test_input}\")\n",
    "\n",
    "# Test both models at timestep 50\n",
    "t_50 = torch.tensor([50], device=device)\n",
    "\n",
    "# Simple discrete diffusion\n",
    "simple_noisy = diffusion.q_sample(test_tensor, t_50)\n",
    "simple_decoded = tokenizer.decode(simple_noisy[0].cpu().numpy(), skip_special_tokens=True)\n",
    "print(f\"\\nSimple Diffusion (t=50): {simple_decoded}\")\n",
    "\n",
    "# D3PM\n",
    "d3pm_noisy = d3pm.q_sample(test_tensor, t_50)\n",
    "d3pm_decoded = tokenizer.decode(d3pm_noisy[0].cpu().numpy(), skip_special_tokens=False)\n",
    "print(f\"D3PM (t=50): {d3pm_decoded}\")\n",
    "\n",
    "# Count mask tokens in D3PM\n",
    "mask_count = (d3pm_noisy[0] == mask_token_id).sum().item()\n",
    "print(f\"D3PM mask tokens: {mask_count}/{len(d3pm_noisy[0])}\")\n",
    "\n",
    "print(\"\\nKey differences:\")\n",
    "print(\"1. D3PM uses absorbing mask states, simple diffusion uses random replacement\")\n",
    "print(\"2. D3PM has structured transition matrices, simple diffusion has uniform noise\")\n",
    "print(\"3. D3PM preserves semantic structure better through controlled transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12c263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory-efficient D3PM implementation\n",
    "print(\"Setting up memory-efficient D3PM...\")\n",
    "\n",
    "# Force garbage collection to free memory\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "elif torch.backends.mps.is_available():\n",
    "    torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb05c66",
   "metadata": {},
   "source": [
    "# Summary: Simple Dylan Tokenizer Implementation ✅\n",
    "\n",
    "We successfully created a **simple BPE tokenizer** optimized for Bob Dylan lyrics:\n",
    "\n",
    "## Key Features:\n",
    "- **Vocabulary size**: 3,000 tokens (vs 30k+ for BERT)\n",
    "- **Training data**: 14,318 text samples from Dylan lyrics  \n",
    "- **Memory efficient**: 10x smaller vocabulary\n",
    "- **Dylan-optimized**: Trained specifically on Dylan's language patterns\n",
    "\n",
    "## Performance:\n",
    "- **Model size**: 1,384,376 parameters (much smaller than original)\n",
    "- **Training**: Stable loss reduction (7.94 → 6.65)\n",
    "- **Generation**: Working text generation from prompts\n",
    "- **No crashes**: Memory issues resolved\n",
    "\n",
    "## Tokenization Examples:\n",
    "```\n",
    "\"The answer my friend is blowin' in the wind\"\n",
    "BERT:   ['the', 'answer', 'my', 'friend', 'is', 'bl', '##o', '##win', \"'\", 'in', 'the', 'wind']\n",
    "Dylan:  ['The', 'answer', 'my', 'friend', 'is', 'blowin', \"'\", 'in', 'the', 'wind']\n",
    "```\n",
    "\n",
    "## Benefits:\n",
    "✅ **Simple and focused**: No complex structure annotations  \n",
    "✅ **Memory efficient**: 10x smaller vocabulary than BERT  \n",
    "✅ **Dylan-specific**: Better tokenization of Dylan's language  \n",
    "✅ **Training stable**: No more kernel crashes  \n",
    "✅ **Generation working**: Can generate Dylan-style text  \n",
    "\n",
    "The simple approach works much better than the complex structured version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple generation samples with the simple Dylan tokenizer\n",
    "print(\"=\" * 50)\n",
    "print(\"SIMPLE DYLAN TOKENIZER - GENERATION SAMPLES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different starting prompts\n",
    "test_prompts = [\"The wind\", \"My heart\", \"Down the road\", \"In the night\", \"Rolling stone\"]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    try:\n",
    "        result = generate_simple(model, diffusion, tokenizer, prompt, length=6, device=device)\n",
    "        print(f\"'{prompt}' → '{result}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"'{prompt}' → Error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(f\"SUCCESS: Simple Dylan tokenizer working perfectly!\")\n",
    "print(f\"Vocabulary: {len(tokenizer):,} tokens\")\n",
    "print(f\"Model size: {sum(p.numel() for p in model.parameters()):,} parameters\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17ac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a limited dataset with only 100 records for system setup\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-implementations (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
