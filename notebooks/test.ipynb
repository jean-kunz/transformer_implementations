{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "g = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DumbDataset(Dataset):\n",
    "    \"\"\" \n",
    "    Dumb dataset that generates random x and sorted y\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, length=6, num_digits=3):\n",
    "        self.length = length\n",
    "        self.num_digits = num_digits\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 10000 # ...\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp = torch.randint(self.num_digits, size=(self.length,), dtype=torch.long)\n",
    "        # solve the task: i.e. sort\n",
    "        sol = torch.sort(inp)[0]\n",
    "        \n",
    "        return inp, sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0\n",
      "0 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "# print an example instance of the dataset\n",
    "train_ds = DumbDataset()\n",
    "x, y = train_ds[0]\n",
    "for a, b in zip(x,y):\n",
    "    print(int(a),int(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, model_size:int, nb_heads: int=1, dropout: float=0., bias:bool=True, mlp_factor=4 ) -> None:\n",
    "        super().__init__()\n",
    "        self.layer_norm1 = nn.LayerNorm(model_size, device=device)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=model_size, num_heads=nb_heads, \n",
    "                                          dropout=dropout, bias=bias, batch_first=True, device=device )\n",
    "        \n",
    "        self.mlp1 = nn.Linear(model_size, mlp_factor * model_size, bias=True, device=device)\n",
    "        self.mlp2 = nn.Linear(mlp_factor * model_size, model_size, bias=True, device=device)\n",
    "        self.activation = torch.nn.GELU()\n",
    "        self.layer_norm2 = nn.LayerNorm(model_size, device=device)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b,l,d = x.size()\n",
    "        norm_x = self.layer_norm1(x)\n",
    "        #mask = unidirectional_mask(seq_len=l)\n",
    "        attn_x, attn_w = self.attn(norm_x, norm_x, norm_x)\n",
    "        attn_x = x * attn_x\n",
    "        \n",
    "        norm_attn_x = self.layer_norm2(attn_x)\n",
    "        lin1 = self.activation(self.mlp1(norm_attn_x))\n",
    "        x = x + self.dropout(self.mlp2(lin1))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyGPT(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size: int, seq_len: int, model_size: int = 20, \n",
    "                 nb_layers:int=1, nb_heads:int =1, dropout: float=0., device=torch.device('cpu')) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.model_size = model_size\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "        self.tok_emb = nn.Embedding(vocab_size, embedding_dim=model_size, device=device)\n",
    "        pos = torch.arange(0, seq_len, dtype=torch.int).unsqueeze(0) # shape (1, max_seq_len)\n",
    "        self.register_buffer('pos', pos)\n",
    "        self.pos_emb = nn.Embedding(seq_len, model_size, device=device)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(model_size=model_size, nb_heads=nb_heads, dropout=dropout, bias=True) for i in range(nb_layers)])\n",
    "       \n",
    "        self.lin = nn.Linear(model_size, vocab_size, device=device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l = x.size()\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        #pos = torch.arange(0, self.seq_len, dtype=torch.int).unsqueeze(0)\n",
    "        \n",
    "        x = tok_emb + self.pos_emb(self.pos)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        logits = self.lin(x)\n",
    "        \n",
    "        return logits \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "class Trainer:\n",
    "    \n",
    "    def __init__(self, model, optimizer, device=torch.device('cpu'), max_iter=10):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.iter_num = 0\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    \n",
    "    def run(self, train_ds):\n",
    "        model= self.model\n",
    "        # setup the optimizer\n",
    "        \n",
    "\n",
    "        # setup the dataloader\n",
    "        train_loader = DataLoader(train_ds)\n",
    "        model.train()\n",
    "        data_iter = iter(train_loader)\n",
    "        while True:\n",
    "\n",
    "            # fetch the next batch (x, y) and re-init iterator if needed\n",
    "            try:\n",
    "                batch = next(data_iter)\n",
    "            except StopIteration:\n",
    "                data_iter = iter(train_loader)\n",
    "                batch = next(data_iter)\n",
    "            batch = [t.to(self.device) for t in batch]\n",
    "            x, y = batch\n",
    "\n",
    "            # forward the model\n",
    "            model.train()\n",
    "            logits = model(x)\n",
    "            #loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            loss = nn.CrossEntropyLoss()(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "            # backprop and update the parameters\n",
    "            model.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.iter_num += 1\n",
    "            print(self.iter_num)\n",
    "\n",
    "            # termination conditions\n",
    "            if self.max_iter is not None and self.iter_num >= self.max_iter:\n",
    "                break\n",
    "            \n",
    "vocab_size = 3\n",
    "seq_len = 11\n",
    "model_size = 20\n",
    "model = MyGPT(vocab_size, seq_len, model_size, nb_layers=1, nb_heads=1, device=device)            \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "trainer = Trainer(model, optimizer=optimizer, device=device, max_iter=5)\n",
    "train_ds = DumbDataset(length=seq_len, num_digits=vocab_size)\n",
    "trainer.run(train_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "02e373710fe8ee5e4fefe94e05d12897bb1dce14bb31cd26162c8283164a7cc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
