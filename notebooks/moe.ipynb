{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "# | default_exp moe\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecream import ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of experts\n",
    "\n",
    "https://huggingface.co/blog/AviSoori1x/makemoe-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# Expert module\n",
    "class Expert(nn.Module):\n",
    "    \"\"\"An MLP is a simple linear layer followed by a non-linearity i.e. each Expert\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-1.8681,  0.0758, -0.5876, -0.1003],\n",
      "         [ 0.5114,  0.3268,  0.2343,  0.0684],\n",
      "         [-0.4361,  0.2250,  0.1455, -0.2001],\n",
      "         [ 0.4120,  0.1028,  0.6582,  0.0750]],\n",
      "\n",
      "        [[-0.3097,  0.1241,  0.3161, -0.2436],\n",
      "         [ 0.4127, -0.6060, -0.9747,  0.1802],\n",
      "         [-0.3331, -0.0450,  0.3510,  0.2207],\n",
      "         [ 1.0282, -1.1125, -0.4474, -0.0438]]], grad_fn=<ViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('top k logits:',\n",
       " tensor([[[ 0.0758, -0.1003],\n",
       "          [ 0.5114,  0.3268],\n",
       "          [ 0.2250,  0.1455],\n",
       "          [ 0.6582,  0.4120]],\n",
       " \n",
       "         [[ 0.3161,  0.1241],\n",
       "          [ 0.4127,  0.1802],\n",
       "          [ 0.3510,  0.2207],\n",
       "          [ 1.0282, -0.0438]]], grad_fn=<TopkBackward0>),\n",
       " 'top k indices:',\n",
       " tensor([[[1, 3],\n",
       "          [0, 1],\n",
       "          [1, 2],\n",
       "          [2, 0]],\n",
       " \n",
       "         [[2, 1],\n",
       "          [0, 3],\n",
       "          [2, 3],\n",
       "          [0, 3]]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Understanding how gating/router works\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embed = 32\n",
    "\n",
    "\n",
    "# Example multi-head attention output for a simple illustrative example, consider n_embed=32, context_length=4 and batch_size=2\n",
    "mh_output = torch.randn(2, 4, n_embed)\n",
    "\n",
    "topkgate_linear = nn.Linear(n_embed, num_experts)  # nn.Linear(32, 4)\n",
    "\n",
    "logits = topkgate_linear(mh_output)\n",
    "top_k_logits, top_k_indices = logits.topk(top_k, dim=-1)  # Get top-k experts\n",
    "print(logits)\n",
    "\"top k logits:\", top_k_logits, \"top k indices:\", top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[   -inf,  0.0758,    -inf, -0.1003],\n",
       "          [ 0.5114,  0.3268,    -inf,    -inf],\n",
       "          [   -inf,  0.2250,  0.1455,    -inf],\n",
       "          [ 0.4120,    -inf,  0.6582,    -inf]],\n",
       " \n",
       "         [[   -inf,  0.1241,  0.3161,    -inf],\n",
       "          [ 0.4127,    -inf,    -inf,  0.1802],\n",
       "          [   -inf,    -inf,  0.3510,  0.2207],\n",
       "          [ 1.0282,    -inf,    -inf, -0.0438]]], grad_fn=<ScatterBackward0>),\n",
       " tensor([[[0.0000, 0.5439, 0.0000, 0.4561],\n",
       "          [0.5460, 0.4540, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5199, 0.4801, 0.0000],\n",
       "          [0.4387, 0.0000, 0.5613, 0.0000]],\n",
       " \n",
       "         [[0.0000, 0.4521, 0.5479, 0.0000],\n",
       "          [0.5579, 0.0000, 0.0000, 0.4421],\n",
       "          [0.0000, 0.0000, 0.5325, 0.4675],\n",
       "          [0.7450, 0.0000, 0.0000, 0.2550]]], grad_fn=<SoftmaxBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep the top-k experts and set the rest to -inf\n",
    "zeros = torch.full_like(\n",
    "    logits, float(\"-inf\")\n",
    ")  # full_like clones a tensor and fills it with a specified value (like infinity) for masking or calculations.\n",
    "sparse_logits = zeros.scatter(-1, top_k_indices, top_k_logits)\n",
    "# transform the logits into a probability distribution\n",
    "gating_output = F.softmax(sparse_logits, dim=-1)\n",
    "sparse_logits, gating_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# First define the top k router module\n",
    "class TopkRouter(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(TopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        self.linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_ouput):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.linear(mh_output)\n",
    "        top_k_logits, indices = logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(logits, float(\"-inf\"))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 4]),\n",
       " tensor([[[0.5215, 0.4785, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5903, 0.4097, 0.0000],\n",
       "          [0.4191, 0.0000, 0.5809, 0.0000],\n",
       "          [0.5085, 0.4915, 0.0000, 0.0000]],\n",
       " \n",
       "         [[0.5912, 0.4088, 0.0000, 0.0000],\n",
       "          [0.6839, 0.3161, 0.0000, 0.0000],\n",
       "          [0.6206, 0.3794, 0.0000, 0.0000],\n",
       "          [0.0000, 0.5278, 0.4722, 0.0000]]], grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[0, 1],\n",
       "          [1, 2],\n",
       "          [2, 0],\n",
       "          [0, 1]],\n",
       " \n",
       "         [[0, 1],\n",
       "          [0, 1],\n",
       "          [0, 1],\n",
       "          [1, 2]]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing this out:\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 32\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "top_k_gate = TopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices\n",
    "# And it works!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  0.2000,   2.3000,  10.0000,  -0.1000,  -3.2000, -10.0000]),\n",
       " array([ 0.7981,  2.3955, 10.    ,  0.6444,  0.04  ,  0.    ],\n",
       "       dtype=float32),\n",
       " array([-5.3660e-01,  5.3200e-01,  4.0517e+00, -7.4320e-01,  1.0960e-01,\n",
       "        -1.0000e-04], dtype=float32))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softplus is a smoothed version of RELU function.\n",
    "input = torch.tensor([0.2, 2.3, 10.0, -0.1, -3.2, -10.0])\n",
    "sp = F.softplus(input)\n",
    "\n",
    "rand_noise = torch.randn_like(input)\n",
    "out = rand_noise * F.softplus(input)\n",
    "(\n",
    "    input,\n",
    "    sp.numpy().round(4),\n",
    "    out.numpy().round(4),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "# Changing the above to accomodate noisy top-k gating\n",
    "class NoisyTopkRouter(nn.Module):\n",
    "    \"\"\"Essentially, you don't want all the tokens to be sent to the same set of 'favored' experts.\n",
    "    You want a fine balance of exploitation and exploration. For this purpose, to load balance,\n",
    "    it is helpful to add standard normal noise to the logits from the gating linear layer.\n",
    "    This makes training more efficient\"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(NoisyTopkRouter, self).__init__()\n",
    "        self.top_k = top_k\n",
    "        # layer for router logits\n",
    "        self.topkroute_linear = nn.Linear(n_embed, num_experts)\n",
    "        self.noise_linear = nn.Linear(n_embed, num_experts)\n",
    "\n",
    "    def forward(self, mh_output):\n",
    "        # mh_ouput is the output tensor from multihead self attention block\n",
    "        logits = self.topkroute_linear(mh_output)\n",
    "\n",
    "        # Noise logits\n",
    "        noise_logits = self.noise_linear(mh_output)\n",
    "\n",
    "        # Adding scaled unit gaussian noise to the logits\n",
    "        # softplus ensures that the noise is always positive and right skewed\n",
    "        noise = torch.randn_like(logits) * F.softplus(noise_logits)\n",
    "        # noisy logit add noise to the logits so some tokens are sent to different experts and not just the top-k.\n",
    "        # It pushes the model to explore more.\n",
    "        noisy_logits = logits + noise\n",
    "\n",
    "        top_k_logits, indices = noisy_logits.topk(self.top_k, dim=-1)\n",
    "        zeros = torch.full_like(noisy_logits, float(\"-inf\"))\n",
    "        sparse_logits = zeros.scatter(-1, indices, top_k_logits)\n",
    "        router_output = F.softmax(sparse_logits, dim=-1)\n",
    "        return router_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 8]),\n",
       " tensor([[[0.0000, 0.0000, 0.6716, 0.3284, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4136, 0.0000, 0.0000, 0.5864, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.5642, 0.0000, 0.0000, 0.0000, 0.0000, 0.4358, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4763, 0.5237]],\n",
       " \n",
       "         [[0.2149, 0.0000, 0.0000, 0.0000, 0.7851, 0.0000, 0.0000, 0.0000],\n",
       "          [0.4339, 0.0000, 0.5661, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "          [0.6298, 0.0000, 0.0000, 0.0000, 0.3702, 0.0000, 0.0000, 0.0000],\n",
       "          [0.0000, 0.0000, 0.0000, 0.0000, 0.5971, 0.0000, 0.4029, 0.0000]]],\n",
       "        grad_fn=<SoftmaxBackward0>),\n",
       " tensor([[[2, 3],\n",
       "          [3, 0],\n",
       "          [0, 5],\n",
       "          [7, 6]],\n",
       " \n",
       "         [[4, 0],\n",
       "          [2, 0],\n",
       "          [0, 4],\n",
       "          [4, 6]]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_experts = 8\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "\n",
    "mh_output = torch.randn(2, 4, n_embd)  # Example input\n",
    "noisy_top_k_gate = NoisyTopkRouter(n_embd, num_experts, top_k)\n",
    "gating_output, indices = noisy_top_k_gate(mh_output)\n",
    "gating_output.shape, gating_output, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, n_embed, num_experts, top_k):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.router = NoisyTopkRouter(n_embed, num_experts, top_k)\n",
    "        self.experts = nn.ModuleList([Expert(n_embed) for _ in range(num_experts)])\n",
    "        self.top_k = top_k\n",
    "\n",
    "    def forward(self, x):\n",
    "        gating_output, indices = self.router(x)\n",
    "        ic(gating_output)\n",
    "        final_output = torch.zeros_like(x)\n",
    "\n",
    "        # Reshape inputs for batch processing\n",
    "        flat_x = x.view(-1, x.size(-1))\n",
    "        flat_gating_output = gating_output.view(-1, gating_output.size(-1))\n",
    "\n",
    "        # Process each expert in parallel\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Create a mask for the inputs where the current expert is in top-k\n",
    "            expert_mask = (indices == i).any(dim=-1)\n",
    "            ic(expert_mask)\n",
    "            flat_mask = expert_mask.view(-1)\n",
    "\n",
    "            if flat_mask.any():\n",
    "                expert_input = flat_x[flat_mask]\n",
    "                expert_output = expert(expert_input)\n",
    "\n",
    "                # Extract and apply gating scores\n",
    "                gating_scores = flat_gating_output[flat_mask, i].unsqueeze(1)\n",
    "                weighted_output = expert_output * gating_scores\n",
    "\n",
    "                # Update final output additively by indexing and adding\n",
    "                final_output[expert_mask] += weighted_output.squeeze(1)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| gating_output: tensor([[[0.5464, 0.4536, 0.0000, 0.0000],\n",
      "                            [0.6865, 0.0000, 0.3135, 0.0000],\n",
      "                            [0.5082, 0.0000, 0.0000, 0.4918],\n",
      "                            [0.0000, 0.6408, 0.0000, 0.3592],\n",
      "                            [0.0000, 0.4666, 0.0000, 0.5334],\n",
      "                            [0.7807, 0.2193, 0.0000, 0.0000],\n",
      "                            [0.6676, 0.0000, 0.3324, 0.0000],\n",
      "                            [0.7278, 0.0000, 0.2722, 0.0000]]], grad_fn=<SoftmaxBackward0>)\n",
      "ic| expert_mask: tensor([[ True,  True,  True, False, False,  True,  True,  True]])\n",
      "ic| expert_mask: tensor([[ True, False, False,  True,  True,  True, False, False]])\n",
      "ic| expert_mask: tensor([[False,  True, False, False, False, False,  True,  True]])\n",
      "ic| expert_mask: tensor([[False, False,  True,  True,  True, False, False, False]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the final output: torch.Size([1, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "ic.enable()\n",
    "# ic.disable()\n",
    "\n",
    "# Let's test this out\n",
    "num_experts = 4\n",
    "top_k = 2\n",
    "n_embd = 16\n",
    "dropout = 0.1\n",
    "\n",
    "mh_output = torch.randn(1, 8, n_embd)  # Example multi-head attention output\n",
    "sparse_moe = SparseMoE(n_embd, num_experts, top_k)\n",
    "final_output = sparse_moe(mh_output)\n",
    "print(\"Shape of the final output:\", final_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-implementations-urBBcPaT-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
