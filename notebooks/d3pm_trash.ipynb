{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b550c93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9ccf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy dataset: simple sentences\n",
    "# sentences = [\"the cat sits\", \"a dog runs\", \"birds fly high\", \"fish swim fast\", \"the sun shines\"]\n",
    "\n",
    "# Build a simple vocabulary\n",
    "words = set(\" \".join(lines).split())\n",
    "word_to_idx = {word: idx + 1 for idx, word in enumerate(words)}  # +1 for padding\n",
    "word_to_idx[\"<pad>\"] = 0\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "vocab_size = len(word_to_idx)\n",
    "max_len = max(len(s.split()) for s in lines)\n",
    "\n",
    "\n",
    "# Convert sentences to token sequences\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = [word_to_idx[word] for word in sentence.split()]\n",
    "    return tokens + [0] * (max_len - len(tokens))  # Pad to max_len\n",
    "\n",
    "\n",
    "tokenized_data = [tokenize_sentence(s) for s in lines]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a14e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 13)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array(tokenized_data).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d45c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.tensor(data, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "# lets overfit on the first 100 sentences\n",
    "dataset = TextDataset(tokenized_data)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9f011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| tokenized_data[:2]: [[216, 106, 50, 263, 306, 66, 0, 0, 0, 0, 0, 0, 0],\n",
      "                         [100, 349, 236, 292, 349, 258, 108, 92, 310, 53, 0, 0, 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[216, 106,  50, 263, 306,  66,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [ 67, 311, 236,  24, 349, 258, 273,  92, 310,  53,   0, 340,   0]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# D3PM (Discrete Denoising Diffusion Probabilistic Model)\n",
    "class D3PM:\n",
    "    def __init__(self, num_steps=50, vocab_size=vocab_size):\n",
    "        self.num_steps = num_steps\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.betas = torch.linspace(0.0001, 0.02, num_steps)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "\n",
    "    def q_sample(self, x0, t):\n",
    "        # Get batch size and shape\n",
    "        device = x0.device\n",
    "        b = x0.shape[0]\n",
    "        x0_flat = x0.view(b, -1)\n",
    "        # At t=0, no noise: x_t = x0\n",
    "        xt = x0_flat.clone()\n",
    "        for i in range(b):\n",
    "            step = t[i].item()\n",
    "            if step == 0:\n",
    "                continue\n",
    "            # For step > 0, corrupt x0\n",
    "            mask = torch.rand_like(x0_flat[i].float()) < (1 - self.alphas_cumprod[step])\n",
    "            noise = torch.randint(0, self.vocab_size, x0_flat[i].shape, device=device)\n",
    "            xt[i][mask] = noise[mask]\n",
    "        return xt.view_as(x0)\n",
    "\n",
    "    def sample(self, model, batch_size, device, context=None, guidance_scale=1.0):\n",
    "        x_t = torch.randint(0, self.vocab_size, (batch_size, max_len), device=device)\n",
    "        if context is not None:\n",
    "            # Repeat context to match batch size\n",
    "            context = context.repeat(batch_size, 1)  # Shape: [batch_size, context_len]\n",
    "            x_t[:, : context.shape[1]] = context  # Fix context tokens\n",
    "        for t in reversed(range(self.num_steps)):\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            logits_uncond = model(x_t, t_tensor, context=None)\n",
    "            logits_cond = model(x_t, t_tensor, context=context)\n",
    "            logits = logits_uncond + guidance_scale * (logits_cond - logits_uncond)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            x_t = torch.multinomial(probs.view(-1, self.vocab_size), 1).view(batch_size, max_len)\n",
    "            if context is not None:\n",
    "                x_t[:, : context.shape[1]] = context  # Preserve context\n",
    "        return x_t\n",
    "\n",
    "\n",
    "diffusion = D3PM(num_steps=50, vocab_size=vocab_size)\n",
    "ic(tokenized_data[:2])\n",
    "diffusion.q_sample(torch.tensor(tokenized_data[:2], dtype=torch.long), torch.tensor([0, 49], dtype=torch.long))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041cb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# NanoGPT-like Model with Context\n",
    "class NanoGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, n_embd=64, n_head=4, n_layer=2, max_len=max_len, num_steps=50):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding = nn.Embedding(max_len, n_embd)\n",
    "        self.context_embedding = nn.Embedding(vocab_size, n_embd)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                nn.TransformerEncoderLayer(d_model=n_embd, nhead=n_head, dim_feedforward=n_embd * 4)\n",
    "                for _ in range(n_layer)\n",
    "            ]\n",
    "        )\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.head = nn.Linear(n_embd, vocab_size)\n",
    "        self.max_len = max_len\n",
    "        self.time_embedding = nn.Embedding(num_steps, n_embd)\n",
    "\n",
    "    def forward(self, x, t, context=None):\n",
    "        B, T = x.shape\n",
    "        device = x.device\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=device))\n",
    "        t_emb = self.time_embedding(t).unsqueeze(1)\n",
    "        x = tok_emb + pos_emb + t_emb\n",
    "        if context is not None:\n",
    "            ctx_emb = self.context_embedding(context)\n",
    "            ctx_pos_emb = self.position_embedding(torch.arange(context.shape[1], device=device))\n",
    "            ctx = ctx_emb + ctx_pos_emb\n",
    "            x = torch.cat([ctx, x], dim=1)\n",
    "            x = x[:, :T] + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Check for MPS availability\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9826e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 50\n",
    "model = NanoGPT(vocab_size=vocab_size, num_steps=num_steps).to(device)\n",
    "diffusion = D3PM(num_steps=num_steps, vocab_size=vocab_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e675073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard logs will be saved to: ../runs/d3pm/29-05-2025_17:10:50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7c6dfc96c144c8a406031a33d371b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "üöÄ Training:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a5ca97ebf74c0a899ff01eb1b7a885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 1:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf8864f4577416ba39ae6436b62f48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 2:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8417f655e24b798908603a8e13cc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 3:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0873c86246bc4031a8f738dca5ec18a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 4:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985804428e714eb5b3281d18ecef2136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 5:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fed206055be64091b5d7786baa28c6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 6:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cb848273cb40c0901798625104f771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 7:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25787c123ea24b6a955ffe00ec161c1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 8:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e80b0d6f8a64b6c84fcca653d49b9aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 9:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49298beee30047d8b709b143dba6c74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ‚öôÔ∏è Inner Task 10:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training completed!\n",
      "View logs with: tensorboard --logdir=../runs/d3pm/29-05-2025_17:10:50\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "\n",
    "num_epochs = 10\n",
    "dropout_prob = 0.1  # Probability of dropping context for classifier-free guidance\n",
    "global_step = 0  # Track global step for TensorBoard\n",
    "\n",
    "# Set up TensorBoard logging\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y_%H:%M:%S\")\n",
    "log_dir = f\"../runs/d3pm/{timestamp}\"\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "print(f\"TensorBoard logs will be saved to: {log_dir}\")\n",
    "\n",
    "epoch_bar = tqdm(range(num_epochs), desc=\"üöÄ Training\", position=0, leave=True)\n",
    "for epoch in epoch_bar:\n",
    "    total_loss = 0\n",
    "    running_avg_loss = 0\n",
    "    batch_count = 0\n",
    "\n",
    "    batch_nb = len(dataloader)\n",
    "    inner_pbar = tqdm(range(batch_nb), desc=f\"  ‚öôÔ∏è Inner Task {epoch + 1}\", position=1, leave=False, colour=\"green\")\n",
    "\n",
    "    for batch_idx in inner_pbar:\n",
    "        batch = next(iter(dataloader))\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        t = torch.randint(0, num_steps, (batch.shape[0],), device=device, dtype=torch.long)\n",
    "        x_t = diffusion.q_sample(batch, t)\n",
    "        context = batch[:, :1] if np.random.rand() > dropout_prob else None\n",
    "        logits = model(x_t, t, context=context)\n",
    "        loss = F.cross_entropy(logits.view(-1, vocab_size), batch.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        running_avg_loss = total_loss / batch_count\n",
    "        global_step += 1\n",
    "\n",
    "        # Log batch loss to TensorBoard\n",
    "        writer.add_scalar(\"Loss/Batch\", loss.item(), global_step)\n",
    "        writer.add_scalar(\"Loss/Running_Average\", running_avg_loss, global_step)\n",
    "\n",
    "        # Log learning rate\n",
    "        writer.add_scalar(\"Learning_Rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "\n",
    "        inner_pbar.set_postfix({\"Step\": f\"{batch_idx + 1}/{batch_nb}\", \"Status\": \"Processing...\"})\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    epoch_bar.set_postfix({\"Epoch\": f\"{epoch + 1}/{num_epochs}\", \"Loss\": f\"{avg_loss:.4f}\"})\n",
    "\n",
    "    # Log epoch loss to TensorBoard\n",
    "    writer.add_scalar(\"Loss/Epoch\", avg_loss, epoch + 1)\n",
    "\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"loss\": avg_loss,\n",
    "        }\n",
    "        torch.save(checkpoint, f\"../models/d3pm_epoch_{epoch + 1}.pth\")\n",
    "        print(f\"Saved checkpoint: d3pm_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()\n",
    "print(\"‚úÖ Training completed!\")\n",
    "print(f\"View logs with: tensorboard --logdir={log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af0d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Add gradient norm logging and model parameter histograms\n",
    "# Uncomment the lines below if you want more detailed logging\n",
    "\n",
    "# Log model parameters and gradients every 10 batches\n",
    "# if global_step % 10 == 0:\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if param.grad is not None:\n",
    "#             writer.add_histogram(f'Gradients/{name}', param.grad, global_step)\n",
    "#         writer.add_histogram(f'Parameters/{name}', param, global_step)\n",
    "\n",
    "#     # Log gradient norm\n",
    "#     total_norm = 0\n",
    "#     for p in model.parameters():\n",
    "#         if p.grad is not None:\n",
    "#             param_norm = p.grad.data.norm(2)\n",
    "#             total_norm += param_norm.item() ** 2\n",
    "#     total_norm = total_norm ** (1. / 2)\n",
    "#     writer.add_scalar('Gradient_Norm', total_norm, global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1270f352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: Come the of him a-meowin‚Äô city And it‚Äôs to   place man,\n",
      "Generated: Come the of merrier‚Äù big I‚Äôm Gate tryin‚Äô   was  on\n",
      "Generated: Come the of That very Cops   How    \n"
     ]
    }
   ],
   "source": [
    "# Generate samples with context\n",
    "start_sentence = \"Come the days of\"\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    start_words = start_sentence.split()\n",
    "    context = torch.tensor([word_to_idx[word] for word in start_words if word in word_to_idx], device=device)  # Sh\n",
    "    samples = diffusion.sample(model, batch_size=3, device=device, context=context, guidance_scale=2.0)\n",
    "    for sample in samples:\n",
    "        text = [idx_to_word[idx.item()] for idx in sample if idx.item() in idx_to_word]\n",
    "        print(\"Generated:\", \" \".join(text).replace(\"<pad>\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
