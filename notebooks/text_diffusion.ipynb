{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ca32e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from my_transformer.utils import save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4be615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "BATCH_SIZE = 64\n",
    "BLOCK_SIZE = 128  # Max sequence length for training\n",
    "EMBEDDING_DIM = 64\n",
    "N_HEADS = 4\n",
    "N_LAYERS = 2\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_DIFFUSION_STEPS = 100\n",
    "TRAIN_STEPS = 50000  # More steps needed for better results\n",
    "GRADIENT_ACCUMULATION_STEPS = 1  # For larger effective batch size\n",
    "\n",
    "# Device setup\n",
    "device = \"mps\" if torch.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Data Loading and Preparation (from NanoGPT) ---\n",
    "# !wget https://raw.githubusercontent.com/karpathy/makemore/master/tinyshakespeare.txt\n",
    "# Check if tinyshakespeare.txt exists, if not, download it\n",
    "# data_file = \"tinyshakespeare.txt\"\n",
    "data_file = Path(\"../dataset\") / \"shakespeare.txt\"\n",
    "if not os.path.exists(data_file):\n",
    "    print(f\"Downloading {data_file}...\")\n",
    "    import requests\n",
    "\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/makemore/master/tinyshakespeare.txt\"\n",
    "    r = requests.get(url)\n",
    "    with open(data_file, \"w\") as f:\n",
    "        f.write(r.text)\n",
    "    print(f\"Downloaded {data_file}\")\n",
    "else:\n",
    "    print(f\"{data_file} already exists.\")\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Create character vocabulary\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}  # String to int\n",
    "itos = {i: ch for i, ch in enumerate(chars)}  # Int to string\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(chars)}\")\n",
    "\n",
    "# Train/validation split\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9 * len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "\n",
    "# Data batching function\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - BLOCK_SIZE, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i : i + BLOCK_SIZE] for i in ix])\n",
    "    return x.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663d5db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "stoi\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278b182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm but with optional bias. PyTorch's LayerNorm doesn't support bias=False\"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") and config.flash\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        qkv = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, attn_mask=None, dropout_p=self.dropout.p, is_causal=False\n",
    "            )\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "            att = F.softmax(att, dim=-1)  # No causal mask for diffusion\n",
    "            att = self.dropout(att)\n",
    "            y = att @ v  # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)  # Will modify to remove causal mask\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffusionConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, bias, flash=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.flash = flash\n",
    "\n",
    "\n",
    "class DenoisingUnet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_embd * 4), nn.SiLU(), nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        )\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)  # Predict logit for noise (or original)\n",
    "\n",
    "        # Weight tying (typical for transformer LMs)\n",
    "        self.lm_head.weight = self.wte.weight  # Share the same weight for embedding and unembedding\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x_noisy_embed, timesteps):\n",
    "        B, T, C = x_noisy_embed.size()\n",
    "\n",
    "        # Positional embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=timesteps.device)\n",
    "        pos_emb = self.wpe(pos)\n",
    "\n",
    "        # Timestep embeddings\n",
    "        # We need to create sinusoidal embeddings for timesteps first, then pass through MLP\n",
    "        # This is a common practice in diffusion models\n",
    "        half_dim = self.config.n_embd // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n",
    "        emb = timesteps.float()[:, None] * emb[None, :]  # (B, dim)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)  # (B, dim * 2)\n",
    "        timestep_emb = self.timestep_mlp(emb).unsqueeze(1)  # (B, 1, C)\n",
    "\n",
    "        # Add timestep embedding to sequence embeddings\n",
    "        # This is often added at various points in a real U-Net. For simplicity, we add it once here.\n",
    "        x = x_noisy_embed + pos_emb + timestep_emb\n",
    "\n",
    "        x = self.drop(x)\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Project back to vocabulary logits (or noise prediction directly)\n",
    "        # For simplicity, we directly predict the logit for the original token\n",
    "        # A more direct diffusion approach would predict the noise or the 'x_0' directly.\n",
    "        # We'll use this to predict the target for the loss function.\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a604591",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm but with optional bias. PyTorch's LayerNorm doesn't support bias=False\"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.flash = hasattr(torch.nn.functional, \"scaled_dot_product_attention\") and config.flash\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "        qkv = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n",
    "\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(\n",
    "                q, k, v, attn_mask=None, dropout_p=self.dropout.p, is_causal=False\n",
    "            )\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / (k.size(-1) ** 0.5))\n",
    "            att = F.softmax(att, dim=-1)  # No causal mask for diffusion\n",
    "            att = self.dropout(att)\n",
    "            y = att @ v  # (B, nh, T, T) @ (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)  # Will modify to remove causal mask\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffusionConfig:\n",
    "    def __init__(self, vocab_size, block_size, n_layer, n_head, n_embd, dropout, bias, flash=True):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.n_layer = n_layer\n",
    "        self.n_head = n_head\n",
    "        self.n_embd = n_embd\n",
    "        self.dropout = dropout\n",
    "        self.bias = bias\n",
    "        self.flash = flash\n",
    "\n",
    "\n",
    "class DenoisingUnet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_embd * 4), nn.SiLU(), nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        )\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)  # Predict logit for noise (or original)\n",
    "\n",
    "        # Weight tying (typical for transformer LMs)\n",
    "        self.lm_head.weight = self.wte.weight  # Share the same weight for embedding and unembedding\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x_noisy_embed, timesteps):\n",
    "        B, T, C = x_noisy_embed.size()\n",
    "\n",
    "        # Positional embeddings\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=timesteps.device)\n",
    "        pos_emb = self.wpe(pos)\n",
    "\n",
    "        # Timestep embeddings\n",
    "        # We need to create sinusoidal embeddings for timesteps first, then pass through MLP\n",
    "        # This is a common practice in diffusion models\n",
    "        half_dim = self.config.n_embd // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n",
    "        emb = timesteps.float()[:, None] * emb[None, :]  # (B, dim)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)  # (B, dim * 2)\n",
    "        timestep_emb = self.timestep_mlp(emb).unsqueeze(1)  # (B, 1, C)\n",
    "\n",
    "        # Add timestep embedding to sequence embeddings\n",
    "        # This is often added at various points in a real U-Net. For simplicity, we add it once here.\n",
    "        x = x_noisy_embed + pos_emb + timestep_emb\n",
    "\n",
    "        x = self.drop(x)\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Project back to vocabulary logits (or noise prediction directly)\n",
    "        # For simplicity, we directly predict the logit for the original token\n",
    "        # A more direct diffusion approach would predict the noise or the 'x_0' directly.\n",
    "        # We'll use this to predict the target for the loss function.\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f5d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianDiffusion:\n",
    "    def __init__(self, num_timesteps, embed_dim, device):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "\n",
    "        # Linear noise schedule\n",
    "        self.betas = torch.linspace(0.0001, 0.02, num_timesteps, device=device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "\n",
    "        # Calculations for posterior q(x_{t-1} | x_t, x_0)\n",
    "        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n",
    "\n",
    "    def q_sample(self, x_start_embed, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion (adding noise)\n",
    "        x_start_embed: (B, T, C) - ground truth embeddings\n",
    "        t: (B,) - timesteps\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start_embed)\n",
    "\n",
    "        sqrt_alphas_cumprod_t = self.sqrt_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1)\n",
    "\n",
    "        x_noisy = sqrt_alphas_cumprod_t * x_start_embed + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        return x_noisy, noise\n",
    "\n",
    "    def p_mean_variance(self, model, x_t_embed, t, x_start_embed_uncond=None):\n",
    "        \"\"\"\n",
    "        Predict parameters of the reverse diffusion process.\n",
    "        Here, we simplify by directly predicting the noise.\n",
    "        A more direct approach might predict x_0, and then derive mean/variance.\n",
    "        \"\"\"\n",
    "        # Model predicts noise (or x_0)\n",
    "        # For our simplified model, the DenoisingUnet directly predicts logits of x_0\n",
    "        # So we adapt this to be a noise prediction via the common reparameterization.\n",
    "        predicted_x_start_logits = model(x_t_embed, t)  # (B, T, vocab_size)\n",
    "        # Convert predicted logits to embeddings (average or gumbel-softmax during training for discrete)\n",
    "        # For simplicity, we'll assume the model is learning to predict an \"x_0_pred\" that's compatible\n",
    "        # with our embedding space for loss calculation.\n",
    "        # A true Denoising diffusion would predict epsilon (noise) or x_0.\n",
    "        # Let's make it predict noise (epsilon) for DDPM formulation clarity.\n",
    "\n",
    "        # Adapt DenoisingUnet to predict noise (epsilon) instead of x_0 logits.\n",
    "        # This means the final lm_head should predict noise in embedding space.\n",
    "        # Let's adjust DenoisingUnet's lm_head for this:\n",
    "        # Instead of lm_head, we'll have a final linear layer to predict noise in embedding dim.\n",
    "\n",
    "        # For this example, let's assume the model outputs `predicted_noise`.\n",
    "        # This requires changing the last layer of DenoisingUnet:\n",
    "        # self.lm_head = nn.Linear(config.n_embd, config.n_embd, bias=False) # Predict noise in embedding space\n",
    "\n",
    "        # If DenoisingUnet predicts noise:\n",
    "        # predicted_noise = model(x_t_embed, t) # (B, T, C)\n",
    "        # Then we would derive x_0_pred from predicted_noise\n",
    "        # pred_x_start = (x_t_embed - self.sqrt_one_minus_alphas_cumprod[t].view(-1,1,1) * predicted_noise) / self.sqrt_alphas_cumprod[t].view(-1,1,1)\n",
    "\n",
    "        # For our current DenoisingUnet, it predicts logits of x_0.\n",
    "        # Let's make the loss directly compare the noisy embedding to the target embedding.\n",
    "        # We will train the model to predict the *original* x_start_embed from the noisy x_t_embed.\n",
    "        # This simplifies the DDPM equations for training.\n",
    "\n",
    "        # For sampling, if the model predicts x_start_embed_pred:\n",
    "        # Mean of q(x_{t-1} | x_t, x_0_pred)\n",
    "        # mean = (x_t_embed - self.betas[t].view(-1,1,1) * predicted_noise / self.sqrt_one_minus_alphas_cumprod[t].view(-1,1,1)) / self.sqrt_alphas_cumprod[t].view(-1,1,1)\n",
    "\n",
    "        # For simplicity, let's assume `model` outputs the *predicted noise* `eps_pred`\n",
    "        # and we need to modify DenoisingUnet's last layer to predict `config.n_embd`\n",
    "        # instead of `vocab_size`.\n",
    "        # Let's adjust DenoisingUnet to predict noise directly for clarity.\n",
    "        pass  # This function is more complex and depends on the exact formulation of the reverse process.\n",
    "        # We'll use a simplified training loss below.\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x_t_embed, t):\n",
    "        \"\"\"\n",
    "        Reverse diffusion (denoising step for sampling)\n",
    "        \"\"\"\n",
    "        # This implementation requires the model to predict `predicted_noise` (epsilon)\n",
    "        # which is the common DDPM setup.\n",
    "        # Let's assume `model` directly predicts `predicted_noise`.\n",
    "        # So DenoisingUnet's `lm_head` will output `config.n_embd`\n",
    "        # and we'll use MSE loss on noise.\n",
    "\n",
    "        predicted_noise = model(x_t_embed, t)\n",
    "\n",
    "        # Derived x_0_pred from predicted_noise\n",
    "        pred_x_start = (\n",
    "            x_t_embed - self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1) * predicted_noise\n",
    "        ) / self.sqrt_alphas_cumprod[t].view(-1, 1, 1)\n",
    "\n",
    "        # Clamp x_0_pred to be within reasonable bounds (e.g., embedding space range)\n",
    "        # Not strictly necessary if embeddings are normalized\n",
    "        # pred_x_start = torch.clamp(pred_x_start, -1., 1.) # if embeddings are normalized\n",
    "\n",
    "        model_mean = pred_x_start * self.sqrt_alphas_cumprod_prev[t].view(\n",
    "            -1, 1, 1\n",
    "        ) + self.sqrt_one_minus_alphas_cumprod_t[t].view(-1, 1, 1) * (\n",
    "            1.0 - self.alphas_cumprod_prev[t].view(-1, 1, 1)\n",
    "        ) * predicted_noise / (1.0 - self.alphas_cumprod[t].view(-1, 1, 1))\n",
    "\n",
    "        # This formula is incorrect and simplified. Correct mean formula from DDPM:\n",
    "        # mean = (x_t - self.betas[t] * predicted_noise / self.sqrt_one_minus_alphas_cumprod[t]) / self.sqrt_alphas[t]\n",
    "\n",
    "        # Let's use simpler DDPM sampling for this example\n",
    "        beta_t = self.betas[t].view(-1, 1, 1)\n",
    "        sqrt_one_minus_alphas_cumprod_t = self.sqrt_one_minus_alphas_cumprod[t].view(-1, 1, 1)\n",
    "        sqrt_alpha_t = torch.sqrt(self.alphas[t]).view(-1, 1, 1)\n",
    "\n",
    "        # Mean of q(x_{t-1} | x_t, x_0)\n",
    "        model_mean = (x_t_embed - beta_t * predicted_noise / sqrt_one_minus_alphas_cumprod_t) / sqrt_alpha_t\n",
    "\n",
    "        if t.min() == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = self.posterior_variance[t].view(-1, 1, 1)\n",
    "            noise = torch.randn_like(x_t_embed)\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, model, shape):\n",
    "        \"\"\"\n",
    "        Complete sampling loop.\n",
    "        shape: (B, T, C) - desired shape of the output embeddings\n",
    "        \"\"\"\n",
    "        B, T, C = shape\n",
    "        img = torch.randn(shape, device=self.device)  # Start with pure noise\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.num_timesteps)), desc=\"sampling loop time step\", total=self.num_timesteps):\n",
    "            t = torch.full((B,), i, device=self.device, dtype=torch.long)\n",
    "            img = self.p_sample(model, img, t)\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, x_start_embed, num_samples=1):\n",
    "        \"\"\"\n",
    "        Simplified sampling for text by starting from noise and denoising.\n",
    "        This won't use x_start_embed directly, but needs its shape.\n",
    "        \"\"\"\n",
    "        return self.p_sample_loop(model, x_start_embed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69abb412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Modify DenoisingUnet to predict noise (epsilon) ---\n",
    "class DenoisingUnet(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.wte = nn.Embedding(config.vocab_size, config.n_embd)\n",
    "        self.wpe = nn.Embedding(config.block_size, config.n_embd)\n",
    "        self.timestep_mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, config.n_embd * 4), nn.SiLU(), nn.Linear(config.n_embd * 4, config.n_embd)\n",
    "        )\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "        self.h = nn.ModuleList([Block(config) for _ in range(config.n_layer)])\n",
    "        self.ln_f = LayerNorm(config.n_embd, bias=config.bias)\n",
    "\n",
    "        # CHANGE: Predict noise in embedding space (output dim is embed_dim)\n",
    "        self.noise_head = nn.Linear(config.n_embd, config.n_embd, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x_noisy_embed, timesteps):\n",
    "        B, T, C = x_noisy_embed.size()\n",
    "\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=timesteps.device)\n",
    "        pos_emb = self.wpe(pos)\n",
    "\n",
    "        half_dim = self.config.n_embd // 2\n",
    "        emb = np.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, device=timesteps.device) * -emb)\n",
    "        emb = timesteps.float()[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "        timestep_emb = self.timestep_mlp(emb).unsqueeze(1)\n",
    "\n",
    "        x = x_noisy_embed + pos_emb + timestep_emb\n",
    "\n",
    "        x = self.drop(x)\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "\n",
    "        # Predict noise\n",
    "        predicted_noise = self.noise_head(x)  # (B, T, C)\n",
    "        return predicted_noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cce15b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Instantiation ---\n",
    "config = DiffusionConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    block_size=BLOCK_SIZE,\n",
    "    n_layer=N_LAYERS,\n",
    "    n_head=N_HEADS,\n",
    "    n_embd=EMBEDDING_DIM,\n",
    "    dropout=0.1,\n",
    "    bias=False,  # NanoGPT prefers no bias\n",
    ")\n",
    "\n",
    "model = DenoisingUnet(config).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "diffusion = GaussianDiffusion(NUM_DIFFUSION_STEPS, EMBEDDING_DIM, device)\n",
    "\n",
    "# --- Training Loop ---\n",
    "print(\"Starting training...\")\n",
    "model.train()\n",
    "optimizer.zero_grad()  # Initialize gradients\n",
    "\n",
    "TRAIN_STEPS = 1200\n",
    "for i in tqdm(range(1, TRAIN_STEPS + 1), desc=\"Training\"):\n",
    "    # Get a batch of original discrete tokens\n",
    "    batch_tokens = get_batch(\"train\")  # (B, T)\n",
    "\n",
    "    # Convert discrete tokens to embeddings (x_0)\n",
    "    x_start_embed = model.wte(batch_tokens)  # (B, T, C)\n",
    "\n",
    "    # Sample a random timestep t\n",
    "    t = torch.randint(0, NUM_DIFFUSION_STEPS, (BATCH_SIZE,), device=device).long()\n",
    "\n",
    "    # Add noise to x_start_embed\n",
    "    x_noisy_embed, noise = diffusion.q_sample(x_start_embed, t)\n",
    "\n",
    "    # Predict noise using the model\n",
    "    predicted_noise = model(x_noisy_embed, t)\n",
    "\n",
    "    # Calculate MSE loss between predicted noise and actual noise\n",
    "    loss = F.mse_loss(predicted_noise, noise) / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    if i % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Step {i}: Loss = {loss.item() * GRADIENT_ACCUMULATION_STEPS:.4f}\")\n",
    "        # Optionally, save model or run a quick validation/sample\n",
    "        save_model(model, \"txt_diffusion\", \"1_0\", i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f728029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sampling ---\n",
    "print(\"\\nStarting sampling...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Generate an initial random noise vector (shape matching expected output)\n",
    "    # This is where we control the length, e.g., BLOCK_SIZE\n",
    "    dummy_input_for_shape = torch.zeros(1, BLOCK_SIZE, EMBEDDING_DIM).to(device)\n",
    "    sampled_embeddings = diffusion.sample(model, dummy_input_for_shape)  # (B, T, C)\n",
    "\n",
    "    # To convert embeddings back to characters, we need a final projection\n",
    "    # For a real text diffusion model, you might use a sophisticated decoding step\n",
    "    # like a VAE decoder or a nearest-neighbor lookup in the embedding space.\n",
    "    # For simplicity, we'll find the nearest embedding in our vocabulary.\n",
    "\n",
    "    # Get all vocabulary embeddings\n",
    "    all_vocab_embeddings = model.wte.weight.data  # (vocab_size, EMBEDDING_DIM)\n",
    "\n",
    "    # Reshape sampled embeddings for batch processing\n",
    "    sampled_embeddings_flat = sampled_embeddings.view(-1, EMBEDDING_DIM)  # (B*T, C)\n",
    "\n",
    "    # Calculate cosine similarity or Euclidean distance to find nearest vocab embedding\n",
    "    # Cosine similarity is good for normalized embeddings\n",
    "    # We'll use Euclidean distance for simplicity here\n",
    "    distances = torch.cdist(sampled_embeddings_flat, all_vocab_embeddings)  # (B*T, vocab_size)\n",
    "\n",
    "    # Get the index of the closest vocabulary embedding\n",
    "    predicted_indices = torch.argmin(distances, dim=1)  # (B*T,)\n",
    "\n",
    "    # Reshape back to original sequence shape (B, T)\n",
    "    predicted_indices = predicted_indices.view(sampled_embeddings.size(0), sampled_embeddings.size(1))\n",
    "\n",
    "    # Decode and print\n",
    "    print(\"\\n--- Generated Sample (first batch item) ---\")\n",
    "    generated_text = decode(predicted_indices[0].tolist())\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f42535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870cb00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-implementations (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
