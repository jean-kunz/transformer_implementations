{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tokenizers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use transformer tokenezier to validate my code.\n",
    "from transformers import AutoTokenizer\n",
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "class ByteLevelPreTokenizer:\n",
    "    def __init__(self, add_prefix_space: bool = False):\n",
    "        self.add_prefix_space = add_prefix_space\n",
    "\n",
    "    def pre_tokenize_str(self, s):\n",
    "        if self.add_prefix_space:\n",
    "            s = \" \" + s\n",
    "        raw_tokens = [t for t in re.split(r\"(\\s|,|\\.|;|:|!|\\?|\\t)\", s) if t]\n",
    "        prev_t = None\n",
    "        tokens = []\n",
    "        start = 0\n",
    "        end = 0\n",
    "        for t in raw_tokens:\n",
    "            start = s[:].find(t, end)\n",
    "            end = start + len(t)\n",
    "            if t == \" \":\n",
    "                t = \"Ġ\"\n",
    "            if prev_t == \"Ġ\" and t != \"Ġ\":\n",
    "                t = prev_t + t\n",
    "                tokens.pop()\n",
    "                start -= 1\n",
    "            tokens.append((t, (start, end)))\n",
    "            prev_t = t\n",
    "        return tokens\n",
    "\n",
    "\n",
    "class ByteLevelDecoder:\n",
    "    def decode(self, tokens):\n",
    "        return \"\".join(t for t, p in tokens).replace(\"Ġ\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_encode_decode():\n",
    "    txt = \"Hello, world! How  are you?good .\"\n",
    "    toks = ByteLevelPreTokenizer(add_prefix_space=False).pre_tokenize_str(txt)\n",
    "    assert ByteLevelDecoder().decode(toks) == txt\n",
    "\n",
    "\n",
    "test_encode_decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pre_tokenizer_against_transformers():\n",
    "    \"\"\"Test ByteLevelPreTokenizer against transformers' ByteLevel pre-tokenizer\"\"\"\n",
    "    ByteLevelPreTokenizer(add_prefix_space=False).pre_tokenize_str(\"Hello, world! How  are you?good .\")\n",
    "    sents = [\n",
    "        \"Hello, world! How  are you?good .\",\n",
    "        \"Hello world,  How are you ? Go !\",\n",
    "    ]\n",
    "    add_prefix_space: bool = False\n",
    "    trf_pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=add_prefix_space)\n",
    "    my_pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=add_prefix_space)\n",
    "    my_decoder = ByteLevelDecoder()\n",
    "\n",
    "    for s in sents:\n",
    "        toks = my_pre_tokenizer.pre_tokenize_str(s)\n",
    "        trf_toks = trf_pre_tokenizer.pre_tokenize_str(s)\n",
    "        for t, trf_t in zip(toks, trf_toks):\n",
    "            assert t[0] == trf_t[0], f\"{t} != {trf_t}\"\n",
    "            assert t[1][0] == trf_t[1][0], f\"{t} != {trf_t}\"\n",
    "            assert t[1][1] == trf_t[1][1], f\"{t} != {trf_t}\"\n",
    "\n",
    "        assert my_decoder.decode(my_pre_tokenizer.pre_tokenize_str(s)) == \" \" + s if add_prefix_space else s\n",
    "\n",
    "\n",
    "test_pre_tokenizer_against_transformers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class Encoding:\n",
    "    def __init__(self, ids, tokens, offsets: Optional[list] = None, attention_mask: Optional[list] = None):\n",
    "        self.ids = ids\n",
    "        self.tokens = tokens\n",
    "        self.offsets = offsets\n",
    "        self.attention_mask = attention_mask\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"ids: {self.ids}\\ntokens: {self.tokens}\\noffsets: {self.offsets}\\nattention_mask: {self.attention_mask}\"\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self, vocab_size: int = 1000) -> None:\n",
    "        self.vocab_size: int = vocab_size\n",
    "        self.alphabet: list[str] = []\n",
    "        self.vocab: list[str] = []\n",
    "        self.merges: dict[tuple[str, str], str] = {}\n",
    "        self.itov: dict[int, str] = {}\n",
    "        self.vtoi: dict[str, int] = {}\n",
    "        self.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False)\n",
    "        self.padding: bool = False\n",
    "        self.length: int = -1\n",
    "        self.train_length: int = -1\n",
    "        self.padding_token: str = \"[PAD]\"\n",
    "\n",
    "        self.padding_direction: str = \"right\"\n",
    "        self.unknown_token: str = \"[UNK]\"\n",
    "        self.additional_special_tokens: list[str] = []\n",
    "\n",
    "    def enable_padding(self, padding_token: str = \"[PAD]\", length: int = 128, direction: str = \"right\"):\n",
    "        self.padding = True\n",
    "        self.padding_token = padding_token\n",
    "        self.length = length\n",
    "        self.padding_direction = direction\n",
    "\n",
    "    def pre_tokenize(self, text: str) -> list[str]:\n",
    "        return self.pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "    def compute_words_freqs(self, text: str) -> dict[str, int]:\n",
    "        word_freqs = defaultdict(int)\n",
    "        for word, p in self.pre_tokenizer.pre_tokenize_str(text):\n",
    "            word_freqs[word] += 1\n",
    "        return word_freqs\n",
    "\n",
    "    def init_vocab(self, word_freqs: dict[str, int]):\n",
    "        char_sets = set()\n",
    "        for word in word_freqs.keys():\n",
    "            char_sets.update(set(word))\n",
    "        alphabet = list(char_sets)\n",
    "        alphabet.sort()\n",
    "        vocab = [self.padding_token, self.unknown_token] + self.additional_special_tokens + alphabet.copy()\n",
    "        return vocab\n",
    "\n",
    "    def compute_pair_freqs(self, splits: dict[str, list[str]], word_freqs: dict[str, int]):\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for w, freq in word_freqs.items():\n",
    "            chars = splits[w]\n",
    "            for i in range(len(chars) - 1):\n",
    "                pair_freqs[(chars[i], chars[i + 1])] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "    def merge_pair(self, a, b, word_splits: dict[str, list[str]], word_freqs: dict[str, int]):\n",
    "        \"\"\"through all words split (into char or group of char), find the one that match a and b, and merge them\n",
    "        into a+b. Merge will replace a and b in the word_splits dict.\"\"\"\n",
    "        for w, freq in word_freqs.items():\n",
    "            split = word_splits[w]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            for i in range(len(split) - 1):\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            word_splits[w] = split\n",
    "\n",
    "        return word_splits\n",
    "\n",
    "    def find_best_pair(self, pair_freqs):\n",
    "        best_pair = \"\"\n",
    "        max_freq = None\n",
    "        for pair, freq in pair_freqs.items():\n",
    "            if max_freq is None or max_freq < freq:\n",
    "                max_freq = freq\n",
    "                best_pair = pair\n",
    "        return best_pair\n",
    "\n",
    "    def train_from_iterator(self, txt_iterator) -> None:\n",
    "        # for txt_item in txt_iterator:\n",
    "        text = \"\\n\".join(txt_iterator)\n",
    "        self.train(text)\n",
    "\n",
    "    def train(self, text: str) -> None:\n",
    "        \"\"\"Train a BPE tokenizer on a text for a given vocab size. A vocab size of 1000 means we will have\n",
    "        1000 tokens in our vocab, including the alphabet.\n",
    "\n",
    "        Arguments:\n",
    "            text -- _description_\n",
    "\n",
    "        Keyword Arguments:\n",
    "            vocab_size -- the nb of non alphabet (char) tokens. It represent a maximum nb of tokens (default: {1000}).\n",
    "        \"\"\"\n",
    "        word_freqs = self.compute_words_freqs(text)\n",
    "        self.vocab = self.init_vocab(word_freqs)\n",
    "        word_splits = {w: [c for c in w] for w in word_freqs.keys()}\n",
    "\n",
    "        while len(self.vocab) < self.vocab_size:\n",
    "            pair_freqs = self.compute_pair_freqs(word_splits, word_freqs)\n",
    "\n",
    "            best_pair = self.find_best_pair(pair_freqs)\n",
    "            if len(best_pair) == 2:\n",
    "                a, b = best_pair\n",
    "                word_splits = self.merge_pair(a, b, word_splits=word_splits, word_freqs=word_freqs)\n",
    "                self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "                self.vocab.append(best_pair[0] + best_pair[1])\n",
    "            else:\n",
    "                # when we can't find a pair, we stop, even if we don't reach the vocab size.\n",
    "                break\n",
    "\n",
    "        self.vtoi = {v: i for i, v in enumerate(self.vocab)}\n",
    "        self.itov = {i: v for i, v in enumerate(self.vocab)}\n",
    "\n",
    "    def encode(self, txt: str) -> Encoding:\n",
    "        pre_tokens = self.pre_tokenizer.pre_tokenize_str(txt)\n",
    "        word_splits = [([c for c in w], p) for w, p in pre_tokens]\n",
    "        for pair, merge in self.merges.items():\n",
    "            for i, (word_split, p) in enumerate(word_splits):\n",
    "                j = 0\n",
    "                while j < len(word_split) - 1:\n",
    "                    if word_split[j] == pair[0] and word_split[j + 1] == pair[1]:\n",
    "                        word_split = word_split[:j] + [merge] + word_split[j + 2 :]\n",
    "                    else:\n",
    "                        j += 1\n",
    "                word_splits[i] = (word_split, p)\n",
    "        encoded = []\n",
    "        offsets = []\n",
    "        tokens = []\n",
    "        attention_mask = []\n",
    "\n",
    "        # padding length is based on the number of tokens, not the number of chars.\n",
    "\n",
    "        for w, (s, e) in word_splits:\n",
    "            prev_tok_end = s\n",
    "            for i, c in enumerate(w):\n",
    "                tok_id = self.vtoi.get(c, self.vtoi.get(self.unknown_token))\n",
    "                encoded.append(tok_id)\n",
    "                tokens.append(self.itov.get(tok_id))\n",
    "                # attention mask is 1 for any tokens except it its a padding token\n",
    "                attention_mask.append(1)\n",
    "                offsets.append((prev_tok_end, prev_tok_end + len(c)))\n",
    "                prev_tok_end += len(c)\n",
    "\n",
    "        if self.padding:\n",
    "            encoded, offsets, tokens, attention_mask = self._pad(encoded, offsets, tokens, attention_mask)\n",
    "\n",
    "        enc = Encoding(ids=encoded, tokens=tokens, offsets=offsets, attention_mask=attention_mask)\n",
    "        return enc\n",
    "\n",
    "    def _pad(self, encoded, offsets, tokens, attention_mask):\n",
    "        nb_tokens = len(encoded)\n",
    "        if self.length > nb_tokens:\n",
    "            padding_length = self.length - nb_tokens\n",
    "            if self.padding_direction == \"right\":\n",
    "                encoded.extend([self.vtoi[self.padding_token]] * padding_length)\n",
    "                tokens.extend([self.padding_token] * padding_length)\n",
    "                attention_mask.extend([0] * padding_length)\n",
    "                offsets.extend([(0, 0)] * padding_length)\n",
    "            elif self.padding_direction == \"left\":\n",
    "                encoded = [self.vtoi[self.padding_token]] * padding_length + encoded\n",
    "                tokens = [self.padding_token] * padding_length + tokens\n",
    "                attention_mask = [0] * padding_length + attention_mask\n",
    "                offsets = [(0, 0)] * padding_length + offsets\n",
    "        return encoded, offsets, tokens, attention_mask\n",
    "\n",
    "    def decode(self, encoded: list[int]) -> str:\n",
    "        decoded = [self.itov.get(i, self.unknown_token).replace(\"Ġ\", \" \") for i in encoded]\n",
    "        return \"\".join(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = BPETokenizer(vocab_size=50)\n",
    "text = \"A Hello world, this is a test. A new world is coming, Hell yes . I love this\"\n",
    "tk.train(text)\n",
    "txt_to_enc = \"Hello world, I love to test this new thing\"\n",
    "\n",
    "encodings = tk.encode(txt_to_enc)\n",
    "\n",
    "assert \"\".join([t for t in encodings.tokens]).replace(\"Ġ\", \" \") == txt_to_enc\n",
    "assert \"\".join([tk.decode([i]) for i in encodings.ids]) == txt_to_enc\n",
    "assert \"\".join([txt_to_enc[s:e] for s, e in encodings.offsets]) == txt_to_enc\n",
    "\n",
    "encodings = tk.encode(\"Hello TOTO\")\n",
    "assert encodings.tokens[-5:] == [\"Ġ\", \"[UNK]\", \"[UNK]\", \"[UNK]\", \"[UNK]\"]\n",
    "\n",
    "tk.enable_padding(direction=\"right\", length=15)\n",
    "padded_encodings = tk.encode(\"Hello world TOTO\")\n",
    "assert padded_encodings.tokens[-5:] == [\"[UNK]\", \"[PAD]\", \"[PAD]\", \"[PAD]\", \"[PAD]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example using transformer tokenizer on a new training dataset\n",
    "tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "# tokenizer.enable_padding(length=20, pad_token=\"[PAD]\")\n",
    "trainer = trainers.BpeTrainer(vocab_size=50, special_tokens=[\"[PAD]\", \"[UNK]\"])  # pad is 0, unk is 1\n",
    "tokenizer.train_from_iterator([text], trainer=trainer)\n",
    "tokenizer.get_vocab_size()\n",
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.enable_padding(length=20, pad_token=\"[PAD]\")\n",
    "sent_to_encode = \"Hello World TOTO\"\n",
    "encodings = tokenizer.encode(sent_to_encode)\n",
    "encodings.offsets, encodings.tokens, encodings.attention_mask, encodings.ids\n",
    "# tokenizer.id_to_token(0)\n",
    "print(encodings.ids), print(encodings.tokens)\n",
    "# tokenizer.get_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.enable_padding(pad_id=2, pad_token=\"<|PAD|>\", length=55)\n",
    "tokenizer.enable_padding(length=30, pad_token=\"[PAD]\")\n",
    "tokenizer.encode(txt_to_enc[:50]).attention_mask\n",
    "tokenizer.encode(txt_to_enc[:50]).tokens\n",
    "# tokenizer.padding\n",
    "# tokenizer.get_vocab()\n",
    "tokenizer.encode(txt_to_enc[:50]).tokens\n",
    "tokenizer.token_to_id(\"TOTO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = BPETokenizer(vocab_size=50)\n",
    "\n",
    "text = \"A Hello world, this is a test. A new world is coming, Hell yes . I love this\"\n",
    "tk.train(text)\n",
    "text_enc = tk.encode(\"Hello, I love to test this new thing\")\n",
    "print(tk.decode(text_enc.ids))\n",
    "print(text_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export(\"./tokenizers.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
