{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tokenizers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " tst_str = \"Hello world, How are you? Go !\"\n",
    " tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(tst_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on whitespace and punctuation not using an existing tokenizer\n",
    "# | export\n",
    "def split_on_whitespace_and_punctuation(text):\n",
    "    \"\"\"Splits text on whitespace and punctuation.\"\"\"\n",
    "    tokens = []\n",
    "    token = \"\"\n",
    "    for c in text:\n",
    "        if c == \" \":\n",
    "            if token:\n",
    "                tokens.append(token)\n",
    "            token = \"Ġ\"\n",
    "        elif c in [\".\", \",\", \"!\", \"?\", \";\", \":\", '\"', \"'\"]:\n",
    "            # misse last !\n",
    "            if token:\n",
    "                tokens.append(token)\n",
    "            token = c\n",
    "            # tokens.append(token)\n",
    "            # token = \"\"\n",
    "        else:\n",
    "            token += c\n",
    "    if token:\n",
    "        tokens.append(token)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "text = \"Hello world, How are you? Go !\"\n",
    "text = \"A Go  !H\"\n",
    "tokens = split_on_whitespace_and_punctuation(text)\n",
    "# assert tokens == [\"Hello\",  \"Ġworld\", \",\",\"ĠHow\", \"Ġare\", \"Ġyou\", \"?\", 'ĠGo', 'Ġ!']\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.alphabet: list[str] = []\n",
    "        self.vocab: list[str] = []\n",
    "        self.merges: dict[tuple[str, str], str] = {}\n",
    "        self.itov: dict[int, str] = {}\n",
    "        self.vtoi: dict[str, int] = {}\n",
    "\n",
    "    # todo improve pre_tokenize to handle punctuation, etc.\n",
    "    def pre_tokenize(self, text: str) -> list[str]:\n",
    "        \"\"\"Split text into words, including space as token\"\"\"\n",
    "        return [i for j in text.lower().split() for i in (j, \" \")][:-1]  # to remove last space that is added.\n",
    "\n",
    "    def compute_words_freqs(self, text: str) -> dict[str, int]:\n",
    "        word_freqs = defaultdict(int)\n",
    "        for word in self.pre_tokenize(text):\n",
    "            word_freqs[word] += 1\n",
    "        return word_freqs\n",
    "\n",
    "    def init_vocab(self, word_freqs: dict[str, int]):\n",
    "        char_sets = set()\n",
    "        for word in word_freqs.keys():\n",
    "            char_sets.update(set(word))\n",
    "        alphabet = list(char_sets)\n",
    "        alphabet.sort()\n",
    "        vocab = [\"<|OOV|>\", \"<|EOT|>\"] + alphabet.copy()\n",
    "        return vocab\n",
    "\n",
    "    def compute_pair_freqs(self, splits: dict[str, list[str]], word_freqs: dict[str, int]):\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for w, freq in word_freqs.items():\n",
    "            chars = splits[w]\n",
    "            for i in range(len(chars) - 1):\n",
    "                pair_freqs[(chars[i], chars[i + 1])] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "    def merge_pair(self, a, b, word_splits: dict[str, list[str]], word_freqs: dict[str, int]):\n",
    "        \"\"\"through all words split (into char or group of char), find the one that match a and b, and merge them\n",
    "        into a+b. Merge will replace a and b in the word_splits dict.\"\"\"\n",
    "        for w, freq in word_freqs.items():\n",
    "            split = word_splits[w]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            for i in range(len(split) - 1):\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            word_splits[w] = split\n",
    "\n",
    "        return word_splits\n",
    "\n",
    "    def find_best_pair(self, pair_freqs):\n",
    "        best_pair = \"\"\n",
    "        max_freq = None\n",
    "        for pair, freq in pair_freqs.items():\n",
    "            if max_freq is None or max_freq < freq:\n",
    "                max_freq = freq\n",
    "                best_pair = pair\n",
    "        return best_pair\n",
    "\n",
    "    def train(self, text: str, vocab_size: int = 1000) -> None:\n",
    "        \"\"\"Train a BPE tokenizer on a text for a given vocab size. A vocab size of 1000 means we will have\n",
    "        1000 tokens in our vocab, including the alphabet.\n",
    "\n",
    "        Arguments:\n",
    "            text -- _description_\n",
    "\n",
    "        Keyword Arguments:\n",
    "            vocab_size -- the nb of non alphabet (char) tokens. It represent a maximum nb of tokens (default: {1000}).\n",
    "        \"\"\"\n",
    "        word_freqs = self.compute_words_freqs(text)\n",
    "        self.vocab = self.init_vocab(word_freqs)\n",
    "        word_splits = {w: [c for c in w] for w in word_freqs.keys()}\n",
    "\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            pair_freqs = self.compute_pair_freqs(word_splits, word_freqs)\n",
    "\n",
    "            best_pair = self.find_best_pair(pair_freqs)\n",
    "            if len(best_pair) == 2:\n",
    "                a, b = best_pair\n",
    "                word_splits = self.merge_pair(a, b, word_splits=word_splits, word_freqs=word_freqs)\n",
    "                self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "                self.vocab.append(best_pair[0] + best_pair[1])\n",
    "            else:\n",
    "                # when we can't find a pair, we stop, even if we don't reach the vocab size.\n",
    "                break\n",
    "        self.vtoi = {v: i for i, v in enumerate(self.vocab)}\n",
    "        self.itov = {i: v for i, v in enumerate(self.vocab)}\n",
    "        # print(f\"Vocab: {self.vocab}, len: {len(self.vocab)}\")\n",
    "        # print(f\"Merges: {self.merges}, len: {len(self.merges)}\")\n",
    "\n",
    "    def encode(self, txt: str) -> list[int]:\n",
    "        print(\"-- encode:\")\n",
    "        words = self.pre_tokenize(txt)\n",
    "        word_splits = [[c for c in w] for w in words]\n",
    "        print(word_splits)\n",
    "        for pair, merge in self.merges.items():\n",
    "            for i, word_split in enumerate(word_splits):\n",
    "                j = 0\n",
    "                while j < len(word_split) - 1:\n",
    "                    if word_split[j] == pair[0] and word_split[j + 1] == pair[1]:\n",
    "                        word_split = word_split[:j] + [merge] + word_split[j + 2 :]\n",
    "                    else:\n",
    "                        j += 1\n",
    "                word_splits[i] = word_split\n",
    "        print(f\"word splits: {word_splits}\")\n",
    "        # if not in vocab, replace by <|OOV|>\n",
    "        encoded = [self.vtoi.get(c, 0) for w in word_splits for c in w]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, encoded: list[int]) -> str:\n",
    "        decoded = [self.itov.get(i, \"<|OOV|>\") for i in encoded]\n",
    "        return \"\".join(decoded)\n",
    "\n",
    "\n",
    "text = \"A Hello world, this is a test. A new world is coming, Hell yes.\"\n",
    "tk = BPETokenizer()\n",
    "tk.train(text, vocab_size=50)\n",
    "text_enc = tk.encode(\"Hello, I love to test this new thing\")\n",
    "tk.decode(text_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export(\"./tokenizers.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer-implementations-urBBcPaT-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
