{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tokenizers\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "from collections import defaultdict\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use transformer tokenezier to validate my code.\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders\n",
    "pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True, use_regex=True)\n",
    "decoder = decoders.ByteLevel()\n",
    "\n",
    "tokenizer = Tokenizer(model=models.BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "tokenizer.decoder = decoder\n",
    "\n",
    "toks = pre_tokenizer.pre_tokenize_str(\"Hello, world! How  are you?good .\")\n",
    "toks = [t for t, p in toks]\n",
    "print(toks)\n",
    "decoder.decode(toks)\n",
    "#type(toks[0])\n",
    "#toks\n",
    "\n",
    "# no prefix space ['Hello', ',', 'Ġworld', '!', 'ĠHow', 'Ġ', 'Ġare', 'Ġyou', '?', 'good', 'Ġ.']\n",
    "# prefix space   ['ĠHello', ',', 'Ġworld', '!', 'ĠHow', 'Ġ', 'Ġare', 'Ġyou', '?', 'good', 'Ġ.']\n",
    "# prefix + regex ['ĠHello', ',', 'Ġworld', '!', 'ĠHow', 'Ġ', 'Ġare', 'Ġyou', '?', 'good', 'Ġ.']\n",
    "# prefix no regex['ĠHello,Ġworld!ĠHowĠĠareĠyou?goodĠ.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ByteLevelPreTokenizer:\n",
    "    def __init__(self, add_prefix_space:bool=False):\n",
    "        self.add_prefix_space = add_prefix_space\n",
    "\n",
    "    def pre_tokenize_str(self, s):\n",
    "        if self.add_prefix_space:\n",
    "            s = \" \" + s\n",
    "        raw_tokens = [t for t in re.split(r\"(\\s|,|\\.|;|:|!|\\?|\\t)\", s) if t]\n",
    "        prev_t = None\n",
    "        tokens = []\n",
    "        for t in raw_tokens:\n",
    "            if t == \" \":\n",
    "                t = \"Ġ\"\n",
    "            if prev_t == \"Ġ\" and t != \"Ġ\":\n",
    "                t = prev_t + t\n",
    "                tokens.pop()\n",
    "            tokens.append(t)\n",
    "            prev_t = t\n",
    "        return tokens\n",
    "    \n",
    "class ByteLevelDecoder:\n",
    "    def decode(self, tokens):\n",
    "        return \"\".join(tokens).replace(\"Ġ\", \" \")\n",
    "    \n",
    "    \n",
    "ByteLevelPreTokenizer(add_prefix_space=False).pre_tokenize_str(\"Hello, world! How  are you?good .\")    \n",
    "sents = [\n",
    "    \"Hello, world! How  are you?good .\", \"Hello world,  How are you ? Go !\",\n",
    "]\n",
    "trf_pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "my_pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=True)\n",
    "trf_decoder = decoders.ByteLevel()\n",
    "my_decoder = ByteLevelDecoder()\n",
    "\n",
    "for s in sents:\n",
    "    assert my_pre_tokenizer.pre_tokenize_str(s) == [\n",
    "        t for t, p in trf_pre_tokenizer.pre_tokenize_str(s)\n",
    "    ], f\"{my_pre_tokenizer.pre_tokenize_str(s)} != {[t for t, p in trf_pre_tokenizer.pre_tokenize_str(s)]}\"\n",
    "    \n",
    "    assert my_decoder.decode(my_pre_tokenizer.pre_tokenize_str(s))==\" \"+s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.alphabet: list[str] = []\n",
    "        self.vocab: list[str] = []\n",
    "        self.merges: dict[tuple[str, str], str] = {}\n",
    "        self.itov: dict[int, str] = {}\n",
    "        self.vtoi: dict[str, int] = {}\n",
    "        self.pre_tokenizer = ByteLevelPreTokenizer(add_prefix_space=False)\n",
    "\n",
    "    def pre_tokenize(self, text: str) -> list[str]:\n",
    "        return self.pre_tokenizer.pre_tokenize_str(text)\n",
    "\n",
    "    def compute_words_freqs(self, text: str) -> dict[str, int]:\n",
    "        word_freqs = defaultdict(int)\n",
    "        for word in self.pre_tokenize(text):\n",
    "            word_freqs[word] += 1\n",
    "        return word_freqs\n",
    "\n",
    "    def init_vocab(self, word_freqs: dict[str, int]):\n",
    "        char_sets = set()\n",
    "        for word in word_freqs.keys():\n",
    "            char_sets.update(set(word))\n",
    "        alphabet = list(char_sets)\n",
    "        alphabet.sort()\n",
    "        vocab = [\"<|OOV|>\", \"<|EOT|>\"] + alphabet.copy()\n",
    "        return vocab\n",
    "\n",
    "    def compute_pair_freqs(self, splits: dict[str, list[str]], word_freqs: dict[str, int]):\n",
    "        pair_freqs = defaultdict(int)\n",
    "        for w, freq in word_freqs.items():\n",
    "            chars = splits[w]\n",
    "            for i in range(len(chars) - 1):\n",
    "                pair_freqs[(chars[i], chars[i + 1])] += freq\n",
    "        return pair_freqs\n",
    "\n",
    "    def merge_pair(self, a, b, word_splits: dict[str, list[str]], word_freqs: dict[str, int]):\n",
    "        \"\"\"through all words split (into char or group of char), find the one that match a and b, and merge them\n",
    "        into a+b. Merge will replace a and b in the word_splits dict.\"\"\"\n",
    "        for w, freq in word_freqs.items():\n",
    "            split = word_splits[w]\n",
    "            if len(split) == 1:\n",
    "                continue\n",
    "            i = 0\n",
    "            for i in range(len(split) - 1):\n",
    "                if split[i] == a and split[i + 1] == b:\n",
    "                    split = split[:i] + [a + b] + split[i + 2 :]\n",
    "                else:\n",
    "                    i += 1\n",
    "            word_splits[w] = split\n",
    "\n",
    "        return word_splits\n",
    "\n",
    "    def find_best_pair(self, pair_freqs):\n",
    "        best_pair = \"\"\n",
    "        max_freq = None\n",
    "        for pair, freq in pair_freqs.items():\n",
    "            if max_freq is None or max_freq < freq:\n",
    "                max_freq = freq\n",
    "                best_pair = pair\n",
    "        return best_pair\n",
    "\n",
    "    def train(self, text: str, vocab_size: int = 1000) -> None:\n",
    "        \"\"\"Train a BPE tokenizer on a text for a given vocab size. A vocab size of 1000 means we will have\n",
    "        1000 tokens in our vocab, including the alphabet.\n",
    "\n",
    "        Arguments:\n",
    "            text -- _description_\n",
    "\n",
    "        Keyword Arguments:\n",
    "            vocab_size -- the nb of non alphabet (char) tokens. It represent a maximum nb of tokens (default: {1000}).\n",
    "        \"\"\"\n",
    "        word_freqs = self.compute_words_freqs(text)\n",
    "        self.vocab = self.init_vocab(word_freqs)\n",
    "        word_splits = {w: [c for c in w] for w in word_freqs.keys()}\n",
    "\n",
    "        while len(self.vocab) < vocab_size:\n",
    "            pair_freqs = self.compute_pair_freqs(word_splits, word_freqs)\n",
    "\n",
    "            best_pair = self.find_best_pair(pair_freqs)\n",
    "            if len(best_pair) == 2:\n",
    "                a, b = best_pair\n",
    "                word_splits = self.merge_pair(a, b, word_splits=word_splits, word_freqs=word_freqs)\n",
    "                self.merges[best_pair] = best_pair[0] + best_pair[1]\n",
    "                self.vocab.append(best_pair[0] + best_pair[1])\n",
    "            else:\n",
    "                # when we can't find a pair, we stop, even if we don't reach the vocab size.\n",
    "                break\n",
    "        self.vtoi = {v: i for i, v in enumerate(self.vocab)}\n",
    "        self.itov = {i: v for i, v in enumerate(self.vocab)}\n",
    "        # print(f\"Vocab: {self.vocab}, len: {len(self.vocab)}\")\n",
    "        # print(f\"Merges: {self.merges}, len: {len(self.merges)}\")\n",
    "\n",
    "    def encode(self, txt: str) -> list[int]:\n",
    "        print(\"-- encode:\")\n",
    "        words = self.pre_tokenize(txt)\n",
    "        word_splits = [[c for c in w] for w in words]\n",
    "        print(word_splits)\n",
    "        for pair, merge in self.merges.items():\n",
    "            for i, word_split in enumerate(word_splits):\n",
    "                j = 0\n",
    "                while j < len(word_split) - 1:\n",
    "                    if word_split[j] == pair[0] and word_split[j + 1] == pair[1]:\n",
    "                        word_split = word_split[:j] + [merge] + word_split[j + 2 :]\n",
    "                    else:\n",
    "                        j += 1\n",
    "                word_splits[i] = word_split\n",
    "        print(f\"word splits: {word_splits}\")\n",
    "        # if not in vocab, replace by <|OOV|>\n",
    "        encoded = [self.vtoi.get(c, 0) for w in word_splits for c in w]\n",
    "        return encoded\n",
    "\n",
    "    def decode(self, encoded: list[int]) -> str:\n",
    "        decoded = [self.itov.get(i, \"<|OOV|>\") for i in encoded]\n",
    "        return \"\".join(decoded)\n",
    "\n",
    "\n",
    "tk = BPETokenizer()\n",
    "ptk = ByteLevelPreTokenizer(add_prefix_space=False)\n",
    "\n",
    "sents = [\"Hello, world! How  are you?good .\", \"Hello world,  How are you ? Go !\"]\n",
    "for s in sents:\n",
    "    gpt_pre_tokenize = [t for t in ptk.pre_tokenize_str(s)]\n",
    "    assert (\n",
    "        tk.pre_tokenize(s) == gpt_pre_tokenize\n",
    "    ), f\"{pre_tokenize(s)} != {gpt_pre_tokenize} (gpt byte level pretokenizer)\"\n",
    "\n",
    "\n",
    "text = \"A Hello world, this is a test. A new world is coming, Hell yes.\"\n",
    "tk.train(text, vocab_size=50)\n",
    "text_enc = tk.encode(\"Hello, I love to test this new thing\")\n",
    "tk.decode(text_enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export(\"./tokenizers.ipynb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
